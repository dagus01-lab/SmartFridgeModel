{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee3ebdd-9338-4da0-8221-ec18150520ea",
   "metadata": {},
   "source": [
    "# Necessary imports for recipes generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4343246-a723-4ac2-989c-44e793e5f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, GPT2TokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from recipes_generator.data.recipe_dataset import RecipeDataset, load_preprocess_raw_json_data, load_preprocess_raw_csv_data\n",
    "import recipes_generator.model.recipe_model\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e4ccc-ec4b-406e-b058-337d259e2531",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11d1a42-dee8-4960-a76a-4e8ee23880d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes1 = load_preprocess_raw_json_data('recipes_generator/data/recipes_raw_nosource_ar.json')\n",
    "recipes2 = load_preprocess_raw_json_data('recipes_generator/data/recipes_raw_nosource_epi.json')\n",
    "recipes3 = load_preprocess_raw_json_data('recipes_generator/data/recipes_raw_nosource_fn.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d066f3-843a-4706-a59d-cda4e6d78a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes4 = load_preprocess_raw_csv_data('recipes_generator/data/recipes_general.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a48afba-1fe2-4ec0-af92-c377dd7fb6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>Prompt: beef eye round, dried thyme leaves, salt, pepper, ready-to-serve beef broth, Burgundy wine, garlic, cornstarch, frozen sugar snap peas\n",
      "Title: Low-Fat Burgundy Beef & Vegetable Stew\n",
      "Ingredients: 1 1/2 beef eye round,  1 dried thyme leaves,  1 salt,  1/2 pepper,  1/2 ready-to-serve beef broth,  1 Burgundy wine,  1/2 garlic,  3 cornstarch,  5 1/2 frozen sugar snap peas\n",
      "Servings: 6\n",
      "Instructions: Trim fat from beef cut into 1-inch pieces. In Dutch oven heat oil over medium high hunt until hot. Add beef (half at a time) and brown evenly stirring occasionally. Pour off drippings. Season with thyme salt and pepper. Stir in broth wine and garlic. Bring to boil; reduce heat to low. Cover tightly and simmer 1 1/2 hours. Add carrots and onions. Cover and continue cooking 35 to 40 minutes or until beef and vegetables are tender. Bring beef stew to a boil over medium-high heat. Add cornstarch mixture; cook and stir 1 minute. Stir in sugar snap peas. \n",
      "Reduce heat to medium and cook 3 to 4 minutes or until peas are heated through.\n",
      "Cook time: PT2H14M\n",
      "Preparation time: PT30M\n",
      "Total time: PT2H44M<|endoftext|>\n",
      "<|startoftext|>Prompt: French baguette, butter, garlic powder, ricotta cheese, parmesan cheese, salt, tomatoes, olive oil, fresh basil leaves\n",
      "Title: Lou's Fabulous Bruschetta\n",
      "Ingredients: 1 French baguette,  butter,  garlic powder,  3/4 ricotta cheese,  parmesan cheese,  salt,  4 -5 tomatoes,  1 -2 olive oil,  4 -5 fresh basil leaves\n",
      "Servings: 8\n",
      "Instructions: Cut baguette into slices. Butter and then sprinkle garlic powder each piece and bake till lightly golden brown. Mix garlic powder Parmesan cheese and salt in the ricotta cheese till a lovely blended taste. Dice tomatoes in small cubes   add Olive Oil and  small strips of fresh basil leaves cut into thin strips. Spread the ricotta mixture generously on the cooled garlic bread slices. Top with tomato mixture and bake at 350F for 10 minutes. Enjoy!\n",
      "Cook time: PT10M\n",
      "Preparation time: PT30M\n",
      "Total time: PT40M<|endoftext|>\n",
      "<|startoftext|>Prompt: Bisquick baking mix, sugar, butter, margarine, eggs, frozen blueberries, Bisquick baking mix, sugar, firm butter, margarine\n",
      "Title: Blueberry Dessert\n",
      "Ingredients: 2 Bisquick baking mix,  1/2 sugar,  1/2 butter,  2 margarine,  16 eggs,  1 frozen blueberries,  1 Bisquick baking mix,  1/4 sugar\n",
      "Servings: 12\n",
      "Instructions: Heat oven to 400 degrees. Mix 2 cups baking mix 1/2 cup sugar 1/2 cup butter  and egg yolks until crumbly. Press into ungreased rectangular pan 13x9x2. Bake 10 min. Beat egg whites until stiff; fold in blueberries. Spread over baked  layer. Mix remaining ingredients until crumbly; sprinkle over blueberry mixture. Bake until golden brown about 20 min.\n",
      "Cook time: PT0M\n",
      "Preparation time: PT35M\n",
      "Total time: PT35M<|endoftext|>\n",
      "<|startoftext|>Prompt: brown sugar, butter, vanilla, eggs, all-purpose flour, salt, white chocolate, butter, sour cream\n",
      "Title: Brownie Heart Cake\n",
      "Ingredients: 1 1/2 brown sugar,  3/4 butter,  1 vanilla,  3 eggs,  3/4 all-purpose flour,  1/2 salt,  1/4 white chocolate,  3/4 butter,  1/4 sour cream\n",
      "Servings: 1\n",
      "Instructions: CAKE:  Grease 5 cup heart shaped pan; dust with unsweetened cocoa powder and set aside.  In bowl blend together brown sugar butter and vanilla; add eggs one at a time beating well with wooden spoon after each addition. Sift together flour cocoa and salt; add all at once to creamed mixture mixing just until blended. Stir in nuts; spread in prepared pan. Loosely cover tip of heart with foil to prevent from drying out. Bake in 350xF oven for 35 to 40 minutes or until just barley firm to the touch. \n",
      "Let cool in pan for 10 minutes; turn out onto rack and let cool completely. FROSTING:  In top of double boiler over hot not boiling water melt white chocolate with butter. (Alternatively in microwaveable dish microwave at Medium/50% for 1 to 2 minutes or until softened.)  Remove from heat and let cool slightly; stir in sour cream until smooth and blended. Refrigerate for 10 minutes; spread smoothly over sides then top of cooled cake. GARNISH:  Sprinkle cocoa in attractive pattern on top of cake. Arrange almonds around top edge.\n",
      "Cook time: PT42M\n",
      "Preparation time: PT35M\n",
      "Total time: PT1H17M<|endoftext|>\n",
      "<|startoftext|>Prompt: fresh lemon juice, olive oil, black beans, fresh corn kernels, plum tomato, scallion, fresh parsley leaves, cayenne, boston lettuce leaves\n",
      "Title: Black Bean, Corn, and Tomato Salad\n",
      "Ingredients: 3 fresh lemon juice,  2 olive oil,  1 black beans,  1 fresh corn kernels,  1 plum tomato,  1 scallion,  2 fresh parsley leaves,  1 cayenne,  4 boston lettuce leaves\n",
      "Servings: 2\n",
      "Instructions: In a bowl whisk together lemon juice oil and salt to taste. Stir in remaining ingredients except lettuce leaves with salt and black pepper to taste. Let salad stand stirring once or twice 15 minutes for flavors to develop. Line 2 plates with lettuce and divide salad between them.\n",
      "Cook time: PT15M\n",
      "Preparation time: PT10M\n",
      "Total time: PT25M<|endoftext|>\n",
      "<|startoftext|>Prompt: olive oil, Italian sausage, garlic, red pepper flakes, salt, black pepper, sun-dried tomato, white wine, cabbage\n",
      "Title: Cabbage and Sausage Soup\n",
      "Ingredients: 2 olive oil,  1 1/2 Italian sausage,  2 garlic,  4 red pepper flakes,  1/4 salt,  1/2 black pepper,  1/4 sun-dried tomato,  1/2 white wine,  1/2 cabbage\n",
      "Servings: 6\n",
      "Instructions: In a medium stockpot or Dutch oven heat olive oil over high heat and saute sausage for 3 minutes. Remove with a slotted spoon and set aside. Add onion garlic red pepper flakes salt pepper and sun dried tomatoes. Reduce heat to medium high and saute until onions are soft about 3 - 5 minutes. Stir in the wine and cook 2 minutes. Add cabbage reserved sausage and chicken stock. bring to a boil reduce heat and simmer 15 minutes. - - - - - - - - - - - - - - - - - - NOTES :    To serve place a piece of garlic toast in the bottom of the bowl and ladle soup over top.\n",
      "Cook time: PT25M\n",
      "Preparation time: PT15M\n",
      "Total time: PT40M<|endoftext|>\n",
      "<|startoftext|>Prompt: butter, almond paste, sugar, eggs, flour, cornstarch, baking powder, milk\n",
      "Title: Almond Pound Cake\n",
      "Ingredients: 2/3 butter,  3 1/2 almond paste,  1 1/4 sugar,  4 eggs,  2 1/4 flour,  5 cornstarch,  1 baking powder,  1/2 milk\n",
      "Servings: 10\n",
      "Instructions: Preheat oven to 350 degrees Fahrenheit. Cream butter and almond paste until fluffy. Add sugar gradually add eggs one at a time.  Mix together flour cornstarch and baking powder then add flour mixture and milk alternately. Bake in greased 9x5 loaf pan for 1 hour. Double recipe for a bundt pan and bake 75 minute. Cool 10 min in pan then turn out and cool completely. Serve with powdered sugar orange glaze or chocolate glaze.\n",
      "Cook time: PT1H\n",
      "Preparation time: PT15M\n",
      "Total time: PT1H15M<|endoftext|>\n",
      "<|startoftext|>Prompt: brewed coffee, ketchup, red wine vinegar, dark brown sugar, onion, garlic cloves, dark molasses, hot dry mustard, Worcestershire sauce, cumin, chili powder\n",
      "Title: Black Coffee Barbecue Sauce\n",
      "Ingredients: 1/2 brewed coffee,  1 ketchup,  1/4 red wine vinegar,  3/4 dark brown sugar,  1 onion,  2 garlic cloves,  2 dark molasses,  2 hot dry mustard,  2 Worcestershire sauce,  2 cumin,  2 chili powder\n",
      "Servings: 1\n",
      "Instructions: Combine all ingredients in a saucepan and simmer over low heat for 20  minutes. Cool then puree in a blender or food processor until smooth. This can be stored in the refrigerator for up to 2 weeks in a covered  container.\n",
      "Cook time: PT20M\n",
      "Preparation time: PT10M\n",
      "Total time: PT30M<|endoftext|>\n",
      "<|startoftext|>Prompt: butter, sugar, eggs, cake flour, baking powder, salt, nutmeg, sour cream, Bourbon, pecans, powdered sugar, Bourbon, water\n",
      "Title: Bourbon Pecan Pound Cake\n",
      "Ingredients: 1/2 butter,  2 1/2 sugar,  6 eggs,  3 cake flour,  2 baking powder,  1 salt,  1 nutmeg,  1 sour cream,  1/2 Bourbon,  1 1/2 pecans,  2 powdered sugar,  1 Bourbon,  2 water\n",
      "Servings: 12\n",
      "Instructions: Combine butter and sugar in bowl of electric mixer and blend until light and fluffy. Add eggs one at a time beating constantly. Sift together flour baking powder salt and nutmeg. Blend sour cream and bourbon. Alternately add flour and sour cream mixture to batter. Add pecans. Grease bottom and sides of tube or bunt pan. Pour in batter and bake at 325 degrees for about one hour and 30 minutes. Test cake frequently after first hour and 15 minutes for doneness. Let cake cool in pan for 15 minutes before turning out on wire rack. \n",
      "To prepare glaze combine sugar and bourbon. Stir while gradually adding water. Add only enough water to make a pourable glaze without allowing mixture to become too thin. Pour glaze over top of warm cake and let dribble down sides.\n",
      "Cook time: PT1H\n",
      "Preparation time: PT1H\n",
      "Total time: PT2H<|endoftext|>\n",
      "<|startoftext|>Prompt: dry yeast, water, sugar, salt, flour, cornmeal, mozzarella cheese, oregano, pepper, green pepper, mushroom, parmesan cheese\n",
      "Title: Chicago Style Pizza\n",
      "Ingredients: 1 dry yeast,  1 1/4 water,  1 sugar,  1 1/2 salt,  1/4 flour,  3 cornmeal,  2 mozzarella cheese,  0.5 oregano,  1 pepper,  green pepper,  mushroom,  1/4 parmesan cheese\n",
      "Servings: 8\n",
      "Instructions: For crust dissolve yeast in water. Add sugar  salt and 2 tablespoons oil. Stir in flour to make a soft  dough. Turn out onto well floured board. Knead about 3  minutes. Put in greased bowl; cover and let rise in warm place  about 1 1/2  hours. Brush a 14-inch deep-dish pizza pan with  2 tablespoons oil; sprinkle  with corn meal. Punch dough down;  press in bottom of pan. Let rise 30 minutes. Punch down  again lightly coat with olive oil. Cook at 450 degrees for  about  10 minutes until set. (Punch down dough after 5 minutes or so.) Arrange cheese  over dough. \n",
      "Place pepperoni  olives peppers and mushrooms over cheese. Spread  pizza sauce. Sprinkle with oregano crushed pepper and  Parmesan cheese. Place  pizza in oven and cook at 450 degrees  until cheese is melted and crust is  golden (about 20 to 25 minutes). Makes  8 slices.\n",
      "Cook time: PT2H38M\n",
      "Preparation time: PT35M\n",
      "Total time: PT3H13M<|endoftext|>\n",
      "<|startoftext|>Prompt: eggs, water, buckwheat flour, light brown sugar, whole wheat flour, buttermilk, salt, caraway seed, all-purpose flour\n",
      "Title: Buckwheat Bread\n",
      "Ingredients: 1 eggs,  2 water,  2 buckwheat flour,  2 light brown sugar,  1 whole wheat flour,  2 buttermilk,  3/4 salt,  1 caraway seed,  1 all-purpose flour\n",
      "Servings: 1\n",
      "Instructions: Proof the yeast in the lukewarm water with the sugar. After you get a yeast sponge add the buttermilk (room temperature) salt eggs and the three different kinds of flour and caraway seeds. Mix well. Take the dough out onto a lightly floured work surface kneading until smooth. Place the dough in a large buttered bowl cover with a towel and let stand in a warm place to rise until doubled in bulk. This will take from 1~/2 to 2 hours. Take the dough out and punch it down a couple of times. \n",
      "Knead and shape the dough into a loaf and place in a buttered loaf pan (9~ by 5~). Let stand in a warm place covered with a towel to rise for about 1 hour or until doubled in size. Preheat the oven to 400 degrees. Brush the bread with a little melted butter and bake in the oven for about 30 minutes or until light brown and the bread sounds hollow when tapped with your finger. Cool wrapped in a towel.\n",
      "Cook time: PT3H\n",
      "Preparation time: PT35M\n",
      "Total time: PT3H35M<|endoftext|>\n",
      "<|startoftext|>Prompt: chicken, carrot, celery, onion, chicken bouillon cubes, butter, flour, sherry wine, lemon juice, salt, baking powder, salt, shortening, milk, peas\n",
      "Title: Chicken and Dumplings\n",
      "Ingredients: 4 chicken,  1 carrot,  2 celery,  1 onion,  2 chicken bouillon cubes,  1/4 butter,  1 flour,  1 sherry wine,  1 lemon juice,  salt,  baking powder,  2 salt,  1/4 shortening,  3 milk,  3/4 peas\n",
      "Servings: 8\n",
      "Instructions: Place chickens in large saucepan cover with water and bring to a boil. Simmer until chickens are tender. Remove chickens from pan and set aside. Remove backbones. Add chopped carrot celery and onion and simmer 30 minutes. Add chicken stock base and remove from heat. Strain and reserve stock. Melt butter and stir in 1 cup flour until smooth. Add to straine reserved chicken stock. Simmer 5 minutes. Add sherry and lemon juice salt to taste and if desired food color. \n",
      "For dumplings combine 1 1/2 cups flour baking powder and 1/4 tsp salt in mixing bowl. Cut in shortening until mixture resembles cornmeal. Stir in milk until just blended. Place about 1/2 inch water in saucepan with wire rack that comes 2 to 3 inches above water line. Cover rack with lightly oiled waxed paper oiled side up. With water gently simmering drop dumplings by tablespoon onto waxed paper leaving room in between for expansion. Steam 8 minutes uncovered then cover and steam 7 minutes longer. \n",
      "Meanwhile remove skin from cooked chicken and bone if desired. Cut in large pieces. Place chicken pieces in casserole. Arrange dumplings on chicken. Cover with sauce and sprinkle with peas for color.\n",
      "Cook time: PT43M\n",
      "Preparation time: PT1H25M\n",
      "Total time: PT2H8M<|endoftext|>\n",
      "<|startoftext|>Prompt: all-purpose flour, granulated sugar, baking powder, salt, milk, vanilla, butter, brown sugar, boiling water\n",
      "Title: Brownie Pudding\n",
      "Ingredients: 1/2 all-purpose flour,  1/2 granulated sugar,  1/2 baking powder,  2 salt,  1/4 milk,  1/2 vanilla,  1 butter,  2 brown sugar,  1/2 boiling water\n",
      "Servings: 10\n",
      "Instructions: Preheat oven to 350 degrees. Combine flour granulated sugar 1/4  cup cocoa  baking powder and salt in a bowl. Stir in milk butter and vanilla  until  smooth. Stir in chocolate chips. Spread in ungreased shallow 1  1/2 quart  casserole.  Sprinkle brown sugar and 1/4 cup cocoa on top. Place in oven pour  boiling water over top. Bake 35 minutes cool 10 minutes before  serving. Serve with ice cream if desired.\n",
      "Cook time: PT35M\n",
      "Preparation time: PT40M\n",
      "Total time: PT1H15M<|endoftext|>\n",
      "<|startoftext|>Prompt: milk, fine salt, white pepper, mace, ginger, nutmeg, powdered soy protein concentrate, eggs, fresh parsley\n",
      "Title: Bratwurst\n",
      "Ingredients: 3 milk,  7 fine salt,  1 white pepper,  1 mace,  1/2 ginger,  2 nutmeg,  2 powdered soy protein concentrate,  2 eggs,  1 fresh parsley\n",
      "Servings: 1\n",
      "Instructions: Grind the veal and pork with a 3/8 in.  (0.95 cm) plate. Place the ground meat into a mixing tub and add all remaining ingredients. Mix very thoroughly. Return to the cooler and chill at least 1 hour. Stuff the prepared casings.Tie into 4 in.  (10 cm) links. Poach the sausages in good veal stock for the best flavor. Cover with the cold stock and heat gently. Maintain a temperature of no more than 160 F (710C). When the sausages reach an internal temperature of 140 F (60 C) remove from the stock. \n",
      "Cool quickly. Dry wrap and refrigerate for use.\n",
      "Cook time: PT1H\n",
      "Preparation time: PT50M\n",
      "Total time: PT1H50M<|endoftext|>\n",
      "<|startoftext|>Prompt: instant coffee granules, cinnamon, sugar, sugar, all-purpose flour, vanilla extract, fat free cream cheese, skim milk, fresh raspberry\n",
      "Title: Brownie Cheesecake Torte\n",
      "Ingredients: 15 1/4 instant coffee granules,  2 cinnamon,  1/2 sugar,  4 sugar,  all-purpose flour,  1/2 vanilla extract,  2 fat free cream cheese,  4 skim milk,  1 fresh raspberry\n",
      "Servings: 12\n",
      "Instructions: Preheat oven to 425 degrees F. Combine first 4 ingredients in a bowl. Firmly press mixture into bottom and 1 inch up sides of a 9-inch springform pan coated with cooking spray. Set aside. Combine 1/2 cup sugar flour vanilla and cheeses;  beat at medium speed of a mixer until well-blended. Add egg whites and 2 tablespoons milk; beat well. Combine 1/2 cup batter 1 tablespoon milk 2 tablespoons sugar and cocoa in a small bowl; stir well. Spoon remaining batter alternately with cocoa mixture into prepared crust. Swirl together using the tip of a knife.  Bake at 425 degrees for 10 minutes. \n",
      "Reduce oven temperature to 250 degrees; bake 45 minutes or until almost set. Cool completely on a wire rack. Garnish with chocolate syrup and fresh raspberries if desired.\n",
      "Cook time: PT55M\n",
      "Preparation time: PT35M\n",
      "Total time: PT1H30M<|endoftext|>\n",
      "<|startoftext|>Prompt: tomatoes, celery, onion, green pepper, salt, cider vinegar, sugar, green chili pepper\n",
      "Title: California Chilled Salsa\n",
      "Ingredients: 2 tomatoes,  1 celery,  1 onion,  1 green pepper,  1 1/2 salt,  1 cider vinegar,  1 sugar,  1 green chili pepper\n",
      "Servings: 10\n",
      "Instructions: * Also delicious made with red sweet peppers or a combination of red and  green for nice color. Combine all ingredients; if finer texture is desired may be put  through food grinder using fine blade. Cover tightly and chill overnight. Serving Ideas : Serve cold as a relish with meat. Kim C.\n",
      "Cook time: PT0M\n",
      "Preparation time: PT25M\n",
      "Total time: PT25M<|endoftext|>\n",
      "<|startoftext|>Prompt: sweet butter, heavy cream, parmesan cheese, salt, pepper\n",
      "Title: Alfredo Sauce\n",
      "Ingredients: 1/4 sweet butter,  1 heavy cream,  3/4 parmesan cheese,  salt,  1/4 pepper\n",
      "Servings: 4\n",
      "Instructions: Place butter in microwave safe pot and heat on high for 30 seconds or until melted. Add cream and warm on high for approximately 1 minute. Add Parmesan cheese and warm until cheese melts. Add salt and pepper to taste. (If serving with shrimp you might not need much salt.). Pour over 4 servings of warm noodles (I use angel hair) and toss to coat. Serve immediately. Notes: I serve it with cubed grilled or broiled chicken breast mixed in with the noodles or shrimp. Add a green salad and you have a complete meal. I have used margarine and canned Parmesan cheese and it worked fine.\n",
      "Cook time: PT10M\n",
      "Preparation time: PT5M\n",
      "Total time: PT15M<|endoftext|>\n",
      "<|startoftext|>Prompt: garlic, salt, buttermilk, sour cream, lime juice, parsley, chives\n",
      "Title: Buttermilk Ranch Dressing\n",
      "Ingredients: 1 garlic,  3 salt,  3/4 buttermilk,  1/2 sour cream,  2 lime juice,  1 parsley,  1 chives\n",
      "Servings: 1\n",
      "Instructions: Mash together garlic and salt to form a paste.  Remove to a small bowl. Whisk in buttermilk sour cream lime juice parsley chives and salt and pepper. Taste  and adjust seasonings. Use immediately or cover and refrigerate.\n",
      "Cook time: PT0M\n",
      "Preparation time: PT20M\n",
      "Total time: PT20M<|endoftext|>\n",
      "<|startoftext|>Prompt: stewed tomatoes, sauerkraut, applesauce, dark brown sugar, beef brisket\n",
      "Title: Brisket with Sauerkraut and Applesauce\n",
      "Ingredients: 1 stewed tomatoes,  8 sauerkraut,  1 applesauce,  2 dark brown sugar,  1 beef brisket\n",
      "Servings: 6\n",
      "Instructions: In a large cook pot place undrained tomatoes sauerkraut with applesauce and  brown sugar. Bring to a boil. Lower heat and add brisket so that mixture can  cover some of brisket.Cover and simmer until meat is fork tender ( about 3 hours ); spooning sauce over brisket intermittently. When meat is done place brisket  on serving platter. Remove excess fat from pot. Season sauce to taste ( more  sugar spices etc. ). Thicken if desired with a bit of cornstarch mixed with  some cold water and add to sauce and cook and stir till thickened. \n",
      "For best  serving prepare brisket ahead of time let cool completely slice meat cover  with sauce and refrigerate till needed ( may be frozen ). Gently reheat and  serve slices of meat and sauce.\n",
      "Cook time: PT3H\n",
      "Preparation time: PT25M\n",
      "Total time: PT3H25M<|endoftext|>\n",
      "<|startoftext|>Prompt: milk, eggs, sugar, salt, real vanilla, unsalted butter\n",
      "Title: Bread Pudding\n",
      "Ingredients: 2 milk,  1 eggs,  2 sugar,  2 salt,  9 real vanilla,  1/2 unsalted butter\n",
      "Servings: 4\n",
      "Instructions: Mix bread and milk together. Add the 2 eggs and the 2 egg yolks with 5 tbsp of sugar and salt. Add to the milk mixture will be the vanilla and butter. Mix well and pour into a slightly greased 1 1/2 quart casserole. Bake in a pie pan of water at 350° oven for 45 minutes. For the Meringue topping: Beat the egg whites until stiff and add the 4 tbsp of sugar to it. Pour in slowly Top the already baked pudding with it--making swirly peaks for decorations. Bake again at 300° until a golden brown.\n",
      "Cook time: PT45M\n",
      "Preparation time: PT20M\n",
      "Total time: PT1H5M<|endoftext|>\n",
      "<|startoftext|>Prompt: butter, flour, salt, margarine, milk, water, shrimp, onion, tomatoes, parsley, lemon, salt, pepper, Tabasco sauce, garlic, cornstarch, olive oil, cumin, bell peppers\n",
      "Title: Brazilian Empadinhas\n",
      "Ingredients: butter,  2 1/2 flour,  1/2 salt,  1/4 margarine,  1 milk,  1 water,  3 shrimp,  1 onion,  1/2 tomatoes,  1/2 parsley,  3 lemon,  1 salt,  1 pepper,  Tabasco sauce,  garlic,  cornstarch,  1/2 olive oil,  2 cumin,  2 bell peppers\n",
      "Servings: 8\n",
      "Instructions: DOUGH Cut butter into salt and flour. Stir in rest of the ingredients and knead to  combine or just process everything until it forms a ball. Let rest covered 1  hour Make balls the size of a walnut from 2/3  of the dough and line small  muffin tins with the dough. Place 2 tsp. of the cold filling into the lined  tins. Make small balls with the rest of the dough. Flatten them to make lids for the mini pies. Brush tops with egg yolk Bake in preheated 350 F oven for 30 to  35 minutes or until golden brown. \n",
      "SHRIMP FILLING Clean and devein shrimp saving heads and shells. Make a broth with the shells  and heads and 2 cups water. Cook 1 hour uncovered over low heat. Strain broth  and cook it down to 1/2 cup. Cook onions and garlic in olive oil until they  start changing color. Add bell pepper and cook 2 minutes Add tomatoes broth  and seasonings and simmer almost to a paste. Add shrimp and cook just until they become opaque. Stir in cornstarch diluted in 2 Tbsp water. Stir until thick. \n",
      "Add parsley. Let cool before using NOTES : May use other fillings such as chicken hearts of palms cheese Miriam Podcameni Posvolsky Rio de Janeiro\n",
      "Cook time: PT2H37M\n",
      "Preparation time: PT1H\n",
      "Total time: PT3H37M<|endoftext|>\n",
      "<|startoftext|>Prompt: baking potatoes, unsalted butter, heavy cream, bacon, onion, celery, sage, white pepper, Brussels sprout\n",
      "Title: Bubble and Squeak\n",
      "Ingredients: 2 baking potatoes,  2 unsalted butter,  1/4 heavy cream,  4 bacon,  1/3 onion,  1/2 celery,  1 sage,  1/2 white pepper,  1 Brussels sprout\n",
      "Servings: 4\n",
      "Instructions: Wash potatoes under cold running water then place in a large saucepan. Cover with salted water bring to a boil reduce heat and simmer until soft uncovered about 15 minutes. Drain and set aside and allow to cool slightly. Peel potatoes and mash. Add butter and cream and mix well. Meanwhile fry bacon until crisp. Drain on paper towel and set aside. In the same skillet add onion to bacon drippings and saute until soft about 3 - 5 minutes. Stir in celery sage and pepper and saute 2 minutes longer. \n",
      "Add lemon juice. Combine vegetables with mashed potatoes stir well to combine and refrigerate for 2 hours. Shape potato mixture into 10 patties. Melt 2 tablespoons butter in a large skillet and saute patties until brown and crispy about 5 minutes per side. Crumble bacon over top and serve.                    - - - - - - - - - - - - - - - - - -\n",
      "Cook time: PT2H27M\n",
      "Preparation time: PT30M\n",
      "Total time: PT2H57M<|endoftext|>\n",
      "<|startoftext|>Prompt: Brussels sprouts, fresh dill, wine vinegar\n",
      "Title: Braised Brussels Sprouts With Vinegar and Dill\n",
      "Ingredients: 3 Brussels sprouts,  1/4 fresh dill,  2 wine vinegar\n",
      "Servings: 12\n",
      "Instructions: Trim sprouts; halved if desired. In large pot of boiling salted water cook Brussels sprouts for 8 minutes if whole 6 minutes if halved or until barely tender. Drain refresh under cold running water and drain again. In well-greased 13x9-inch casserole combine sprouts dill vinegar and salt and pepper to taste; mix well. Bake covered in 350°F oven for 10 minutes. Uncover and bake for 5 minutes longer.\n",
      "Cook time: PT23M\n",
      "Preparation time: PT25M\n",
      "Total time: PT48M<|endoftext|>\n",
      "<|startoftext|>Prompt: flour, baking powder, caster sugar, butter, eggs, milk, vanilla essence\n",
      "Title: Butter Madeira Cake\n",
      "Ingredients: 225 flour,  2 baking powder,  175 caster sugar,  175 butter,  3 eggs,  1 milk,  1 vanilla essence\n",
      "Servings: 8\n",
      "Instructions: Grease lined tin with melted butter. Sift flour and baking powder into  processor and add all remaining ingredients. Run machine for 20 counts. Scrape  down sides. Run machine for a further 30 counts. Transfer to prepared tin and spread evenly with a spatula.  Bake at 160° C for 1 1/2 hours. Leave in tin 15 minutes  before turning onto a rack. VARIATIONS Seed cake = Add 2-3 tsp caraway seeds after scraping down the sides. Orange Cake = omit vanilla. Add 1 tsp orange peel. \n",
      "Lemon Cake = Add 1 tsp lemon peel. Chocolate Speckle Cake = Add 1 crushed Chocolate Flake Bar after scraping down sides. Raisin Cake = Add 3 oz raisins after scraping down sides. Spicy Cake = Add 2-3 tsp Mixed Spice with the other ingredients.\n",
      "Cook time: PT1H45M\n",
      "Preparation time: PT1H10M\n",
      "Total time: PT2H55M<|endoftext|>\n",
      "<|startoftext|>Prompt: butter, flour, sugar, baking powder, salt, milk\n",
      "Title: Butter Dips\n",
      "Ingredients: 1/2 butter,  2 1/4 flour,  1 sugar,  3 1/2 baking powder,  3/4 salt,  1 milk\n",
      "Servings: 1\n",
      "Instructions: Heat oven to 450. Melt butter in oven oblong 13 x 9 1/2 x 3 inch pan. Remove when butter is melted. Sift dry ingredients into bowl. Add milk. Stir slowly with a fork until dough just clings together. Knead lightly about 10 times. Roll out 1/2 inch thick into rectangular 12 x 8 inch pan. Cut in half lengthwise then crosswise into 16 strips. Dip each strip on both sides in melted butter. Place close together in two rows. Bake 15-20 minutes until golden brown. \n",
      "Serve piping hot.\n",
      "Cook time: PT20M\n",
      "Preparation time: PT1H5M\n",
      "Total time: PT1H25M<|endoftext|>\n",
      "<|startoftext|>Prompt: bittersweet chocolate, butter, sugar, flour, eggs, vanilla extract, rum, filter coffee\n",
      "Title: Brownies\n",
      "Ingredients: 150 bittersweet chocolate,  1 butter,  2 sugar,  1 flour,  4 eggs,  50 vanilla extract,  rum,  filter coffee\n",
      "Servings: 12\n",
      "Instructions: Preheat oven. It must be hot to begin the baking. Cut the chocolate and the butter into small pieces and put together in a pan. Cook until butter and chocolate are well melted and mixed (one  or two minutes). Don't let the chocolate burn or it will become too much bitter! I suggest to use always wooden tablespoons if possible. Add the two cups of sugar and let the preparation get cold. Incorporate the eggs mixing very well after each one of them. Parfum with the vanilla the rum and the coffee. Add the nuts and the flour mixing always very well. \n",
      "Put this heavy cream into a rectangular bakery pan and bake it for approximately 30 minutes. Serve cold (room temperature).          Serves: about 12 big brownies (for tea time)                 about 24 small ones  (for coffee after a meal).\n",
      "Cook time: PT30M\n",
      "Preparation time: PT35M\n",
      "Total time: PT1H5M<|endoftext|>\n",
      "<|startoftext|>Prompt: eggs, country sausage, flour tortillas\n",
      "Title: Breakfast Burritos\n",
      "Ingredients: 8 eggs,  1 country sausage,  2 flour tortillas\n",
      "Servings: 4\n",
      "Instructions: Scramble eggs with a small amount of milk. In another pan crumble sausage and brown. Combine with cheese; cook over low flame until cheese melts. Mix in just enough ketchup until mixture holds together. Warm tortillas in microwave on medium for about 30 seconds until soft enough to roll up without cracking. Place about 1/4 cup egg mixture in tortilla and roll up. Serve with salsa.\n",
      "Cook time: PT0M\n",
      "Preparation time: PT35M\n",
      "Total time: PT35M<|endoftext|>\n",
      "<|startoftext|>Prompt: margarine, milk, sugar, marshmallows, coconut, sugar, margarine, evaporated milk, vanilla flavoring, marshmallow creme\n",
      "Title: Almond Joy Cake\n",
      "Ingredients: 1 margarine,  1/2 milk,  3/4 sugar,  3/4 marshmallows,  24 coconut,  14 sugar,  3 margarine,  3/4 evaporated milk,  2/3 vanilla flavoring,  1 marshmallow creme\n",
      "Servings: 12\n",
      "Instructions: Cake:  Bake according to directions on Super Moist Betty Crocker German Chocolate Cake Mix in 9x13 pan. Grease and flour pan. After allowing cake to to cool thoroughly slice in half making two thin layers. Lay top half aside carefully. Center:  Cut up marshmallows with kitchen shears (rub blades with margarine to prevent  sticking). Boil margarine milk and sugar together for 3 minutes.   Pour over marshmallows until melted.   Add coconut and stir. Spread this mixture over bottom half of cake. Sprinkle generously with chopped almonds if desired. \n",
      "Replace top half of cake. Icing:  Combine sugar milk and margarine cook to soft ball stage or until candy  thermometer reaches 238 degrees.  Stir constantly to prevent scorching.    Remove  from heat stir in chocolate chips add marshmallow creme and vanilla flavoring.   Beat until well blended. Cool (not very much). Spread on cake. Sprinkle entire  cake generously with almond  pieces. Cake is better if it sits overnight.\n",
      "Cook time: PT1H\n",
      "Preparation time: PT20M\n",
      "Total time: PT1H20M<|endoftext|>\n",
      "<|startoftext|>Prompt: fresh lemon juice, water, sugar\n",
      "Title: Almond Paste\n",
      "Ingredients: 1 fresh lemon juice,  3 water,  1 sugar\n",
      "Servings: 1\n",
      "Instructions: Force blanched almonds through fine blade of food chopper 4x or whirl in electric blender. Add lemon juice. Cook water and sugar until candy thermometer registers 240*F or until a small amount of mixture dropped into cold water forms a soft ball. Add to ground almonds. Mix well. When cool enough to handle knead until smooth. Cool. Pack in a jar cover & store in refrigerator for at least 1 week to ripen. Makes about 2 pounds. NOTE: If almond paste is too stiff to handle after storage place in top part of double boiler and heat over hot NOT boiling water until sufficiently soft to handle. Use in cookies coffee cakes or in other pastries and desserts.\n",
      "Cook time: PT168H\n",
      "Preparation time: PT15M\n",
      "Total time: PT168H15M<|endoftext|>\n",
      "<|startoftext|>Prompt: brown rice, chicken broth, unsalted butter, oregano, marjoram, summer savory, unsalted butter, celery, fresh snow pea, broccoli floret, green onion, lemons, coconut\n",
      "Title: Brown Rice and Vegetable Pilaf\n",
      "Ingredients: 1 1/2 brown rice,  3 chicken broth,  3 unsalted butter,  1/2 oregano,  1/2 marjoram,  1/2 summer savory,  5 unsalted butter,  1 celery,  1 fresh snow pea,  1 broccoli floret,  1 green onion,  1 1/2 lemons,  2 coconut\n",
      "Servings: 6\n",
      "Instructions: Preheat oven to 325 degrees F. Heat chicken broth to boiling. Combine broth   with brown rice butter oregano marjoram and summery  savory in 3 quart  casserole and bake covered for 1-1/2 hours or until  rice is tender. Keep  hot.   Melt butter in large skillet or dutch oven. Add celery snow peas  broccoli green onion and ginger and stir fry until crisp-tender. Add  vegetables lemon rind and toasted coconut to rice.  Toss lightly  to combine.  Adjust seasonings.\n",
      "Cook time: PT2H\n",
      "Preparation time: PT30M\n",
      "Total time: PT2H30M<|endoftext|>\n",
      "<|startoftext|>Prompt: oranges, grapefruit juice, seedless grapes, honey, oranges, grapefruit section, lemon juice, lime juice, walnuts, apple\n",
      "Title: Brown Bag Apple Salad\n",
      "Ingredients: 1/2 oranges,  1 grapefruit juice,  1 seedless grapes,  1 honey,  1 oranges,  1/4 grapefruit section,  1 lemon juice\n",
      "Servings: 4\n",
      "Instructions: In medium-size bowl stir together orange juice honey and lemon juice. Add apples grapes orange sections and walnuts; toss to coat with juice mixture. Refrigerate or pack into individual containers for lunches and snacks.\n",
      "Cook time: PT0M\n",
      "Preparation time: PT10M\n",
      "Total time: PT10M<|endoftext|>\n",
      "<|startoftext|>Prompt: blue cheese, cream cheese, half-and-half, mushrooms, pecan halves, fresh basil leaves\n",
      "Title: Blue Stuffed Mushrooms\n",
      "Ingredients: 4 blue cheese,  4 cream cheese,  1 half-and-half,  14 mushrooms,  14 pecan halves,  14 fresh basil leaves\n",
      "Servings: 4\n",
      "Instructions: In a medium bowl combine the blue cheese and the cream cheese. Beat them together with an electric mixer so that they are smooth. Add the half & half and beat it in so that the mixture is fluffy. Fill the mushroom caps with the cheese mixture. Top each with a pecan half and a basil leaf.\n",
      "Cook time: PT0M\n",
      "Preparation time: PT15M\n",
      "Total time: PT15M<|endoftext|>\n",
      "<|startoftext|>Prompt: egg, corn syrup, lemon juice, vanilla, brown sugar, all-purpose flour, blueberries\n",
      "Title: Blueberry Buttertarts\n",
      "Ingredients: 12 egg,  1 corn syrup,  1/2 lemon juice,  1 vanilla,  3/4 brown sugar,  1/3 all-purpose flour,  1 blueberries\n",
      "Servings: 12\n",
      "Instructions: Preheat oven to 375F. Leave shells in foil cups and place on a baking sheet. In  large bowl whisk egg with syrup juice and vanilla. In another bowl stir sugar with flour. Divide berries among shells. Pour in egg mixture filling each shell right to top. Bake on bottom rack at 375F oven until bubbly and top is a bit  crusty from 18-23 minutes.Tarts will firm up when cool and can be refrigerated  2 days.\n",
      "Cook time: PT25M\n",
      "Preparation time: PT15M\n",
      "Total time: PT40M<|endoftext|>\n",
      "<|startoftext|>Prompt: tomato sauce with onion, basil leaves, oregano leaves, Italian pork sausage, water, olive oil, mozzarella cheese, romano cheese, parmesan cheese\n",
      "Title: Calzone\n",
      "Ingredients: 1 tomato sauce with onion,  1 basil leaves,  1 oregano leaves,  1 Italian pork sausage,  2 water,  olive oil,  3 mozzarella cheese,  2 1/4 romano cheese,  1 1/2 parmesan cheese\n",
      "Servings: 4\n",
      "Instructions: Open rolls and let stand until they reach room temperature. Heat tomato sauce  with basil and oregano; set aside. Simmer sausages in water to cover for 20  minutes; drain cool remove the casings and slice thinly. For each Calzone  compress half the rolls into a flat cake and roll on a floured board to make an  11-inch diameter circle. Brush lightly with oil and spread half the tomato sauce over half the dough circle to within 1/2 inch of edge. Top sauce with half the  sausage; sprinkle with half the Mozzarella cheese and Romano cheese. Fold plain  half over filling to within 1/4 inch of opposite edge. Roll bottom edge up over  top edge; pinch or crimp together to seal. \n",
      "Brush with oil. Repeat to make second Calzone. Transfer both Calzones with a wide spatula to a greased baking sheet  placing slightly apart. Pierce tops in a few places with a fork. Bake in a 500  degrees oven for 6 minutes or until golden brown. *Any brand of biscuits refrigerated dough may be substituted.\n",
      "Cook time: PT6M\n",
      "Preparation time: PT35M\n",
      "Total time: PT41M<|endoftext|>\n",
      "<|startoftext|>Prompt: garlic, egg tomatoes, olive oil, olive oil, red capsicums, yellow capsicum, fresh basil leaf, red wine vinegar, brown sugar, olive oil\n",
      "Title: Capsicum, Tomato and Crunchy Bread Salad\n",
      "Ingredients: garlic,  1 egg tomatoes,  9 olive oil,  1/4 olive oil,  2 red capsicums,  2 yellow capsicum,  1 fresh basil leaf,  1 red wine vinegar,  1 brown sugar,  2 olive oil\n",
      "Servings: 6\n",
      "Instructions: DRESSING:Separate garlic bulbs into cloves don't peel. Combine garlic tomatoes and oil in baking dish. Bake uncovered in moderately hot oven about 30 minutes or until tomatoes are soft gently turning tomato occasionally; cool. Reserve garlic for the dressing. Cut bread into 2 cm squares combine in a bowl with extra olive oil; mix well. Place bread in single layer on oven tray toast in moderate oven about 10 minutes. Quarter capsicums remove seeds and membranes. Grill capsicums skin side up until skin blisters and blackens. \n",
      "Peel away skin slice capsicums into thick strips. Combine tomatoes capsicums basil in a large bowl; toss gently. Add crunchy bread to salad just before serving. DRESSING Squeeze pulp from garlic.  Blend with vinegar and sugar until smooth. Add oil gradually in a thin stream while motor is running. Blend until thick.\n",
      "Cook time: PT40M\n",
      "Preparation time: PT1H10M\n",
      "Total time: PT1H50M<|endoftext|>\n",
      "<|startoftext|>Prompt: cumin seeds, fennel seeds, garam masala, mango powder, black salt, cayenne pepper, asafoetida powder, ginger\n",
      "Title: Chaat Masala\n",
      "Ingredients: 2 cumin seeds,  1 1/2 fennel seeds,  1 garam masala,  1 mango powder,  1 black salt,  1 cayenne pepper,  1 asafoetida powder,  1/4 ginger\n",
      "Servings: 1\n",
      "Instructions: oast and grind cumin and fennel seeds and combine with remaining ingredients.\n",
      "Cook time: PT0M\n",
      "Preparation time: PT25M\n",
      "Total time: PT25M<|endoftext|>\n",
      "<|startoftext|>Prompt: milk, eggs, evaporated milk, butter, sugar, vanilla, nutmeg, cinnamon, butter, sugar, water, butter, sugar, water\n",
      "Title: Bread Pudding with Jack Daniels Sauce\n",
      "Ingredients: 1 milk,  1 1/3 eggs,  3 evaporated milk,  4 butter,  1/4 sugar,  2/3 vanilla,  2 nutmeg,  cinnamon,  1/2 butter,  1/3 sugar,  1/4 water,  1/4 butter,  1/4 sugar,  1 water\n",
      "Servings: 8\n",
      "Instructions: Cube bread. Set aside in a large mixing bowl. Combine all ingredients except butter. Mix well and pour over bread. Soak bread in milk mixture for 15 minutes. Pour into a 13x9 buttered pan. Top bread mixture with cut up butter. Bake at 325 for 60-90 minutes or until pudding mixture has risen 1 inch. Serve warm Sauce over bread pudding. Sauce combine ingredients and add sugar and water until dissolved. Add butter and simmer until melted. Cook over high heat of 2 minutes. \n",
      "Add Jack Daniels and simmer for 3-5 minutes.\n",
      "Cook time: PT1H52M\n",
      "Preparation time: PT50M\n",
      "Total time: PT2H42M<|endoftext|>\n",
      "<|startoftext|>Prompt: onions, gingerroot, garlic, green serrano chilies, tomatoes, black cardamom pods, cinnamon sticks, cloves, black peppercorns, bay leaves, cumin, cayenne pepper, turmeric, plain yogurt, dark rum, salt, cilantro, chicken pieces\n",
      "Title: Braised Chicken with Onions and Tomatoes\n",
      "Ingredients: 3 onions,  4 gingerroot,  1 garlic,  8 green serrano chilies,  2 -3 tomatoes,  5 black cardamom pods,  3 -4 cinnamon sticks,  2 cloves,  5 -6 black peppercorns,  6 -8 bay leaves,  2 cumin,  3 cayenne pepper,  2 turmeric,  1 plain yogurt,  1/4 dark rum,  1/2 salt,  1/4 cilantro,  1 chicken pieces\n",
      "Servings: 1\n",
      "Instructions: Heat oil in large saucepan over high heat. Add onions and sauti 2 minutes. Stir in ginger garlic and chiles. Add chicken pieces and brown lightly. Add tomatoes and remaining spices yogurt rum and salt. Reduce heat to low and braise 40 minutes until chicken is cooked through. Serve with cilantro and rice. With leftovers. Freezes well.\n",
      "Cook time: PT42M\n",
      "Preparation time: PT20M\n",
      "Total time: PT1H2M<|endoftext|>\n",
      "<|startoftext|>Prompt: potatoes, flour, half-and-half, cheddar cheese, parmesan cheese, paprika\n",
      "Title: Cheesy Scalloped Potato Side Dish\n",
      "Ingredients: 5 potatoes,  1/4 flour,  1 1/2 half-and-half,  1 1/2 cheddar cheese,  1/2 parmesan cheese,  paprika\n",
      "Servings: 1\n",
      "Instructions: Layer potatoes flour milk and salt and pepper (if desired) in a large casserole dish in several layers then top with cheeses and paprika. Bake at 350°F for 1 1/2 hours. Optional crockpot method:  Spray crock pot with non-stick spray.  Layer ingredients in crockpot as directed for oven method.  Cook on high about 5 hours until potatoes are tender. All cooking times are approximate.  Preparation time allows for peeling and slicing potatoes.  Cooking time is for oven method.\n",
      "Cook time: PT1H30M\n",
      "Preparation time: PT25M\n",
      "Total time: PT1H55M<|endoftext|>\n",
      "<|startoftext|>Prompt: cabbage, onion, celery, butter, beets, tomato sauce, brown sugar, bay leaf, rosemary, lemon juice\n",
      "Title: Borscht\n",
      "Ingredients: 2 cabbage,  1 onion,  1 celery,  2 butter,  1 beets,  1 tomato sauce,  1 brown sugar,  1 bay leaf,  1 rosemary,  2 lemon juice\n",
      "Servings: 1\n",
      "Instructions: Saute cabbage onion and celery in butter until soft in large saucepan. Add  tomato sauce beets with juice herbs sugar and enough water to cover  ingredients. Bring to boil reduce heat simmer 25-30 minutes. Add lemon  juice  a few minutes before serving. Serve with chopped boiled eggs sour cream  and pumpernickel bread.\n",
      "Cook time: PT30M\n",
      "Preparation time: PT20M\n",
      "Total time: PT50M<|endoftext|>\n",
      "<|startoftext|>Prompt: flour, ground beef, onion, tomato sauce, ketchup, salt, pepper, cheddar cheese\n",
      "Title: Cheeseburger Casserole\n",
      "Ingredients: 1 1/2 flour,  1 -2 ground beef,  1/4 onion,  1 tomato sauce,  1/2 ketchup,  1 salt,  1/8 pepper,  1 cheddar cheese\n",
      "Servings: 6\n",
      "Instructions: Combine ground beef and flour in skillet. Add onion and brown. Pour off drippings. Add tomato sauce ketchup salt and pepper. Pour into 1 1/2-quart casserole dish. Bake for 10 minutes at 425°F. Top with cheese and biscuits. Bake for 10-15 minutes longer.\n",
      "Cook time: PT25M\n",
      "Preparation time: PT15M\n",
      "Total time: PT40M<|endoftext|>\n",
      "<|startoftext|>Prompt: halibut steaks, extra virgin olive oil, mint leaves, garlic clove\n",
      "Title: Caputo's Halibut With Mint and Balsamic Vinegar\n",
      "Ingredients: 4 halibut steaks,  1/4 extra virgin olive oil,  1/4 mint leaves,  8 garlic clove\n",
      "Servings: 4\n",
      "Instructions: Brush both sides of the fish with some of the olive oil and place on hot grill barbecue or hot skillet. Cook 2 to 3 minutes on each side or until  fish is done. In another pan heat the remaining oil balsamic vinegar and the mint until just warm. Just before serving add the garlic to the sauce and spoon over the fish.\n",
      "Cook time: PT0M\n",
      "Preparation time: PT10M\n",
      "Total time: PT10M<|endoftext|>\n",
      "<|startoftext|>Prompt: beef, black pepper, fresh parsley, Spanish onion, salt, cajun-style stewed tomatoes, dried thyme leaves\n",
      "Title: Cajun Beef Burgers\n",
      "Ingredients: 1 beef,  1/4 black pepper,  1/8 fresh parsley,  1 Spanish onion,  4 salt,  2 -4 cajun-style stewed tomatoes,  2 dried thyme leaves\n",
      "Servings: 4\n",
      "Instructions: To make sauce: In large saucepan heat oil over medium heat until hot. Add onion and salt; cook and stir over low heat 10 minutes. Add tomatoes and thyme; bring  to a broil. Reduce heat; simmer 8 to 10 minutes or until sauce thickens  slightly. Meanwhile shape ground beef into four 1/2-inch thick patties. Sprinkle both sides of patties with black pepper and red pepper; brush lightly with 1  teaspoon oil. Heat large heavy nonstick skillet over medium heat 5 minutes. Place patties in skillet; panbroil 10 to 12 minutes or until centers are no  longer pink turning once. Spoon approx. \n",
      "1/2 of sauce onto bottom halves or  rolls; top with patties. Spoon remaining sauce over patties; sprinkle with  parsley. Close with roll tops. Serve immediately. Makes 4 servings (serving size: 1 sandwich). Tip: Mexican-style stewed tomatoes may be substituted for Cajun-style. To grill place patties on grid over medium ash-covered coals. Grill uncovered 11 to 13 minutes or until centers are no longer pink turning once.\n",
      "Cook time: PT50M\n",
      "Preparation time: PT40M\n",
      "Total time: PT1H30M<|endoftext|>\n",
      "<|startoftext|>Prompt: flour, salt, cornmeal, shortening, cheddar cheese, water, eggs, sugar, butter, flour, buttermilk, lemon zest, lemon juice, sugar\n",
      "Title: Buttermilk Pie in Cornmeal Pastry\n",
      "Ingredients: 1 flour,  1/2 salt,  1/2 cornmeal,  1/2 shortening,  1/2 cheddar cheese,  1/4 water,  3 eggs,  1 sugar,  1 butter,  1/4 flour,  2 buttermilk,  1/4 lemon zest,  2 lemon juice,  3 sugar\n",
      "Servings: 8\n",
      "Instructions: For Pastry: Sift together flour and salt; stir in cornmeal. Cut in shortening until mixture resembles fine crumbs. Stir in grated Cheddar cheese; sprinkle water over mixture gradually mixing lightly with fork. Shape into ball; flatten on lightly floured surface. Roll to about 1/8 thickness. Line 9 pie pan; trim and flute edge. Fill and bake as directed below. For Filling: Separate 3 eggs and set whites aside for use in meringue. Beat yolks adding sugar gradually. Cut butter into flour; add buttermilk lemon peel and juice. Fold in yolks. \n",
      "Pour into 9 Cornmeal Pie Shell. Bake in hot oven (425°F) 10 minutes; reduce temperature to 350°F and bake 20 to 25 minutes. Cool. To make meringue: Beat 4-5 egg whites in a large bowl with electric mixer on high until foamy. Gradually add sugar beating until stiff peaks form. (Amount of sugar actually used will depend on the humidity in the air). Pile meringue lightly over cooled Butterrmilk filling. Bake in moderate oven (350°F.) 12 to 15 minutes.\n",
      "Cook time: PT40M\n",
      "Preparation time: PT1H\n",
      "Total time: PT1H40M<|endoftext|>\n",
      "<|startoftext|>Prompt: lamb, parsley, fresh thyme, lemon, rind of, garlic, cracked black pepper, butter, lemon juice, small potato, shallots, olive oil\n",
      "Title: Butterflied Lamb with Garlic Butter\n",
      "Ingredients: 2 lamb,  1/4 parsley,  1 fresh thyme,  1 lemon,  4 rind of,  1 garlic,  90 cracked black pepper,  1/4 butter,  1 lemon juice,  250 small potato,  2 shallots\n",
      "Servings: 4\n",
      "Instructions: Open lamb out flat place the fat-side down on a board.  Using a meat mallet or  rolling pin pound the lamb to even thickness. Combine herbs rind garlic  pepper and butter in a small bowl; spread the mixture over the fat-side of the  lamb. Place lamb in a large shallow dish drizzle the juice over.  Cover  refrigerate 3 hours or overnight. Place the potatoes and onions in a large  roasting dish; drizzle with oil.  Bake uncovered in a moderately hot oven 20  minutes. Place lamb fat-side up over the vegetables in the roasting dish.  Bake uncovered in moderately hot oven about 40 minutes or until lamb doneness as  desired. \n",
      "Remove the lamb from the dish cover loosely with foil to keep warm. Drain the excess juices from the pan bake potatoes and onions in a very hot  oven another 15 minutes or until crisp. Slice lamb and serve with the  vegetables.\n",
      "Cook time: PT4H15M\n",
      "Preparation time: PT45M\n",
      "Total time: PT5H<|endoftext|>\n",
      "<|startoftext|>Prompt: beets, water, fresh dill, sour cream\n",
      "Title: Borsch\n",
      "Ingredients: 6 -8 beets,  8 water,  1 fresh dill,  1 sour cream\n",
      "Servings: 4\n",
      "Instructions: Scrub well beets (DO NOT PEEL THEM)  Put in large pot with 8 cups water. Bring  to boil and then reduce to  simmer and cook until tender. (SAVE COOKING BEET  WATER). When tender take slotted spoon and put beets immediately into cold  water. Slip skins off with your hands. If you have a cuisinart chop beets into tiny pieces. If no cuisinart then you will have to chop the beets up by hand. Put chopped beets into pot add fresh dill chopped 1 can Campbell's Tomato  Zest soup (undiluted) and 6 cups of the cooking beet water. \n",
      "If you want a  thinner borscht use a little more of the beet water. Stir well. Serve with 1  tbsp.  sour cream per individual serving. Add salt to your own taste.  I don't  use any salt.  ENJOY!\n",
      "Cook time: PT0M\n",
      "Preparation time: PT1H\n",
      "Total time: PT1H<|endoftext|>\n",
      "<|startoftext|>Prompt: butter, flour, milk, chicken broth, eggs, parmesan cheese, butter, onion, garlic, chicken, broccoli, parsley, mozzarella cheese, parmesan cheese, salt, nutmeg\n",
      "Title: Chicken and Broccoli Lasagna\n",
      "Ingredients: 4 butter,  4 flour,  2 milk,  1 chicken broth,  3 eggs,  1/2 parmesan cheese,  butter,  2 onion,  1 garlic,  1 chicken,  1 1/2 broccoli,  2 parsley,  1/4 mozzarella cheese,  2 parmesan cheese,  1/4 salt,  1/2 nutmeg\n",
      "Servings: 9\n",
      "Instructions: Sauce: Melt butter stir in flour.  Add milk whisk until smooth.  Stir in broth.  Cook stirring constantly until thick. Beat eggs in   separate bowl add small amount of sauce; then combine egg mix with   all sauce. Stir in cheese and seasonings. Cook noodles. Saute chicken in butter with onion and garlic. Stir in broccoli cook 5 min.  until tender.  Add seasonings. Layer in 9 x 13 pan: sauce mozzarella noodles chicken. Repeat. Finish with cheese. Freeze. Bake 350 for 50 min until bubbly.\n",
      "Cook time: PT55M\n",
      "Preparation time: PT15M\n",
      "Total time: PT1H10M<|endoftext|>\n",
      "<|startoftext|>Prompt: all-purpose flour, whole wheat flour, baking powder, baking soda, ginger, salt, carrot, plain low-fat yogurt, canola oil, honey\n",
      "Title: Carrot Ginger Biscuits\n",
      "Ingredients: 1 1/2 all-purpose flour,  1/2 whole wheat flour,  1 1/2 baking powder,  1 1/2 baking soda,  1 1/2 ginger,  1/2 salt,  1/2 carrot,  2 plain low-fat yogurt,  2 canola oil,  2 honey\n",
      "Servings: 12\n",
      "Instructions: Preheat oven to 450. In a large bowl stir together the all-purpose flour and  whole wheat flour baking powder and baking soda ginger and salt.  In a small  bowl stir together the carrot yogurt oil and honey. Add carrot mixture to  dry ingredients and stir until just combined. Place the dough on a well-floured surface.  Using a lightly floured rolling pin roll out the dough until 1/2 thick. Using a 2 1/4 biscuit cutter or glass   cut out rounds gently rerolling the scraps to form 12 biscuits. \n",
      "Place the  biscuits on an ungreased baking sheet spacing them 1 apart.  Bake for 12 to 15 minutes or until golden. Serve warm.\n",
      "Cook time: PT15M\n",
      "Preparation time: PT35M\n",
      "Total time: PT50M<|endoftext|>\n",
      "<|startoftext|>Prompt: boneless skinless chicken breast halves, ham slices, swiss cheese, all-purpose flour, parmesan cheese, salt, dried sage, pepper, dry white wine, cornstarch, water, rice\n",
      "Title: Chicken Breasts Saltimbocca\n",
      "Ingredients: 6 boneless skinless chicken breast halves,  6 ham slices,  6 swiss cheese,  1/4 all-purpose flour,  1/4 parmesan cheese,  1 salt,  1/2 dried sage,  1/4 pepper,  1/3 dry white wine,  1 cornstarch,  1/2 water,  1/4 rice\n",
      "Servings: 6\n",
      "Instructions: Pound chicken breast halves until thin between  two sheets of waxed paper or  foil. Place a slice of ham and cheese on each chicken piece.  Roll up and tuck  ends in; secure with small skewers or  wooden picks. Combine flour Parmesan  cheese salt sage and pepper in a shallow bowl. Coat chicken rolls in flour  mixture.  Refrigerate chicken at least 1 hour. In a large skillet heat oil  over medium heat.  Add chicken rolls and cook turning until browned on all  sides. Place browned chicken in a slow cooker. \n",
      "Combine soup and wine and pour  over chicken rolls. Cover and cook on LOW 4 to 5 hours or until chicken is  tender. Turn control to HIGH. In a small bowl dissolve cornstarch in water;  stir into cooking juices in cooker. Cover and cook on HIGH 10 minutes. Serve  with hot rice.\n",
      "Cook time: PT6H\n",
      "Preparation time: PT1H\n",
      "Total time: PT7H<|endoftext|>\n",
      "<|startoftext|>Prompt: littleneck clams, extra virgin olive oil, garlic, Canadian bacon, smoked ham, dry white vermouth, dry white wine, bottled clam broth, baking potato, spaghetti, flat leaf parsley\n",
      "Title: A New Spaghetti with Clams\n",
      "Ingredients: 36 littleneck clams,  1 extra virgin olive oil,  3 garlic,  1 Canadian bacon,  1/4 smoked ham,  1/2 dry white vermouth,  1 dry white wine,  1 bottled clam broth,  8 baking potato,  1/2 spaghetti\n",
      "Servings: 1\n",
      "Instructions: Bring 4 quarts of water to a boil in a large pot for cooking pasta. Scrub the clam shells under cold water with a stiff bristle brush. Heat the olive oil in a large nonstick skillet. Add the garlic and cook over a medium heat until fragrant but not brown about 1 minute. Stir in the bacon and pepper flakes and cook for 1 minute. Add vermouth and bring to a boil. Add the clam broth and bring to a boil. Stir in the potatoes and clams tightly cover the pan and cook until the shells open and the potatoes are tender about 8 minutes. \n",
      "Cook the spaghetti in the boiling water until al dente about 8 minutes. Drain the pasta in a colander. Stir it into the sauce and cook until thoroughly heated and coated with sauce about 2 minutes. Stir in the parsley and serve at once.\n",
      "Cook time: PT20M\n",
      "Preparation time: PT20M\n",
      "Total time: PT40M<|endoftext|>\n",
      "<|startoftext|>Prompt: all-purpose flour, sugar, baking powder, salt, unsalted butter, fresh blueberries, lemon, zest of, heavy cream, eggs\n",
      "Title: Blueberry Scones\n",
      "Ingredients: 2 all-purpose flour,  3 sugar,  1 baking powder,  3/4 salt,  6 unsalted butter,  1 1/2 fresh blueberries,  1 lemon,  1/3 zest of,  2 heavy cream\n",
      "Servings: 8\n",
      "Instructions: Adjust rack to center of oven and heat to 400 degrees. Place a Silpat baking mat on a baking sheet and set aside. In a large bowl sift together flour 3 tablespoons sugar baking powder and salt. Using a pastry blender or two knives cut in butter until the largest pieces are the size of small peas. Stir in blueberries and zest. Using a fork whisk together cream and egg in a liquid measuring cup. Make a well in the center of dry ingredients and pour in cream mixture. Stir lightly with fork just until dough comes together. \n",
      "Turn out onto a lightly floured surface and knead a few times to mix well. Pat dough into a 6-inch square about 1 1/4 inches thick. Using a floured knife cut into four 3-inch squares. Cut squares in half on the diagonal to form eight triangles. Transfer to prepared baking sheet. Brush tops with cream and sprinkle with sugar. Bake until golden brown 20 to 22 minutes. Transfer scones from baking sheet to wire racks to cool.\n",
      "Cook time: PT22M\n",
      "Preparation time: PT55M\n",
      "Total time: PT1H17M<|endoftext|>\n",
      "<|startoftext|>Prompt: flour, baking powder, salt, vegetable shortening, sugar, egg, milk, lemon zest, blueberries\n",
      "Title: Blueberry Cookies\n",
      "Ingredients: 2 flour,  2 baking powder,  1/2 salt,  1/2 vegetable shortening,  1 sugar,  1 egg,  1/4 milk,  1 lemon zest,  1 1/2 blueberries\n",
      "Servings: 48\n",
      "Instructions: Combine flour baking powder and salt. In a large bowl cream the shortening and  sugar. Beat in the egg. Beat in the milk almond extract and lemon rind. Gradually blend in the dry ingredients. Fold in the blueberries. Cover and chill for at least 4 hours.Preheat oven to 375°. Drop dough by spoonfuls about  1-inch apart onto ungreased cookie sheets. Bake for 12-15 minutes until pale golden. Transfer to wire racks to cool. NOTES: Canned or frozen blueberries can be used but fresh gives the best results.\n",
      "Cook time: PT15M\n",
      "Preparation time: PT4H\n",
      "Total time: PT4H15M<|endoftext|>\n",
      "<|startoftext|>Prompt: butter, sugar, cinnamon, nutmeg, all-purpose flour, tart apples, butter, eggs, milk, rum, vanilla extract, pecans, baking powder, baking soda, salt\n",
      "Title: Abby's Pecan Apple Cake\n",
      "Ingredients: 2 butter,  1 1/2 sugar,  1/2 cinnamon,  1/2 nutmeg,  1 1/2 all-purpose flour,  3 tart apples,  1/2 butter,  2 eggs,  1/3 milk,  2 rum,  2 vanilla extract,  2 pecans,  1 1/2 baking powder,  1 1/2 baking soda,  1 1/2 salt\n",
      "Servings: 10\n",
      "Instructions: *  such as Granny Smith peeled halved and sliced (3 cups)  Preheat the oven to 350 degrees. Brush the sides of a 8 x 3 1/4-inch  springform pan with the melted butter. Mix together 1/2 cup sugar cinnamon  nutmeg and 1/4 cup flour and sprinkle the mixture evenly over the bottom of  the pan. Wrap foil around the pan to prevent leakage. Starting at the outside  edge arrange a ring of apple slices in the pan slightly overlapping and  pointing to the center. (It will feel backwards.) Fill in the center with  another circle of apples with some overlap occurring. \n",
      "Layer any remaining  apple slices evenly overlapping to prevent the batter from escaping. With a  wooden spoon or electric mixer beat together the butter and 1 cup sugar. Add  the eggs milk rum and vanilla. The batter will look curdled. Add 1 1/4  cups flour the nuts baking powder baking soda and salt beating only  until the flour is completely incorporated. Pour the batter over the apples  and spread evenly. Place the pan on a baking sheet and bake in the middle of  the oven until a toothpick inserted in the cake comes out clean about 70  minutes. \n",
      "Cover with a piece of foil if the top begins to brown too quickly. Let the cake rest in the pan on a rack for 5 minutes then using a small  flexible knife gently separate the sides of the cake from the pan. Invert  the cake on the rack letting it stay in the pan for another 10 minutes then  remove the pan lifting it up carefully.\n",
      "Cook time: PT1H10M\n",
      "Preparation time: PT20M\n",
      "Total time: PT1H30M<|endoftext|>\n",
      "<|startoftext|>Prompt: carrot, tahini sesame butter, tofunaise, mayonnaise, raisins\n",
      "Title: Carrot & Tahini Sandwich\n",
      "Ingredients: 1 carrot,  1 -2 tahini sesame butter,  1 tofunaise,  mayonnaise,  raisins\n",
      "Servings: 1\n",
      "Instructions: In small bowl combine all ingredients. Use as sandwich filling or spread.\n",
      "Cook time: PT0M\n",
      "Preparation time: PT15M\n",
      "Total time: PT15M<|endoftext|>\n",
      "<|startoftext|>Prompt: carrot, eggs, sugar, all-purpose flour, salt, baking powder, baking soda, cinnamon, walnuts, cream cheese, sugar, unsalted butter, real vanilla\n",
      "Title: Carrot Cake II\n",
      "Ingredients: 1 carrot,  4 eggs,  2 sugar,  3 all-purpose flour,  1/2 salt,  2 baking powder,  2 baking soda,  2 cinnamon,  1/2 walnuts,  1 1/2 cream cheese,  1 sugar,  1 unsalted butter,  4 real vanilla\n",
      "Servings: 8\n",
      "Instructions: Preheat oven to 325°. Add 1 egg at a time to the grated carrots until all 4 have been added. Beat after each egg  Add the oil and mix. Then sift all of the dry ingredients together and add it to the oil egg and carrot mixture in 4 parts beating after each addition. Next add the walnuts and mix well. Pour into a greased/floured tube pan. Bake for 1 hour at 325° or or when a tooth pick is inserted into the cake and it comes out clean. FOR THE ICING: Soften butter and cheese. Blend well and then add the sugar and vanilla. \n",
      "Beat until smooth and creamy. Frost only after cake is cooled.\n",
      "Cook time: PT1H\n",
      "Preparation time: PT15M\n",
      "Total time: PT1H15M<|endoftext|>\n",
      "<|startoftext|>Prompt: cannellini beans, ketchup, chili powder, thyme, water, bacon bits, lamb chop, onion\n",
      "Title: Lamb and Pork Cassoulet\n",
      "Ingredients: 1 cannellini beans,  1 ketchup,  1 chili powder,  1 thyme,  1/2 water,  1/4 bacon bits,  1 lamb chop,  1 onion\n",
      "Servings: 2\n",
      "Instructions: Saute onion and garlic in oil. Add lamb and pork. Saute 5 minutes until cooked  though. Combine everything in a small deep oven safe dish. Bake uncovered 350   30 minutes. HINTS: for those who like more beans & less meat Add another can of beans & half as much meat. Just use a tad more water. If you liked it or hated it please let me know -- .\n",
      "Cook time: PT1H\n",
      "Preparation time: PT15M\n",
      "Total time: PT1H15M<|endoftext|>\n",
      "<|startoftext|>Prompt: graham crackers, butter, carrots, light cream cheese, granulated sugar, maple syrup, eggs, water, granulated sugar, carrot\n",
      "Title: Carrot Maple Cheesecake\n",
      "Ingredients: 1 1/3 graham crackers,  1/4 butter,  2 carrots,  3 light cream cheese,  1/3 granulated sugar,  1/3 maple syrup,  3 eggs,  1/4 water,  1/4 granulated sugar,  1/2 carrot\n",
      "Servings: 12\n",
      "Instructions: CRUST:  Combine ingredients; mix well. Press into 9 inch springform pan.  Bake in preheated 350 F oven for 10 minutes. FILLING:  Simmer shredded carrots in water to cover for 5 minutes.  Drain and remove excess liquid. In food processor combine carrots cream cheese sugar and maple syrup process until completely smooth and evenly colored. Add eggs one at a time and mix until just blended. Pour onto crumb crust and bake at 300 F for 45 minutes or until center is almost set.  Cool to room temperature.  Refrigerate 3 hours or overnight. . \n",
      "OPTIONAL GARNISH:  In small saucepan simmer water with sugar for 5 minutesAdd shredded carrots and simmer 5 minutes.   Drain well and scatter on top of chilled cheesecake.\n",
      "Cook time: PT1H5M\n",
      "Preparation time: PT45M\n",
      "Total time: PT1H50M<|endoftext|>\n",
      "<|startoftext|>Prompt: plain flour, salt, black pepper, egg, milk, water\n",
      "Title: Catherine's Excellent Yorkshire Pudding\n",
      "Ingredients: 4 plain flour,  1 salt,  1 black pepper,  egg,  1 milk,  1/4 water\n",
      "Servings: 1\n",
      "Instructions: Sieve the flour into mixing bowl. Add the salts and pepper. Make hole in middle  of flour and add egg. Start mixing gently whilst gradually adding the milk and  water. Use small foil cases and put a small amount of cooking oil in each and  place on a high shelf in a preheated oven gas mark 7. ( the fat needs to be hot before you add the mixture ) Pour equal amounts of the mixture into each case  and place in the oven Cook for 20-25 mins Serve with all roast meats ( don't  forget the lashings of gravy ) Please enjoy   from Cathy the Northern Cook.\n",
      "Cook time: PT20M\n",
      "Preparation time: PT15M\n",
      "Total time: PT35M<|endoftext|>\n",
      "<|startoftext|>Prompt: black-eyed peas, onion, garlic cloves, cumin, thyme, bay leaf, long-grain rice, vegetable broth, tomatoes, allspice, salt\n",
      "Title: Caribbean Cowpeas and Rice\n",
      "Ingredients: 1 black-eyed peas,  1 onion,  1 garlic cloves,  2 cumin,  1/2 thyme,  1 bay leaf,  1 long-grain rice,  1 vegetable broth,  1 tomatoes,  1/2 allspice,  1/2 salt\n",
      "Servings: 4\n",
      "Instructions: Soak peas. COmbine peas with onion jalapeno garlic cumin thyme bay leaf and 2 cups water. Boil. Reduce and simmer. Add rice broth tomato allspice and salt.\n",
      "Cook time: PT0M\n",
      "Preparation time: PT25M\n",
      "Total time: PT25M<|endoftext|>\n",
      "<|startoftext|>Prompt: fettuccine, olive oil, garlic, milk, parmesan cheese, fresh dill, fresh chives, nutmeg, salt, pepper, asparagus, smoked salmon, lemon juice\n",
      "Title: Carnation Lean Fettuccine Alfredo\n",
      "Ingredients: 12 fettuccine,  1 olive oil,  1 garlic,  2 milk,  3/4 parmesan cheese,  4 fresh dill,  1 fresh chives,  1 nutmeg,  1 salt,  1 pepper,  6 asparagus,  2/3 smoked salmon,  1 lemon juice\n",
      "Servings: 4\n",
      "Instructions: In a large pot of boiling water cook the fettuccine for 8-10 minutes until  tender but firm. Drain. In a non-stick skillet heat oil and cook garlic for 30  seconds. Add to drained pasta along with milk and cheese. Cook over medium heat stirring gently for four to five minutes until sauce is thickened. Remove from  heat and let stand two to three minutes to thicken more. Stir in dill and chives and season with salt and pepper. Stir in blanched  asparagus salmon and lemon juice. Garnish each serving with more freshly grated Parmesan if desired.\n",
      "Cook time: PT10M\n",
      "Preparation time: PT30M\n",
      "Total time: PT40M<|endoftext|>\n",
      "<|startoftext|>Prompt: short-grain rice, brown sugar, butter, milk, nutmeg\n",
      "Title: Caramel Rice Pudding\n",
      "Ingredients: 1/2 short-grain rice,  1/3 brown sugar,  30 butter,  1 1/2 milk,  300 nutmeg\n",
      "Servings: 4\n",
      "Instructions: Wash the rice well and drain. Mix with remaining ingredients and put in a buttered baking dish and bake in a slow oven 140 C for 2 - 2 1/2 hours until rice is tender and creamy. Stir every 30 minutes during cooking and add more milk if necessary. When ready the milk and cream should be a rich caramel cream and a golden crust should have formed. Serve hot or cold. This can be cooked in individual dishes if wished and just eat from them. You can use brown raw or demerara sugars.\n",
      "Cook time: PT3H\n",
      "Preparation time: PT55M\n",
      "Total time: PT3H55M<|endoftext|>\n",
      "<|startoftext|>Prompt: frozen yogurt, 1% low-fat milk, applesauce, cinnamon\n",
      "Title: Caramel Apple Milkshakes\n",
      "Ingredients: 3 frozen yogurt,  3/4 1% low-fat milk,  1/2 applesauce,  1 cinnamon\n",
      "Servings: 4\n",
      "Instructions: Place the yogurt milk cinnamon and topping in a blender. Cover and blend  until smooth. Pour into 4 glasses. Serve immediately.\n",
      "Cook time: PT0M\n",
      "Preparation time: PT25M\n",
      "Total time: PT25M<|endoftext|>\n",
      "<|startoftext|>Prompt: frozen limeade concentrate, ginger ale\n",
      "Title: Champagne Punch\n",
      "Ingredients: 1 frozen limeade concentrate,  1 ginger ale\n",
      "Servings: 20\n",
      "Instructions: Mix the juice concentrates in punch bowl (do not add water). Stir in the Gingerale then add the Champagne (do not stir after adding the champagne!) ** Note ** The large bottles of Gingerale and Champagne should be equal volumes (I use 2 L.  bottles).\n",
      "Cook time: PT0M\n",
      "Preparation time: PT5M\n",
      "Total time: PT5M<|endoftext|>\n",
      "<|startoftext|>Prompt: green apples, self rising flour, salt, sugar, butter, water, butter, brown sugar, golden syrup, water\n",
      "Title: Butterscotch Apple Dumpling\n",
      "Ingredients: 2 green apples,  1 self rising flour,  1/8 salt,  2 sugar,  2 butter,  2 water,  1 butter,  1 brown sugar,  1 golden syrup,  1 1/4 water\n",
      "Servings: 8\n",
      "Instructions: Sift flour and salt.  Add sugar.  Rub in butter then add water and mix to a soft dough. Peel and quarter apples. Divide pastry into 8 equal portions. Roll out each portion of pastry and wrap each around an apple quarter. Place in an oven proof dish. Combine sauce ingredients in saucepan and bring to boil. Pour sauce over pastries in dish. Bake 25-30 minutes in  a moderate oven about 325 degrees Fahrenheit.  Serve with ice cream.\n",
      "Cook time: PT30M\n",
      "Preparation time: PT45M\n",
      "Total time: PT1H15M<|endoftext|>\n",
      "<|startoftext|>Prompt: buttermilk, honey, butter, margarine, bread flour, rolled oats, salt, active dry yeast, bread machine yeast\n",
      "Title: Buttermilk-Oat Bread\n",
      "Ingredients: 3/4 buttermilk,  1 honey,  1 1/2 butter,  1 3/4 margarine,  1/3 bread flour,  1/2 rolled oats,  1 salt\n",
      "Servings: 16\n",
      "Instructions: For toasted rolled oats place oats in a shallow baking pan.  Bake at 350 for 15 to 20 minutes or till oats are lightly browned stirring occasionally.   Cool. Add all of the ingredients to a bread machine according to manufacturer's  directions.   Bake the bread using the regular or white setting.\n",
      "Cook time: PT20M\n",
      "Preparation time: PT30M\n",
      "Total time: PT50M<|endoftext|>\n",
      "<|startoftext|>Prompt: blueberries, water, lemons, rind of, sugar, lemon juice\n",
      "Title: Blueberry Pancake Syrup\n",
      "Ingredients: 4 blueberries,  3 water,  2 lemons,  3 rind of,  sugar\n",
      "Servings: 1\n",
      "Instructions: Pour the blueberries into a saucepan and crush them with a potato masher or a wooden spoon until most of the skins are broken. Add 1 cup of water  and strips of lemon peel and bring to a simmer. Turn heat down to low and cook the berries for 5 minutes at just under a simmer. Pour the hot berries into a strainer lined with two layers of cheesecloth and let the blueberry juice drip through. Twist the cloth to extract all the juice; there should be about 2 cups. Discard the berry pulp. \n",
      "Combine the remaining  2 cups water with the sugar in a small saucepan. Bring the mixture to a boil stirring until the sugar is dissolved and the mixture is clear. Wash down the sides of the pan with a wet pastry brush then boil the syrup without stirring until it reaches 260 degrees on a candy thermometer. Add the blueberry syrup to the sugar syrup and bring the mixture to boil. Boil for 1 minute.  Let the syrup cool then add lemon juice to taste. Pour the syrup into two pint jars and refrigerate if you plan to use it in a month or two. \n",
      "To can (for indefinite storage) Pour into canning jars with 1/2 inch headspace.  Process in hot water bath for 30 minutes. Cool.\n",
      "Cook time: PT36M\n",
      "Preparation time: PT50M\n",
      "Total time: PT1H26M<|endoftext|>\n",
      "<|startoftext|>Prompt: bacon, olive oil, onions, paprika, cloves, nutmeg, bay leaf, parsley, garlic, piri-piri, red wine\n",
      "Title: Chanfana Ou Lampantana\n",
      "Ingredients: 3 bacon,  120 olive oil,  125 onions,  25 paprika,  2 cloves,  5 nutmeg,  2 bay leaf,  2 parsley,  1 garlic,  1 piri-piri,  3 red wine\n",
      "Servings: 1\n",
      "Instructions: Preheat oven to 200 degrees C.  Place the meat and bacon in a pot and add the oil and lard.  Add the onions spices herbs and seasonings.    Cover completely with the wine. Cover the pot and place it in a very hot oven (200 degrees C) for 1 hour. Remove the lit and stir.   Add more wine if necessary. Cover the pot again and reduce the oven temperature of 100 degrees C. Continue cooking preferably overnight or until the meat is tender. (The time taken will depend on the age of the lamb or goat.)  Serve with potatoes boiled in their Jackets.\n",
      "Cook time: PT4H\n",
      "Preparation time: PT20M\n",
      "Total time: PT4H20M<|endoftext|>\n",
      "<|startoftext|>Prompt: owdered sugar\n",
      "Title: Cherry Sandwich Maker Snack\n",
      "Ingredients: 2 owdered sugar\n",
      "Servings: 1\n",
      "Instructions: Spread a thin layer between buttered bread and place in sandwich maiker. Note: do not add too much filling or it will make one terrible mess in sandwich maker. Let brown. Remove from sandwich maker and sprinkle with powdered sugar. Makes two triangular tarts. My kids love them.\n",
      "Cook time: PT0M\n",
      "Preparation time: PT30M\n",
      "Total time: PT30M<|endoftext|>\n",
      "<|startoftext|>Prompt: cream cheese, sugar, eggs, sour cream, vanilla, lemon, rind of, flour, blueberries\n",
      "Title: Cheesecake Cupcakes with Blueberries\n",
      "Ingredients: 12 cream cheese,  8 sugar,  1 eggs,  2 sour cream,  1/2 vanilla,  1/2 lemon,  1/2 rind of,  3 flour,  1 blueberries\n",
      "Servings: 12\n",
      "Instructions: Heat oven to 325 degrees.Line 12 muffin cups with foil liners. Place a cookie in bottom of each liner; trim cookies to fit if necessary.Beat together cream  cheese and sugar on med. speed in large bowl until smooth. Add eggs beating  just until blended. Beat in sour cream vanilla and lemon rind. On low speed  beat in flour. Stir in blueberries. Spoon batter into cups dividing  equally.Bake for 35-40 min. until cakes are set. Cool cakes in pan on rack for  20 min. Remove cakes to rack to cool. \n",
      "Refrigerate.                    - - - - - - - - - - - - - - - - - -\n",
      "Cook time: PT0M\n",
      "Preparation time: PT50M\n",
      "Total time: PT50M<|endoftext|>\n",
      "<|startoftext|>Prompt: cheese, pineapple\n",
      "Title: Cheese and Pineapple Dip\n",
      "Ingredients: 1 cheese,  1 pineapple\n",
      "Servings: 1\n",
      "Instructions: Mix reduced cream tomato & onion soup cheese and crushed pineapple together until well combined. Chill for 2 hours before serving. Makes  about 3 cups. Cheers  Doreen Doreen Randal  Wanganui. New Zealand.\n",
      "Cook time: PT2H\n",
      "Preparation time: PT20M\n",
      "Total time: PT2H20M<|endoftext|>\n",
      "<|startoftext|>Prompt: barbecued chicken, coconut, unsweetened pineapple slices, spring onions, curry powder, five-spice powder, garlic clove, fresh ginger, mayonnaise, coconut milk\n",
      "Title: Chicken and Pineapple Salad With Curry Mayonnaise\n",
      "Ingredients: 1 barbecued chicken,  1/2 coconut,  1 unsweetened pineapple slices,  4 spring onions,  1 curry powder,  1/4 five-spice powder,  1 garlic clove,  1 fresh ginger,  3/4 mayonnaise,  1/4 coconut milk\n",
      "Servings: 4\n",
      "Instructions: CURRIED MAYONNAISE: Combine all ingredients thoroughly. CHICKEN AND PINEAPPLE SALAD: Toast coconut on oven tray in moderate oven for about 5 minutes. Cut chicken and pineapple into chunks; place in bowl mix in coconut shallots and Mayonnaise. Refrigerate 1 hour before serving. Serves 4. Cheers  Doreen Doreen Randal  Wanganui. New Zealand.\n",
      "Cook time: PT1H5M\n",
      "Preparation time: PT20M\n",
      "Total time: PT1H25M<|endoftext|>\n",
      "<|startoftext|>Prompt: dry yeast, water, honey, white flour, water, garlic clove, tomatoes, tomato sauce, mozzarella cheese, parmesan cheese, romano cheese, olive oil, salt, pepper, oregano, basil\n",
      "Title: Carrie's Pizza Rolls\n",
      "Ingredients: 1 dry yeast,  1/2 water,  1/2 honey,  4 white flour,  1 water,  1 garlic clove,  1 tomatoes,  1 tomato sauce,  1 mozzarella cheese,  1/2 parmesan cheese,  1 romano cheese,  2 olive oil,  1 salt,  1 pepper,  1 oregano,  1 basil\n",
      "Servings: 1\n",
      "Instructions: Dough: Dissolve yeast with honey and 1/2 cup  water; let stand 10 minutes. Put 4 cups flour in large  mixing bowl; stir in oil water and yeast mixture. Mix   thoroughly. Knead 8 to 10 minutes. Let dough rise 1 hour until  doubled. Punch  down and roll on floured surface to about 1/2- inch. Cut into rectangles about 4 x 6-inches or desired size of  pizza roll. Sauce: Place all ingredients (except  the  Mozzarella cheese) in a crock-pot and simmer on low for  about 2 hours. \n",
      "Let cool to room temperature. Hot sauce will  make dough soggy. Rolls: Place a small amount of sauce on the  dough rectangles spreading it to cover the entire   piece of dough. Sprinkle Mozzarella cheese over half of the  dough. Roll up the  dough burrito-style sealing edges with  moistened fingers. Place rolls on a  cookie sheet. Allow to rest for  about 10 minutes. Bake in a hot 500 degrees  oven until  dough is golden brown. Serve rolls with a side bowl of hot pizza  sauce  for dipping the pizza rolls inches. \n",
      "These freeze beautifully. Can be   reheated in the microwave. Variations: You may wish to add slices of  mushroom  olive shredded beef pepperoni slices or  whatever in addition to the  Mozzarella cheese inside the pizza roll.\n",
      "Cook time: PT3H30M\n",
      "Preparation time: PT55M\n",
      "Total time: PT4H25M<|endoftext|>\n",
      "<|startoftext|>Prompt: DOLE® Banana, sugar, margarine, eggs, amaretto liqueur, vanilla extract, all-purpose flour, baking soda, salt\n",
      "Title: Almond Fudge Banana Cake\n",
      "Ingredients: 3 DOLE® Banana,  1 1/2 sugar,  1/2 margarine,  3 eggs,  3 amaretto liqueur,  1 vanilla extract,  1 1/3 all-purpose flour,  1/3 baking soda,  1 salt\n",
      "Servings: 1\n",
      "Instructions: Mash bananas and set aside.  Beat sugar and margarine until light and fluffy. Beat in eggs liqueur and vanilla. Combine dry ingredients. Stir in almonds. Add to sugar mixture alternately with bananas. Beat well. Pour batter into greased 10-inch Bundt pan. Bake in preheated 350°F oven 45 to 50 minutes or until toothpick inserted in center comes out almost clean and cake pulls away from side of pan. Cool 10 minutes. Remove cake from pan to wire rack to cool completely. \n",
      "Drizzle glaze over top and down side of cake.  Make 16-20 servings.\n",
      "Cook time: PT1H\n",
      "Preparation time: PT50M\n",
      "Total time: PT1H50M<|endoftext|>\n",
      "<|startoftext|>Prompt: cherry tomatoes, scallions, parsley, rosemary, garlic, extra virgin olive oil, extra virgin olive oil, garlic, provolone cheese, parmesan cheese\n",
      "Title: Cherry Tomatoes on Provolone Garlic Bread\n",
      "Ingredients: 2 cherry tomatoes,  2 scallions,  1/4 parsley,  1 rosemary,  3 garlic,  1/3 extra virgin olive oil,  3 extra virgin olive oil,  garlic,  3 provolone cheese,  3 parmesan cheese\n",
      "Servings: 4\n",
      "Instructions: For the tomatoes mix tomatoes scallions parsley rosemary garlic olive oil and vinegar in a shallow bowl. Season with salt and pepper. Cover bowl and let tomatoes marinate at room temperature for at least 1 hour but preferably 3-4 hours. Stir occasionally to distribute seasonings. For the bread combine olive oil and garlic and let mixture stand for about 10 minutes so flavors can mingle.  Meanwhile heat the broiler. Brush one side of each bread slice with garlic and olive oil mixture and broil them oiled-side up until lightly browned. \n",
      "Put a slice of provolone and a generous sprinkling of Parmesan on each slice reserving a little of the parmesan for garnish. Set aside.  Just before serving heat up broiler again and toast cheese until bubbly. Serve bread in shallow bowls spooning about 3/4 cup of tomatoes and marinade over each slice garnishing with remaining parmesan.\n",
      "Cook time: PT1H10M\n",
      "Preparation time: PT35M\n",
      "Total time: PT1H45M<|endoftext|>\n",
      "<|startoftext|>Prompt: chicken breasts, olive oil, Velveeta cheese, corn, seasoning salt, parsley flakes, pepper\n",
      "Title: Cheesy Chicken Noodle Casserole\n",
      "Ingredients: 2 chicken breasts,  0.5 olive oil,  1 Velveeta cheese,  1/4 corn,  0.5 seasoning salt,  1 parsley flakes,  1 pepper\n",
      "Servings: 4\n",
      "Instructions: Cook egg noodles in boiling water till chewy. Cook chicken in olive oil and  Lawry salt till chicken is not pink in middle then drain oil. Combine cream of chicken soup egg noodles and chicken till boiling. Reduce heat and add  Velveeta cheese corn and pepper. Cook until all of Velveeta is melted. Remove from heat and sprinkle with parsley flakes.\n",
      "Cook time: PT0M\n",
      "Preparation time: PT35M\n",
      "Total time: PT35M<|endoftext|>\n",
      "<|startoftext|>Prompt: butter, sugar, egg, flour, baking powder, sultana, butter, golden syrup, sugar\n",
      "Title: Caramel Sultana Square\n",
      "Ingredients: 125 butter,  125 sugar,  1 egg,  1 1/2 flour,  2 baking powder,  1 sultana,  butter,  90 golden syrup,  7 sugar\n",
      "Servings: 1\n",
      "Instructions: Cream butter and sugar add egg then dry ingredients. Divide shortcake in two. Press one half in tin. Melt together and pour over base.  Sprinkle with sultanas and cover with remaining mixture.  Bake in a moderate oven 20 - 30 minutes.\n",
      "Cook time: PT30M\n",
      "Preparation time: PT20M\n",
      "Total time: PT50M<|endoftext|>\n",
      "<|startoftext|>Prompt: aracter(0\n",
      "Title: \"\n",
      "Ingredients: pple Pie recipe from Food.com aracter(0\n",
      "Servings: 113\n",
      "Instructions: .\n",
      "Cook time: 67395\n",
      "Preparation time: Dannygirl\n",
      "Total time: PT1H6M<|endoftext|>\n",
      "<|startoftext|>Prompt: ranges\n",
      "Title: Campfire Orange Cake\n",
      "Ingredients: 1 ranges\n",
      "Servings: 6\n",
      "Instructions: You could probably do this with any cake mix.  Foil is nice (keeps ash  out) but apparently not necessary unless you are worried about burning the  bottom of the orange cups.  Follow the directions on the package for the  cake batter and do like below: Gingerbread Cups  Halve and clean inside of oranges out.  Prepare a package of  gingerbread mix fill cups 3/4 full. Place cups in a bed of coals not too  hot.  Bake slowly turning around from time to time.\n",
      "Cook time: PT0M\n",
      "Preparation time: PT30M\n",
      "Total time: PT30M<|endoftext|>\n",
      "<|startoftext|>Prompt: carrot, lettuce, cucumber, radish, raisins, mayonnaise, celery, tuna salad, salmon salad\n",
      "Title: Bugwiches\n",
      "Ingredients: 1 carrot,  lettuce,  cucumber,  radish,  raisins,  mayonnaise,  celery,  tuna salad,  salmon salad\n",
      "Servings: 1\n",
      "Instructions: How to assemble: Slice roll horizontally about 1/2 inch from bottom. Pull out  bread from top half leaving walls about 1/2 inch thick. Place cucumber or  radish slices over bottom halves of rolls sticking out slightly to make mouths  or tongues. Spoon a mound of filling on top and cover with top halves of rolls. Insert carrot sticks into filling for legs. Make two slits on each side of top  half of bun and insert lettuce leaves as wings. Decorate with vegetable pieces  for eyes antennae scales and other body parts. \n",
      "Attach using mayonnaise.\n",
      "Cook time: PT0M\n",
      "Preparation time: PT40M\n",
      "Total time: PT40M<|endoftext|>\n",
      "<|startoftext|>Prompt: cheese, pepper\n",
      "Title: Cheese Rolls\n",
      "Ingredients: 1 cheese,  pepper\n",
      "Servings: 1\n",
      "Instructions: Butter thin slices of white bread cut off crusts. Spread with grated tasty cheese and sprinkle with a little pepper. Roll up as for asparagus rolls. Bake in oven until crisp.\n",
      "Cook time: PT0M\n",
      "Preparation time: PT45M\n",
      "Total time: PT45M<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for recipe in recipes4[20:100]:\n",
    "    print(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5be71112-9716-49a9-b319-ed2a637634b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_list = recipes1 + recipes2 + recipes3 + recipes4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84720437-7c4e-4cd3-b45c-4f0b2c1d0972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39441\n",
      "18152\n",
      "55791\n",
      "502883\n"
     ]
    }
   ],
   "source": [
    "print(len(recipes1))\n",
    "print(len(recipes2))\n",
    "print(len(recipes3))\n",
    "print(len(recipes4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0557a17f-6925-4914-a32e-b7b648d20e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data:  493013\n",
      "Number of test data:  1032\n"
     ]
    }
   ],
   "source": [
    "#train_list, test_list = np.split(recipe_list, [int(.8*len(recipe_list))])\n",
    "random.shuffle(recipe_list)\n",
    "train_list, test_list = recipe_list[:int(.8*len(recipe_list))], recipe_list[int(.8*len(recipe_list))]\n",
    "print('Number of train data: ', len(train_list))\n",
    "print('Number of test data: ', len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a3cdbf0-f7ed-4ccc-8999-061c9530d130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('distilbert/distilgpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "# I'm not really doing anything with the config buheret\n",
    "configuration = GPT2Config.from_pretrained('distilbert/distilgpt2', output_hidden_states=False)\n",
    "\n",
    "# instantiate the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilbert/distilgpt2\", config=configuration)\n",
    "\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b38a44b9-b940-4d8f-a7ce-e2bbb49fdb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443,711 training samples\n",
      "49,302 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "dataset = RecipeDataset(train_list, tokenizer)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "del dataset\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14fa8d5-f2da-4d9b-97a2-696800715e7e",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27379559-d557-4390-93c8-c1ef414c7049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using device type:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Finetune\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "# some parameters I cooked up that work reasonably well\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# this produces sample output every 100 steps\n",
    "sample_every = 1000\n",
    "# I save the model every 5000 step\n",
    "save_every = 5000\n",
    "# save the model to this file name\n",
    "save_file = 'recipes_generation_model'\n",
    "\n",
    "\n",
    "\n",
    "training_stats = []\n",
    "print(\"Currently using device type: \", device)\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fecd9372-70ac-46db-9725-211b74c2a374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps:  110928\n",
      "Currently using device type:  cuda\n",
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "Batch 1,000  of  55,464. Loss: 0.59565669298172.\n",
      "Batch 2,000  of  55,464. Loss: 0.6105248332023621.\n",
      "Batch 3,000  of  55,464. Loss: 0.6797724366188049.\n",
      "Batch 4,000  of  55,464. Loss: 0.5145111083984375.\n",
      "Batch 5,000  of  55,464. Loss: 0.3921472132205963.\n",
      "Batch 6,000  of  55,464. Loss: 0.4083992540836334.\n",
      "Batch 7,000  of  55,464. Loss: 0.5628868937492371.\n",
      "Batch 8,000  of  55,464. Loss: 0.5639268159866333.\n",
      "Batch 9,000  of  55,464. Loss: 0.5016463994979858.\n",
      "Batch 10,000  of  55,464. Loss: 0.6887199878692627.\n",
      "Batch 11,000  of  55,464. Loss: 0.46227481961250305.\n",
      "Batch 12,000  of  55,464. Loss: 0.6451244950294495.\n",
      "Batch 13,000  of  55,464. Loss: 0.5732980966567993.\n",
      "Batch 14,000  of  55,464. Loss: 0.5107012987136841.\n",
      "Batch 15,000  of  55,464. Loss: 0.6942955255508423.\n",
      "Batch 16,000  of  55,464. Loss: 0.32260021567344666.\n",
      "Batch 17,000  of  55,464. Loss: 0.46275025606155396.\n",
      "Batch 18,000  of  55,464. Loss: 0.5112006664276123.\n",
      "Batch 19,000  of  55,464. Loss: 0.5614563822746277.\n",
      "Batch 20,000  of  55,464. Loss: 0.5650010108947754.\n",
      "Batch 21,000  of  55,464. Loss: 0.4661498963832855.\n",
      "Batch 22,000  of  55,464. Loss: 0.6221420168876648.\n",
      "Batch 23,000  of  55,464. Loss: 0.45770102739334106.\n",
      "Batch 24,000  of  55,464. Loss: 0.4017240107059479.\n",
      "Batch 25,000  of  55,464. Loss: 0.43423011898994446.\n",
      "Batch 26,000  of  55,464. Loss: 0.6627912521362305.\n",
      "Batch 27,000  of  55,464. Loss: 0.4967426657676697.\n",
      "Batch 28,000  of  55,464. Loss: 0.40463876724243164.\n",
      "Batch 29,000  of  55,464. Loss: 0.47506067156791687.\n",
      "Batch 30,000  of  55,464. Loss: 0.5150555968284607.\n",
      "Batch 31,000  of  55,464. Loss: 0.33054977655410767.\n",
      "Batch 32,000  of  55,464. Loss: 0.45608463883399963.\n",
      "Batch 33,000  of  55,464. Loss: 0.3716697096824646.\n",
      "Batch 34,000  of  55,464. Loss: 0.51457679271698.\n",
      "Batch 35,000  of  55,464. Loss: 0.35945984721183777.\n",
      "Batch 36,000  of  55,464. Loss: 0.5152595043182373.\n",
      "Batch 37,000  of  55,464. Loss: 0.5423918962478638.\n",
      "Batch 38,000  of  55,464. Loss: 0.4641113579273224.\n",
      "Batch 39,000  of  55,464. Loss: 0.3893982172012329.\n",
      "Batch 40,000  of  55,464. Loss: 0.7957540154457092.\n",
      "Batch 41,000  of  55,464. Loss: 0.42374005913734436.\n",
      "Batch 42,000  of  55,464. Loss: 0.48174604773521423.\n",
      "Batch 43,000  of  55,464. Loss: 0.2783561050891876.\n",
      "Batch 44,000  of  55,464. Loss: 0.603326678276062.\n",
      "Batch 45,000  of  55,464. Loss: 0.5242139101028442.\n",
      "Batch 46,000  of  55,464. Loss: 0.5022168755531311.\n",
      "Batch 47,000  of  55,464. Loss: 0.3936695456504822.\n",
      "Batch 48,000  of  55,464. Loss: 0.5599559545516968.\n",
      "Batch 49,000  of  55,464. Loss: 0.507904589176178.\n",
      "Batch 50,000  of  55,464. Loss: 0.4702237844467163.\n",
      "Batch 51,000  of  55,464. Loss: 0.5738179683685303.\n",
      "Batch 52,000  of  55,464. Loss: 0.4458064138889313.\n",
      "Batch 53,000  of  55,464. Loss: 0.37638238072395325.\n",
      "Batch 54,000  of  55,464. Loss: 0.43789637088775635.\n",
      "Batch 55,000  of  55,464. Loss: 0.731189489364624.\n",
      "\n",
      "  Average training loss: 0.48\n",
      "  Perplexity: 1.61\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.45\n",
      "  Validation perplexity: 1.57\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.05 GiB. GPU 0 has a total capacity of 11.55 GiB of which 498.25 MiB is free. Including non-PyTorch memory, this process has 11.04 GiB memory in use. Of the allocated memory 9.53 GiB is allocated by PyTorch, and 1.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fine-tune the GPT-2 model on the recipe dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrecipes_generator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecipe_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[0;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(history)\n",
      "File \u001b[0;32m~/Documents/documenti uni/sistemi digitali/projects/SmartFridge/AIChefModel/recipes_generator/model/recipe_model.py:276\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataset, val_dataset, epochs, batch_size, device, sample_every, save_every, save_file, learning_rate, warmup_steps, epsilon)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.05 GiB. GPU 0 has a total capacity of 11.55 GiB of which 498.25 MiB is free. Including non-PyTorch memory, this process has 11.04 GiB memory in use. Of the allocated memory 9.53 GiB is allocated by PyTorch, and 1.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Fine-tune the GPT-2 model on the recipe dataset\n",
    "from recipes_generator.model.recipe_model import train_model\n",
    "history = train_model(model, train_dataset, val_dataset, epochs, batch_size, device, save_file=save_file)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3a4c1b3-dca6-4fd5-bedc-0ce8955e28c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/vocab.json',\n",
       " 'tokenizer/merges.txt',\n",
       " 'tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(save_file)\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3bb7a-7ae9-499b-b187-ce6adc2936e4",
   "metadata": {},
   "source": [
    "# Training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12a4cfdb-e831-42c0-b29d-4c4798309d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"tokenizer\", bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "# I'm not really doing anything with the config buheret\n",
    "configuration = GPT2Config.from_pretrained(save_file, output_hidden_states=False)\n",
    "\n",
    "# instantiate the model\n",
    "model = GPT2LMHeadModel.from_pretrained(save_file, config=configuration)\n",
    "\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c454b99-c1ba-440d-adb1-f0092ea59af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "  Validation Loss: 22.15\n",
      "  Validation perplexity: 4175532544.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from recipes_generator.model.recipe_model import evaluate_model\n",
    "test_dataset = RecipeDataset(test_list, tokenizer, max_length=768)\n",
    "print('Testing...')\n",
    "test_loss, test_perplexity = evaluate_model(model, test_dataset, batch_size, device)\n",
    "test_eval_df = pd.DataFrame(columns = [\"test_loss\", \"test_perplexity\"])\n",
    "test_eval_df['test_loss'] = test_loss\n",
    "test_eval_df['test_perplexity'] = test_perplexity\n",
    "test_eval_df.to_csv(\"test_eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b15979-b2ca-4657-904e-c8a1c3a0ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_eval_df = pd.DataFrame(columns = [\"epoch\", \"training loss\", \"validation loss\", \"train perplexity\", \"validation perplexity\"])\n",
    "train_loss = []\n",
    "train_perp = []\n",
    "valid_loss = []\n",
    "valid_perp = []\n",
    "for epoch in history:\n",
    "    train_loss.append(epoch['Training Loss'])\n",
    "    train_perp.append(epoch['Training Perplexity'])\n",
    "    valid_loss.append(epoch['Valid. Loss'])\n",
    "    valid_perp.append(epoch['Valid. Perplexity'])\n",
    "\n",
    "training_eval_df['epoch'] = [x for x in range(len(training_stats))]\n",
    "training_eval_df['training loss'] = train_loss\n",
    "training_eval_df['validation loss'] = valid_loss\n",
    "training_eval_df['train perplexity'] = train_perp\n",
    "training_eval_df['validation perplexity'] = valid_perp\n",
    "\n",
    "training_eval_df.to_csv(\"train_eval.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a1d13-d781-4618-a7c6-aa118724af5a",
   "metadata": {},
   "source": [
    "# Examples of generated recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5afe7d8-de8c-46ff-b728-54bc5d71c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_loaded = GPT2LMHeadModel.from_pretrained(\"model\", config=configuration, ignore_mismatched_sizes=True)\n",
    "def infer(prompt, model, tokenizer):\n",
    "    input = f\"<|startoftext|>Prompt: {prompt.strip()}\"\n",
    "    input = tokenizer(input, return_tensors=\"pt\")\n",
    "    input_ids      = input[\"input_ids\"]\n",
    "    attention_mask = input[\"attention_mask\"]\n",
    "\n",
    "    output = model.generate(input_ids.to(device),\n",
    "                            attention_mask=attention_mask.to(device),\n",
    "                            max_new_tokens=200,\n",
    "                            num_beams=5, \n",
    "                            no_repeat_ngram_size=2, \n",
    "                            max_length = 600,\n",
    "                            num_return_sequences=1,\n",
    "                            eos_token_id=tokenizer.eos_token_id,\n",
    "                            do_sample = True, top_k = 100, top_p = 0.85)\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d1670bb-8e07-41de-85de-ed5512a8cf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=600) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=600) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: chocolate,flour,sugar, eggs, vanilla essence, self-raising flour, cocoa powder\n",
      "Title: Chocolate Mousse Cake\n",
      "Ingredients: 200 chocolate\n",
      "Servings: 1\n",
      "Instructions: Preheat oven to 180°C. Grease and line a 20cm springform cake tin. Melt chocolate in a heatproof bowl over a pan of simmering water. Remove from heat and stir in sugar until dissolved. Add eggs one at a time beating well after each addition. Stir in vanilla. Fold in sifted flour and cocoa. Pour into prepared tin and bake for 30-35 minutes or until a skewer inserted into the centre comes out clean. Cool in the tin for 10 minutes then remove to a wire rack to cool completely. To make the mousse cream the cream until soft peaks form. Gradually add the sugar and continue to beat until stiff and glossy. \n",
      "Spread over the top of the cake.\n",
      "Cook time: PT35M\n",
      "Preparation time : PT15H\n",
      "Total time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=600) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: chicken, flour, lemon, parmesan cheese, butter, margarine\n",
      "Title: Chicken Parmesan\n",
      "Ingredients: 1 chicken\n",
      "Servings: 4\n",
      "Instructions: Preheat oven to 350°F. Wash chicken and pat dry with paper towels. Sprinkle with salt and pepper. Place chicken in a shallow baking dish. In a small bowl combine the flour and the lemon juice. Dredge chicken pieces in flour mixture. Melt the butter and pour over chicken. Bake uncovered for 45 minutes or until chicken is tender.\n",
      "Cook time: PT45M\n",
      "Preparation time : PT10M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=600) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: courgette, aubergine,flour,lemon, milk, eggs, parmesan cheese, butter, margarine, olive oil, onion, garlic clove, celery, carrot, parsnip, green beans, zucchini, tomatoes, eggplant, flour, salt, pepper, nutmeg, cayenne pepper\n",
      "Title: Vegetable Frittata\n",
      "Ingredients: 1/2 cour Vegie,  1 auornine\n",
      "Servings: 4\n",
      "Instructions: Preheat the oven to 180C/350F/gas mark 4. Grease an ovenproof dish with butter and line with baking paper. Heat the oil in a frying pan and add the onion and garlic and cook for a few minutes until soft but not coloured. Add the veg and saute for another minute or two. Pour in the stock and bring to the boil. Reduce the heat and simmer for 10-15 minutes or until all the liquid has evaporated. Whisk the egg yolks with a little of the milk and\n",
      "Prompt: courgette, pepper, flour, mid-seasoned cheese, butter, margarine\n",
      "Title: Courgettes Au Gratin\n",
      "Ingredients: 1/2 courpe,  pepper\n",
      "Servings: 4\n",
      "Instructions: Preheat oven to 180°C. Grease a shallow 2 litre baking dish with the butter. Arrange courgtes in a single layer in the dish. Sprinkle with pepper and sprinkle with cheese. Pour cream over all. Cover with foil and bake for 30 minutes. Uncover and cook for a further 10 minutes or until the top is golden and bubbling. Serve hot.\n",
      "Cook time: PT40M\n",
      "Preparation time : PT10M\n"
     ]
    }
   ],
   "source": [
    "prompts = ['chocolate,flour,sugar', 'chicken, flour, lemon, parmesan cheese', 'courgette, aubergine,flour,lemon', 'courgette, pepper, flour, mid-seasoned cheese']\n",
    "generated_recipes = []\n",
    "for p in prompts:    \n",
    "    recipe = infer(p, model, tokenizer)\n",
    "    print(recipe)\n",
    "    generated_recipes.append(recipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43c3bb38-a1f6-4052-852d-dcf6e0ab7a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=600) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=600) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: chocolate,flour,sugar, eggs, vanilla essence, self raising flour, cocoa powder\n",
      "Title: Chocolate Mousse Cake\n",
      "Ingredients: 200 chocolate\n",
      "Servings: 1\n",
      "Instructions: Preheat oven to 180°C. Grease and line the base of a 20cm springform cake tin. Melt the chocolate in a heatproof bowl over a pan of simmering water. Remove from the heat and stir in the sugar. Add the egg yolks one at a time beating well after each addition. Beat the cream until soft peaks form. Fold the melted chocolate into the whipped cream. Sift the self-raising flour and cocoa into a bowl and fold in gently. In a clean bowl beat egg whites until stiff and glossy. Gently fold the meringue into chocolate mixture. Pour the mixture into prepared tin and bake for 30-35 minutes or until a skewer comes out clean. \n",
      "Leave to cool for 10 minutes then turn out onto a wire rack and cool completely.\n",
      "Cook time:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=600) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: chicken, flour, lemon, parmesan cheese, butter, margarine\n",
      "Title: Chicken Parmesan\n",
      "Ingredients: 1 chicken\n",
      "Servings: 4\n",
      "Instructions: Preheat oven to 350°F. Wash chicken and pat dry. Sprinkle with salt and pepper. Melt butter in baking dish. Place chicken in dish and sprinkle with cheese. Bake for 30 minutes or until chicken is done.\n",
      "Cook time: PT30M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=600) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: courgette, aubergine,flour,lemon, onion, garlic cloves, cumin seed, mustard seeds, garam masala, turmeric powder, salt, tomatoes, tomato puree, water, green chilies, coriander leaves\n",
      "Title: Roasted Vegetable Curry\n",
      "Ingredients: 1/2 - 3/4 cournut,  1 -2 Auberginine\n",
      "Servings: 4\n",
      "Instructions: Preheat oven to 180°C. Place all the vegetables in a large roasting tin. Sprinkle with salt and pepper. Roast for 30 minutes. Remove from the oven and set aside. In a frying pan heat the oil and fry the onion until soft. Add the garlic and saute for a further 30 seconds. Stir in the spices and cook for 1 minute. Return the veg to the pan and add the pureed tomatoes and water. Bring to a boil and then reduce the heat and simmer for 10-15 minutes or until the veggies are tender.\n",
      "Cook time: PT45M\n",
      "Prompt: courgette, pepper, flour, mid-seasoned cheese, butter, margarine\n",
      "Title: Courgettes Au Gratin\n",
      "Ingredients: 1/2 courpe,  1 pepper\n",
      "Servings: 4\n",
      "Instructions: Preheat oven to 180°C. Grease a shallow 1.5 litre casserole dish. Sprinkle with half the cheese. Repeat layers. Melt the butter in a frying pan and fry the vegetables for 5 minutes or until soft. Pour into the dish and sprinkle with the remaining cheese and breadcrumbs. Bake for 30 minutes.\n",
      "Cook time: PT30M\n",
      "Preparation time : PT10M\n"
     ]
    }
   ],
   "source": [
    "prompts = ['chocolate,flour,sugar', 'chicken, flour, lemon, parmesan cheese', 'courgette, aubergine,flour,lemon', 'courgette, pepper, flour, mid-seasoned cheese']\n",
    "model_loaded = GPT2LMHeadModel.from_pretrained(save_file, ignore_mismatched_sizes=True).to(device)\n",
    "tokenizer_loaded = GPT2TokenizerFast.from_pretrained(\"tokenizer\")\n",
    "generated_recipes_from_loaded = []\n",
    "for p in prompts:    \n",
    "    recipe = infer(p, model_loaded, tokenizer_loaded)\n",
    "    print(recipe)\n",
    "    generated_recipes_from_loaded.append(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94f63a5d-889b-4881-a033-5429ee46cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.3.attn.c_attn.g_idx', 'transformer.h.0.attn.c_attn.g_idx', 'transformer.h.5.attn.c_proj.scales', 'transformer.h.1.attn.c_attn.qweight', 'transformer.h.0.attn.c_proj.scales', 'transformer.h.4.attn.c_attn.qweight', 'transformer.h.4.attn.c_proj.scales', 'transformer.h.5.mlp.c_proj.g_idx', 'transformer.h.0.mlp.c_proj.g_idx', 'transformer.h.2.mlp.c_fc.qzeros', 'transformer.h.3.mlp.c_fc.qzeros', 'transformer.h.0.attn.c_attn.scales', 'transformer.h.5.mlp.c_proj.scales', 'transformer.h.4.attn.c_attn.g_idx', 'transformer.h.1.mlp.c_fc.g_idx', 'transformer.h.2.mlp.c_fc.qweight', 'transformer.h.5.attn.c_attn.qweight', 'transformer.h.1.mlp.c_fc.qweight', 'transformer.h.0.mlp.c_proj.scales', 'transformer.h.2.attn.c_proj.qzeros', 'transformer.h.0.mlp.c_fc.g_idx', 'transformer.h.4.mlp.c_proj.g_idx', 'transformer.h.1.mlp.c_proj.g_idx', 'transformer.h.1.mlp.c_fc.qzeros', 'transformer.h.2.mlp.c_fc.g_idx', 'transformer.h.2.attn.c_attn.qweight', 'transformer.h.2.attn.c_proj.g_idx', 'transformer.h.5.mlp.c_fc.g_idx', 'transformer.h.5.attn.c_attn.scales', 'transformer.h.0.mlp.c_fc.scales', 'transformer.h.1.attn.c_proj.scales', 'transformer.h.5.attn.c_proj.qzeros', 'transformer.h.3.attn.c_proj.qzeros', 'transformer.h.0.mlp.c_fc.qzeros', 'transformer.h.3.attn.c_proj.scales', 'transformer.h.4.attn.c_attn.scales', 'transformer.h.3.mlp.c_fc.qweight', 'transformer.h.2.attn.c_proj.scales', 'transformer.h.1.mlp.c_fc.scales', 'transformer.h.3.attn.c_attn.qweight', 'transformer.h.2.mlp.c_proj.qweight', 'transformer.h.5.attn.c_proj.g_idx', 'transformer.h.2.mlp.c_proj.qzeros', 'transformer.h.5.mlp.c_fc.qweight', 'transformer.h.2.mlp.c_fc.scales', 'transformer.h.0.mlp.c_proj.qweight', 'transformer.h.3.attn.c_proj.qweight', 'transformer.h.1.mlp.c_proj.qzeros', 'transformer.h.5.mlp.c_fc.scales', 'transformer.h.4.mlp.c_proj.scales', 'transformer.h.2.mlp.c_proj.g_idx', 'transformer.h.2.attn.c_attn.g_idx', 'transformer.h.4.attn.c_proj.qzeros', 'transformer.h.0.attn.c_proj.qweight', 'transformer.h.3.mlp.c_fc.scales', 'transformer.h.3.mlp.c_proj.g_idx', 'transformer.h.5.attn.c_proj.qweight', 'transformer.h.3.mlp.c_proj.qweight', 'transformer.h.3.mlp.c_proj.qzeros', 'transformer.h.3.attn.c_attn.qzeros', 'transformer.h.4.mlp.c_fc.qzeros', 'transformer.h.0.mlp.c_fc.qweight', 'transformer.h.4.attn.c_proj.g_idx', 'transformer.h.1.mlp.c_proj.scales', 'transformer.h.0.attn.c_attn.qzeros', 'transformer.h.2.attn.c_proj.qweight', 'transformer.h.5.attn.c_attn.g_idx', 'transformer.h.4.mlp.c_proj.qzeros', 'transformer.h.2.mlp.c_proj.scales', 'transformer.h.4.mlp.c_fc.g_idx', 'transformer.h.5.mlp.c_fc.qzeros', 'transformer.h.3.mlp.c_fc.g_idx', 'transformer.h.0.attn.c_proj.qzeros', 'transformer.h.3.mlp.c_proj.scales', 'transformer.h.5.attn.c_attn.qzeros', 'transformer.h.4.attn.c_attn.qzeros', 'transformer.h.4.mlp.c_fc.qweight', 'transformer.h.1.attn.c_attn.g_idx', 'transformer.h.1.attn.c_proj.qweight', 'transformer.h.3.attn.c_proj.g_idx', 'transformer.h.4.mlp.c_fc.scales', 'transformer.h.3.attn.c_attn.scales', 'transformer.h.2.attn.c_attn.qzeros', 'transformer.h.1.mlp.c_proj.qweight', 'transformer.h.2.attn.c_attn.scales', 'transformer.h.1.attn.c_proj.g_idx', 'transformer.h.1.attn.c_proj.qzeros', 'transformer.h.0.attn.c_proj.g_idx', 'transformer.h.1.attn.c_attn.scales', 'transformer.h.1.attn.c_attn.qzeros', 'transformer.h.0.attn.c_attn.qweight', 'transformer.h.5.mlp.c_proj.qweight', 'transformer.h.4.attn.c_proj.qweight', 'transformer.h.0.mlp.c_proj.qzeros', 'transformer.h.4.mlp.c_proj.qweight', 'transformer.h.5.mlp.c_proj.qzeros']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFGPT2LMHeadModel were not initialized from the PyTorch model and are newly initialized: ['transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "The layer \"tfgpt2lm_head_model_4\" has never been called and thus has no defined input shape. Note that the `input_shape` property is only available for Functional and Sequential models.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model_loaded \u001b[38;5;241m=\u001b[39m TFGPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecipes_generation_model_optimized\u001b[39m\u001b[38;5;124m\"\u001b[39m, ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m      6\u001b[0m model_loaded\u001b[38;5;241m.\u001b[39mbuild((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m768\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel_loaded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_shape\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_loaded\u001b[38;5;241m.\u001b[39moutput_shape)\n\u001b[1;32m      9\u001b[0m converter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mTFLiteConverter\u001b[38;5;241m.\u001b[39mfrom_keras_model(model_loaded)\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/tf_keras/src/engine/base_layer.py:2105\u001b[0m, in \u001b[0;36mLayer.input_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the input shape(s) of a layer.\u001b[39;00m\n\u001b[1;32m   2091\u001b[0m \n\u001b[1;32m   2092\u001b[0m \u001b[38;5;124;03mOnly applicable if the layer has exactly one input,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;124;03m    RuntimeError: if called in Eager mode.\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes:\n\u001b[0;32m-> 2105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   2106\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m has never been called \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand thus has no defined input shape. Note that the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`input_shape` property is only available for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunctional and Sequential models.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2110\u001b[0m     )\n\u001b[1;32m   2111\u001b[0m all_input_shapes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m   2112\u001b[0m     [\u001b[38;5;28mstr\u001b[39m(node\u001b[38;5;241m.\u001b[39minput_shapes) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes]\n\u001b[1;32m   2113\u001b[0m )\n\u001b[1;32m   2114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(all_input_shapes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: The layer \"tfgpt2lm_head_model_4\" has never been called and thus has no defined input shape. Note that the `input_shape` property is only available for Functional and Sequential models."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Config, GPT2Tokenizer\n",
    "config = GPT2Config.from_pretrained(\"recipes_generation_model_optimized\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"recipes_generation_model\")\n",
    "model_loaded = TFGPT2LMHeadModel.from_pretrained(\"recipes_generation_model_optimized\", ignore_mismatched_sizes=True, config=config)\n",
    "\n",
    "model_loaded.build((1, 768))\n",
    "inputs = tokenizer(\"<|startoftext|>Prompt:aubergines,courgettes,eggs\", return_tensors=\"tf\")\n",
    "outputs = model(inputs)\n",
    "print(model_loaded.input_shape)\n",
    "print(model_loaded.output_shape)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_loaded)\n",
    "\n",
    "# For FP16 quantization:\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(\"recipes_generator.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c01c6c-8caf-47b3-94f3-5ba08e343a73",
   "metadata": {},
   "source": [
    "## Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19fc3d36-ce6b-42dd-9e08-c85a07487f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 22:46:12.169225: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-04 22:46:12.197162: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-04 22:46:12.864677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6693c67c174db9ab846a26d4bad531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing transformer.h blocks :   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dagus/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from optimum.gptq import GPTQQuantizer, load_quantized_model\n",
    "import torch\n",
    "random.shuffle(recipe_list)\n",
    "recipe_list = recipe_list[:10000]\n",
    "model_name = \"recipes_generation_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "quantizer = GPTQQuantizer(bits=4, dataset=recipe_list ,model_seqlen = 768)#,block_name_to_quantize = \"model.decoder.layers\")\n",
    "quantized_model = quantizer.quantize_model(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50812971-b010-401f-ac3a-e3df09cc49d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"recipes_generation_model_optimized\"\n",
    "quantizer.save(quantized_model,save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c313f998-1b08-4f27-ae86-d8d16f3c19dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=600) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: aubergines, courgette, cheese, parmesan cheese\n",
      "Title: Aubergine Stuffed With Courgettes and Cheese\n",
      "Ingredients: 3 -4 au plantains,  1/2 courte\n",
      "Servings: 1\n",
      "Instructions: Preheat the oven to 180C/350F/gas mark 4. Prick the auplants all over with a fork and place on a baking tray. Bake for about 30 minutes or until soft. Remove from oven and allow to cool slightly. Scoop out the flesh leaving a 1cm thick shell. Place in a bowl and mash with the back of a spoon. Stir in the cheese and season to taste. Spoon the mixture back into the skins and cover with plastic wrap. Refrigerate for at least 1 hour before serving.\n",
      "Cook time: PT30M\n",
      "Preparation time : PT10M\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, GPT2TokenizerFast\n",
    "import torch\n",
    "configuration = GPT2Config.from_pretrained(\"recipes_generation_model_optimized\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"recipes_generation_model_optimized\", config=configuration, ignore_mismatched_sizes=True)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"recipes_generation_model\")\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "def infer(prompt, model, tokenizer):\n",
    "    input = f\"<|startoftext|>Prompt: {prompt.strip()}\"\n",
    "    input = tokenizer(input, return_tensors=\"pt\")\n",
    "    input_ids      = input[\"input_ids\"]\n",
    "    attention_mask = input[\"attention_mask\"]\n",
    "\n",
    "    output = model.generate(input_ids.to(device),\n",
    "                            attention_mask=attention_mask.to(device),\n",
    "                            max_new_tokens=200,\n",
    "                            num_beams=5, \n",
    "                            no_repeat_ngram_size=2, \n",
    "                            max_length = 600,\n",
    "                            num_return_sequences=1,\n",
    "                            eos_token_id=tokenizer.eos_token_id,\n",
    "                            do_sample = True, top_k = 100, top_p = 0.85)\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return output\n",
    "print(infer(\"aubergines, courgette, cheese\", model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f188bba3-0e41-4501-b5a4-47b008953b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2197c010-e499-49a8-bb6f-cecf6ff5059f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc607cf6-2984-441a-a0c7-6f1c2f3260fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f177d9f-5398-443f-8471-e38233e5beb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1935840-2151-4293-b147-fa06380d2d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b491413-c894-47e8-b59b-2ef17139e14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636a674-30eb-4b19-9c4e-8de8c900b56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9358ab-5cb6-4050-86dc-711466686b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b9a03-f874-4ca6-a09a-eedf2953b747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec30a6-3dde-4e56-aae7-b2e2831908cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57fddf-2fe2-426b-828d-0248a20e84f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf7945f-8300-4f55-9480-7d4d98222b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec734243-7108-4e64-92d7-aee590d366aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea1fd4f-25cb-4c65-83de-4c0641ebe54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43f1c0c-9989-440d-9685-cf2e25195724",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Got <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, but ScriptModule is expected.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmobile_optimizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize_for_mobile\n\u001b[1;32m      4\u001b[0m model_loaded \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecipes_generation_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m optimized_model \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_for_mobile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_loaded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m optimized_model\u001b[38;5;241m.\u001b[39m_save_for_lite_interpreter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecipes_generator.ptl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/utils/mobile_optimizer.py:33\u001b[0m, in \u001b[0;36moptimize_for_mobile\u001b[0;34m(script_module, optimization_blocklist, preserved_methods, backend)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03mOptimize a torch script module for mobile deployment.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    A new optimized torch script module\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(script_module, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(script_module)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but ScriptModule is expected.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimization_blocklist \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     optimization_blocklist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: Got <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, but ScriptModule is expected."
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "model_loaded = GPT2LMHeadModel.from_pretrained(\"recipes_generation_model\", ignore_mismatched_sizes=True)\n",
    "optimized_model = optimize_for_mobile(model_loaded)\n",
    "optimized_model._save_for_lite_interpreter(\"recipes_generator.ptl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f647d9-8444-4fd0-b5be-59ac25edfc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/dagus/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tracer cannot infer type of BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-2.0504e-02, -9.7479e-02, -1.0166e-02,  ...,  5.3485e-02,\n           7.0027e-04, -3.4996e-02],\n         [-1.7572e-03, -4.0842e-01, -1.6583e-01,  ...,  7.6675e-03,\n           3.6098e-01, -4.8115e-01],\n         [-5.8423e-01, -3.9608e-01, -3.0226e-02,  ..., -1.0967e-01,\n           6.4393e-02, -5.3426e-01],\n         ...,\n         [-1.3370e-01, -6.8587e-01, -5.1921e-01,  ...,  2.2168e-01,\n           2.3090e-01, -8.5214e-01],\n         [-1.4905e+00, -2.8908e-01,  9.4241e-01,  ...,  7.8099e-01,\n          -1.2476e-01,  4.3328e-02],\n         [ 4.3870e-01,  3.1624e-01, -9.3352e-02,  ...,  3.7711e-01,\n          -3.5457e-01,  2.6597e-01]]], grad_fn=<ViewBackward0>), past_key_values=((tensor([[[[ 0.5570,  0.4307, -0.5053,  ...,  0.1363,  0.9767,  0.1827],\n          [-0.0912,  0.5041, -0.3954,  ...,  0.3305,  0.3119,  0.4014],\n          [ 0.0072,  0.3909, -0.2975,  ...,  0.4898, -0.1312,  0.3635],\n          ...,\n          [-0.4274,  0.1554,  0.2435,  ..., -0.3244,  0.0434,  0.1394],\n          [-0.9649,  0.6916,  0.0453,  ...,  0.1980, -0.6092, -0.3877],\n          [-0.5655, -0.3282,  0.3821,  ...,  0.3691,  0.1234,  0.0638]],\n\n         [[-0.1360,  0.0653, -0.1330,  ...,  0.2739,  0.4653,  0.2990],\n          [ 0.1335, -0.7138, -1.1276,  ..., -0.1697,  2.4266,  2.0509],\n          [ 0.1810, -0.2216, -1.1807,  ..., -0.5042,  1.7773,  0.6654],\n          ...,\n          [-0.5312,  0.7201, -0.2335,  ...,  0.6675,  2.2712,  1.0832],\n          [-0.0121,  0.5752, -1.9812,  ..., -0.3047,  2.2088,  2.1648],\n          [-0.0128,  0.1358, -1.3117,  ..., -0.7086,  1.8075,  0.4795]],\n\n         [[-1.0590,  0.9298,  0.7462,  ...,  0.3235, -1.9719, -0.5007],\n          [-0.7101,  0.7712,  0.6021,  ..., -0.2041, -1.1030,  0.1506],\n          [-0.8992,  0.4033,  0.8071,  ..., -0.3003, -1.5242, -0.1516],\n          ...,\n          [ 0.3206,  0.3093, -0.0665,  ..., -0.2586, -0.5263,  0.4690],\n          [-0.4050,  0.6720,  0.1057,  ..., -0.7532, -0.6950,  0.2516],\n          [-0.8969, -0.1990, -0.4222,  ..., -0.5416, -0.5635,  0.3747]],\n\n         ...,\n\n         [[ 0.0858,  0.0722, -0.3878,  ...,  0.3259,  0.5415,  0.1013],\n          [ 0.2234,  0.0124, -0.1886,  ...,  0.4475,  0.2705,  0.1010],\n          [ 0.0496,  0.1294, -0.2966,  ...,  0.5023,  0.2179,  0.0952],\n          ...,\n          [ 0.1593,  0.0107,  0.4153,  ...,  0.6001,  0.2461, -0.5677],\n          [ 0.3655,  0.0877,  0.0523,  ...,  0.6682,  0.7060, -0.2129],\n          [ 0.2091,  0.2781, -0.3434,  ...,  0.3215,  0.2962, -0.3418]],\n\n         [[ 0.6778,  1.1371, -0.0898,  ...,  0.7484,  0.3885, -1.2396],\n          [ 0.5056,  0.5427, -0.0114,  ..., -0.2251, -0.3384, -0.8099],\n          [ 0.6741,  0.3555, -0.4862,  ...,  0.2175,  0.8025, -0.6468],\n          ...,\n          [ 0.8198, -0.8415, -0.5011,  ...,  0.2834,  0.2901, -0.0906],\n          [ 0.7880,  0.1045, -0.2394,  ..., -0.1340, -0.2220,  0.2862],\n          [ 0.3383, -0.6522, -0.0572,  ...,  0.6745,  0.5346,  0.2301]],\n\n         [[ 0.8520, -0.3360,  0.3494,  ...,  0.0521, -0.1131,  0.0193],\n          [ 0.6906, -0.0287,  0.3010,  ...,  0.3510, -0.0530,  0.5820],\n          [ 0.3735, -0.1988,  0.3173,  ...,  0.1107,  0.0266,  0.7005],\n          ...,\n          [-0.5795,  0.0208, -0.2925,  ...,  0.0193,  0.3640,  0.5842],\n          [-0.5743,  0.0940, -0.4091,  ..., -0.0844,  0.5802,  0.6936],\n          [-0.3374,  0.2189, -0.1000,  ...,  0.1029,  0.1641,  0.3679]]]],\n       grad_fn=<PermuteBackward0>), tensor([[[[-8.0180e-03,  8.0329e-03, -4.0732e-03,  ...,  1.1266e-02,\n           -4.7284e-02, -3.1585e-02],\n          [-5.0192e-02, -1.1495e-01,  9.3411e-02,  ..., -1.0769e-01,\n            7.7619e-02, -1.2615e-02],\n          [-5.5170e-02, -1.7998e-01, -3.5946e-02,  ...,  8.8814e-02,\n            1.0066e-01, -2.1808e-02],\n          ...,\n          [-1.7238e-01, -4.9123e-02,  2.8447e-01,  ...,  5.4339e-02,\n           -1.4692e-01,  2.1305e-01],\n          [-5.7185e-02, -9.5717e-02, -1.7490e-01,  ...,  4.6850e-02,\n            1.9946e-02, -2.3055e-02],\n          [-3.0795e-01,  1.6235e-02, -1.3527e-01,  ...,  3.2349e-04,\n           -1.6680e-01,  4.3219e-02]],\n\n         [[ 3.5610e-02, -5.6243e-03, -2.7740e-02,  ..., -2.3574e-01,\n           -3.5889e-02, -5.3016e-02],\n          [ 7.6815e-02,  1.8402e-01, -3.9058e-02,  ..., -4.8489e-02,\n            4.7006e-03, -1.2947e-01],\n          [-3.7526e-02, -2.9001e-02,  1.0461e-01,  ..., -9.8126e-02,\n           -6.7217e-02,  2.5450e-02],\n          ...,\n          [ 1.4372e-01,  2.4436e-01,  6.2322e-02,  ...,  1.9306e-01,\n            1.8172e-02,  6.6559e-02],\n          [ 1.3890e-01,  2.3108e-02,  2.6508e-01,  ...,  2.0737e-01,\n           -1.6579e-01, -1.0938e-01],\n          [ 9.3900e-02,  1.3562e-01, -4.1007e-02,  ...,  3.0970e-01,\n            6.2654e-02,  9.8444e-02]],\n\n         [[-3.7733e-03,  3.0594e-02, -4.4296e-03,  ..., -6.9540e-02,\n           -4.8268e-02, -7.9085e-03],\n          [ 3.2578e-02,  1.1431e-01, -3.0824e-02,  ...,  1.5780e-02,\n            3.2049e-02,  3.9024e-02],\n          [ 5.0971e-02,  2.1678e-02,  6.7992e-02,  ..., -8.3660e-02,\n           -1.4788e-01,  7.8971e-02],\n          ...,\n          [ 1.5150e-01,  2.6723e-02,  5.0081e-02,  ...,  2.0863e-02,\n            8.2861e-02, -9.7223e-03],\n          [-4.7415e-02, -3.0159e-02, -1.4404e-01,  ..., -2.6329e-02,\n           -1.9738e-01,  1.3340e-01],\n          [ 9.4655e-02,  1.4119e-01, -8.0678e-02,  ...,  4.4626e-02,\n            1.7816e-02, -8.9467e-02]],\n\n         ...,\n\n         [[ 5.6597e-02, -6.2191e-02, -1.6690e-01,  ...,  4.4933e-02,\n           -1.5649e-02,  1.0282e-01],\n          [ 7.5998e-03, -4.2261e-02,  6.9691e-02,  ..., -5.2961e-02,\n            1.9873e-02,  1.4563e-02],\n          [ 4.1898e-02, -8.6402e-02,  1.7153e-01,  ...,  6.2017e-02,\n            8.5031e-02,  5.0571e-02],\n          ...,\n          [ 1.1614e-01,  1.2795e-01,  1.7068e-01,  ...,  7.9671e-02,\n           -6.9800e-02, -1.2324e-01],\n          [-2.2107e-01, -9.8541e-02, -1.9118e-03,  ..., -3.2950e-02,\n            1.8100e-02,  1.1982e-02],\n          [-9.4097e-02, -1.2692e-01,  2.4174e-02,  ...,  9.5763e-02,\n            1.3953e-01,  2.2491e-02]],\n\n         [[ 2.2242e-02,  5.7131e-02, -3.0762e-02,  ...,  2.6214e-02,\n            2.9970e-03,  3.3986e-03],\n          [ 1.9044e-04,  4.8271e-03,  1.2718e-01,  ..., -3.2988e-02,\n           -1.5783e-01, -1.1890e-01],\n          [-1.5572e-02,  4.5726e-02,  9.2194e-02,  ..., -3.0128e-02,\n            2.9833e-02, -1.0219e-02],\n          ...,\n          [-9.2922e-02, -3.2937e-02,  1.4097e-01,  ...,  2.5838e-01,\n           -1.9086e-01,  1.5413e-01],\n          [-9.4651e-02,  5.3090e-02, -4.3112e-02,  ..., -1.0775e-03,\n            2.3202e-02, -1.2163e-01],\n          [-2.5654e-01, -6.2985e-02,  4.3611e-02,  ..., -7.3058e-02,\n            2.8411e-02, -1.8159e-01]],\n\n         [[ 7.2680e-02,  5.9114e-02,  4.0050e-02,  ...,  7.9343e-03,\n            5.7168e-02, -1.4002e-03],\n          [-3.3211e-03,  1.3341e-01, -4.4178e-02,  ..., -1.2128e-03,\n            1.5463e-01,  9.6027e-02],\n          [-3.4787e-02, -8.4045e-02,  4.3126e-02,  ...,  7.4981e-02,\n            1.2282e-01,  1.6133e-01],\n          ...,\n          [-7.6664e-03,  7.0749e-02, -9.2053e-02,  ..., -9.6781e-02,\n           -6.4669e-02, -6.3876e-02],\n          [-6.9160e-02, -7.7359e-02, -1.7227e-02,  ...,  1.3172e-01,\n            2.7567e-01,  3.9278e-02],\n          [ 1.3714e-01,  5.5284e-03,  1.0415e-02,  ..., -1.5494e-01,\n            2.9333e-01, -7.0809e-02]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 1.1793e-02,  6.8702e-02,  2.5883e-02,  ..., -6.1109e-01,\n            3.2751e-01, -1.5668e-01],\n          [ 9.9393e-01, -2.7619e+00,  1.6909e-01,  ..., -7.1415e-01,\n            1.4749e-02, -4.9081e-01],\n          [ 3.1611e-01, -2.9056e+00, -1.3283e+00,  ..., -1.1175e+00,\n           -8.9866e-02, -3.0059e-01],\n          ...,\n          [ 2.6685e-01, -3.9288e+00,  3.4233e-01,  ..., -1.5560e+00,\n            3.1667e-01,  5.8307e-01],\n          [ 1.0176e+00, -3.6953e+00,  5.3016e-01,  ..., -1.8431e+00,\n            1.4715e+00, -3.4032e-01],\n          [ 1.2149e+00, -4.1336e+00,  9.5283e-02,  ..., -1.9314e+00,\n            1.0613e+00,  4.5978e-01]],\n\n         [[-5.7965e-01,  8.8246e-02, -2.0743e-01,  ...,  1.1906e+00,\n           -8.7433e-01, -4.7180e-01],\n          [ 8.2884e-01,  9.9997e-01, -2.3276e-01,  ...,  1.0888e+00,\n            1.9437e-01,  8.8736e-01],\n          [ 1.6898e-01, -1.3176e-01, -9.2537e-02,  ...,  1.4694e-01,\n           -8.9460e-01,  1.1820e+00],\n          ...,\n          [-1.9770e-01, -6.6178e-02, -1.8101e-03,  ..., -2.6230e-01,\n            4.0006e-01, -5.9551e-03],\n          [ 5.7990e-01, -8.6826e-01, -3.6566e-01,  ...,  8.2626e-01,\n           -2.5984e-01, -6.5563e-01],\n          [ 1.0927e-01, -1.2468e+00, -4.2303e-01,  ...,  4.9072e-01,\n            1.2922e-01,  1.1370e-01]],\n\n         [[ 4.0789e-01,  1.2727e+00,  1.8788e+00,  ...,  5.9780e-01,\n            8.7386e-01,  4.1653e-02],\n          [ 4.9057e-02,  2.1836e+00, -3.1013e-02,  ..., -1.3722e+00,\n            8.4098e-01, -2.3731e-01],\n          [-1.5963e+00,  2.5154e+00, -3.4983e-01,  ..., -1.1286e+00,\n            1.0861e+00, -5.1332e-01],\n          ...,\n          [-1.8768e+00,  2.4299e+00, -2.1719e+00,  ..., -2.2557e+00,\n            1.2504e-01, -1.4398e+00],\n          [-2.1156e+00,  2.0658e+00, -2.5702e+00,  ..., -3.3195e+00,\n           -3.5918e-01, -4.0161e-01],\n          [-2.4121e+00,  1.3971e+00, -2.5321e+00,  ..., -4.1632e+00,\n            7.5720e-01, -1.2648e+00]],\n\n         ...,\n\n         [[ 1.0486e+00, -6.8936e-01, -1.2578e+00,  ...,  1.1058e+00,\n            1.6745e+00,  4.8036e-01],\n          [ 3.7329e-01, -4.0185e-02, -2.0516e-01,  ..., -6.7562e-01,\n            8.6103e-02,  3.1120e-01],\n          [-1.2242e+00,  3.5845e-01, -1.5191e+00,  ...,  1.7472e-01,\n           -2.0401e+00,  8.2646e-01],\n          ...,\n          [-1.2020e+00,  6.4826e-02, -9.4485e-01,  ..., -6.1301e-01,\n           -2.4546e+00,  1.0371e+00],\n          [-1.9283e+00,  9.5584e-01, -2.8252e-01,  ..., -6.1653e-01,\n           -2.8950e+00, -5.2733e-01],\n          [-1.9092e+00,  7.2577e-01, -2.2470e-01,  ..., -6.7239e-01,\n           -3.2917e+00, -3.8621e-01]],\n\n         [[ 1.3994e+00,  5.1456e-02,  6.3613e-01,  ..., -3.3289e-02,\n           -6.6591e-01, -1.8809e-01],\n          [ 8.7419e-01, -7.6987e-01,  5.6867e-01,  ...,  1.0583e+00,\n           -1.8279e+00, -1.9588e-01],\n          [ 6.5777e-01, -3.9376e-01,  2.4107e-01,  ...,  4.1576e-02,\n           -2.1135e+00, -8.1792e-01],\n          ...,\n          [ 8.9915e-01,  9.5046e-01, -1.5666e-01,  ...,  6.1330e-01,\n           -1.0625e+00,  3.6371e-01],\n          [ 9.0144e-01,  1.2193e-01,  3.7665e-01,  ..., -6.9513e-01,\n           -1.0217e+00,  3.1789e-01],\n          [ 1.4913e+00, -3.2076e-01,  5.7462e-01,  ...,  1.4740e-01,\n           -9.6536e-01, -7.0432e-02]],\n\n         [[ 7.5715e-02,  7.9794e-02, -1.8751e-01,  ..., -1.3675e-02,\n            8.8612e-02, -4.2339e-02],\n          [-6.1103e-01, -4.8150e-02, -5.8175e-01,  ..., -5.3281e-01,\n            1.7137e-01, -1.1033e+00],\n          [-1.1744e-01,  2.7051e-01, -4.9298e-01,  ..., -5.6000e-01,\n            5.3636e-01,  2.5293e-03],\n          ...,\n          [ 3.4148e-01, -5.5151e-01, -1.9702e-01,  ...,  1.0008e+00,\n            1.9048e-01, -2.1098e-01],\n          [ 3.6585e-01,  7.4925e-02, -2.0566e-01,  ..., -3.6997e-01,\n            6.2052e-02,  5.7593e-01],\n          [ 2.4069e-01, -1.6551e-01,  5.0819e-01,  ...,  1.8271e-01,\n            8.4506e-01,  2.5362e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[-0.0216, -0.0634, -0.1309,  ...,  0.0207, -0.1429, -0.3435],\n          [-0.2166, -0.8090,  0.4947,  ..., -0.0640,  0.6865,  0.5147],\n          [-0.5278, -0.3991,  0.5053,  ...,  1.1802,  1.0349,  0.2762],\n          ...,\n          [ 0.0773, -0.4442,  0.3032,  ...,  0.1042,  0.0181,  0.8915],\n          [ 0.7713, -1.2358,  0.7985,  ...,  0.1847,  0.3874,  0.2830],\n          [ 0.5416, -1.1191,  0.5999,  ...,  0.0558,  0.7045, -0.0562]],\n\n         [[ 0.0154, -0.0484,  0.1736,  ..., -0.0458, -0.0265,  0.0054],\n          [ 0.1130, -1.2983,  0.3993,  ..., -0.9947, -0.1493,  0.2514],\n          [ 0.2502, -1.2443,  0.8783,  ..., -0.0324,  0.0656, -0.3491],\n          ...,\n          [-0.2334, -1.0844,  0.4362,  ...,  1.1638, -0.2594, -0.3856],\n          [-0.2402, -0.8605,  1.0080,  ...,  0.7598, -0.5325, -0.0308],\n          [-0.5397, -0.6933,  1.3280,  ...,  1.1463, -0.3266,  0.3956]],\n\n         [[ 0.0221,  0.0467, -0.0992,  ...,  0.0870,  0.0078,  0.0500],\n          [-0.1187, -0.5206, -0.4095,  ..., -0.6356, -0.6862,  0.5913],\n          [ 0.5174, -1.2189,  0.0714,  ...,  0.5196, -0.0588,  0.1633],\n          ...,\n          [ 0.1438, -0.4790, -0.7399,  ...,  0.4313,  1.0592, -0.4186],\n          [ 0.8644, -0.8879, -0.1108,  ...,  1.0016, -0.4976, -0.9884],\n          [-0.2870, -0.7563,  0.2315,  ...,  0.3784, -0.1330, -1.7259]],\n\n         ...,\n\n         [[-0.0368, -0.0812,  0.0243,  ...,  0.0948,  0.0835, -0.0463],\n          [-0.2196,  0.1204, -0.4731,  ...,  0.6988, -0.0452, -0.0662],\n          [ 0.1808,  0.7153, -0.5813,  ...,  1.1190,  0.4318,  0.2993],\n          ...,\n          [-0.2225, -0.7455,  0.3972,  ...,  0.3874, -0.0231, -0.0094],\n          [ 0.9128,  0.4409, -0.3444,  ..., -0.3482,  0.2007,  0.0128],\n          [ 0.2070, -0.9902,  0.4238,  ..., -0.1258, -0.3764,  0.1518]],\n\n         [[-0.0486, -0.0825, -0.1783,  ...,  0.0604,  0.0468,  0.0403],\n          [-0.4465, -0.0717,  0.7522,  ..., -0.1395,  0.2353, -0.1800],\n          [ 0.4464,  0.2111,  0.3491,  ..., -0.8877,  0.7942, -1.0405],\n          ...,\n          [ 0.6770, -0.0920, -0.2157,  ...,  0.1688, -0.1732,  0.3300],\n          [-0.4555,  0.1030,  1.3020,  ..., -0.1254,  0.0357, -0.3885],\n          [-0.0145, -0.1891,  1.2536,  ...,  0.0664,  0.1096, -0.4065]],\n\n         [[ 0.0250, -0.0799, -0.0031,  ..., -0.0195,  0.2424, -0.0070],\n          [-0.1717,  0.3711,  0.4196,  ..., -0.9720, -0.2964, -0.2742],\n          [-0.1718,  1.0055,  0.9368,  ..., -0.0968, -0.9966,  0.2085],\n          ...,\n          [-0.3634, -0.3298, -0.1930,  ...,  0.2963, -0.4168,  0.4885],\n          [-0.1954, -0.2581,  0.3884,  ..., -0.4952, -0.3121,  0.3417],\n          [ 0.2222, -0.3965,  0.7119,  ..., -1.1530, -0.5427, -0.0055]]]],\n       grad_fn=<PermuteBackward0>)), (tensor([[[[-1.8677e+00,  4.1505e-02,  4.8454e-03,  ..., -9.4523e-01,\n            4.5578e-01, -3.2969e+00],\n          [-1.5114e+00,  7.6960e-01,  1.8194e-01,  ...,  4.1868e-01,\n            6.3745e-01, -1.5788e+00],\n          [-1.5890e+00,  6.7367e-02, -2.7600e-01,  ..., -2.8530e-01,\n            3.6418e-01, -1.3652e+00],\n          ...,\n          [-1.3517e+00, -1.5604e+00, -1.7869e+00,  ..., -1.6559e+00,\n           -1.4155e+00,  2.8737e-01],\n          [-2.1506e-01, -1.9834e-01, -3.8074e-01,  ..., -1.2966e+00,\n           -1.5251e+00,  9.3664e-02],\n          [-1.4389e+00,  4.1704e-01, -2.4139e+00,  ..., -1.3451e+00,\n           -1.1002e+00,  4.5198e-01]],\n\n         [[ 2.8162e-01,  2.3912e-01,  5.7205e-01,  ...,  2.8247e-01,\n            3.5734e-01, -2.9627e+00],\n          [-1.7394e+00,  2.4530e-01,  1.7256e+00,  ..., -2.2244e+00,\n           -6.8350e-01,  6.1590e-01],\n          [-1.9922e+00,  5.3320e-01,  2.1084e+00,  ..., -2.0770e+00,\n           -2.4719e+00,  2.6142e+00],\n          ...,\n          [-1.5102e+00,  3.4903e-01,  2.3285e+00,  ..., -3.0108e+00,\n           -1.7068e+00,  3.6820e+00],\n          [-8.7199e-01,  8.3163e-01,  2.2725e+00,  ..., -1.8048e+00,\n           -2.3996e+00,  4.0445e+00],\n          [-4.5799e-01, -9.1126e-03,  3.4736e+00,  ..., -2.8632e+00,\n           -1.4199e+00,  4.3720e+00]],\n\n         [[ 2.2915e-01, -1.2882e+00, -7.0178e-01,  ...,  3.8016e-01,\n           -1.5352e-01, -3.9352e-01],\n          [ 5.1941e-02,  1.2321e+00,  1.5882e-01,  ...,  3.2166e-01,\n            2.1598e-01,  8.5408e-01],\n          [ 9.4449e-02,  9.0423e-01,  1.8057e-01,  ..., -2.1943e-01,\n            6.4667e-01, -7.1334e-02],\n          ...,\n          [ 5.7649e-01,  3.3475e+00,  3.9305e-01,  ..., -9.0214e-02,\n            1.2997e-02,  8.9739e-01],\n          [ 4.7836e-02,  2.8612e+00,  2.6689e-01,  ..., -6.1207e-01,\n            8.7766e-01,  8.3327e-01],\n          [ 3.1726e-01,  3.6431e+00,  2.9849e-01,  ..., -1.3564e-01,\n            4.4745e-01, -5.7273e-01]],\n\n         ...,\n\n         [[-7.7266e-01,  5.5429e-03, -2.3083e-01,  ...,  1.3252e+00,\n           -3.0876e-01,  2.4335e+00],\n          [ 3.5627e-01, -7.6413e-01,  8.3112e-01,  ...,  1.0047e+00,\n            1.2336e+00,  1.9129e+00],\n          [ 3.1758e-01, -1.7131e+00,  9.2744e-01,  ...,  1.2690e+00,\n            1.1416e+00,  1.4016e+00],\n          ...,\n          [ 1.3919e+00, -1.8304e+00,  1.1591e+00,  ..., -4.1035e-01,\n           -3.4167e-01, -3.4256e-01],\n          [ 1.7601e+00, -1.5028e+00,  4.0935e-01,  ..., -4.6187e-01,\n           -2.8926e-01,  2.4551e-01],\n          [ 3.9764e-01, -3.9966e-01,  4.8949e-01,  ..., -5.9412e-01,\n            3.9944e-01, -1.4487e-02]],\n\n         [[-2.6041e-01, -2.6286e-01,  4.2083e-01,  ...,  4.4153e-01,\n            3.7089e-01,  2.7412e-01],\n          [ 3.8969e-01, -6.2055e-01, -1.5017e+00,  ...,  1.2362e+00,\n            3.4629e-01,  3.8472e-01],\n          [ 2.4498e-01,  4.5572e-01, -7.1889e-01,  ...,  3.9112e-01,\n            6.0691e-01,  4.9542e-01],\n          ...,\n          [ 1.4194e+00, -5.9293e-01,  2.4916e-01,  ...,  1.1707e+00,\n           -3.5224e-01, -1.1789e+00],\n          [ 2.0845e+00,  6.1076e-01, -1.2510e+00,  ...,  2.6020e-01,\n           -4.9280e-01,  1.2502e-01],\n          [ 2.0164e+00, -5.6132e-01, -8.3646e-01,  ...,  1.9588e+00,\n           -5.2657e-01, -7.0055e-01]],\n\n         [[ 1.8915e+00,  1.8761e+00, -9.0029e-01,  ..., -2.0158e+00,\n           -1.3396e+00, -4.0708e-01],\n          [ 6.4143e-01, -7.1463e-01, -1.1588e+00,  ..., -5.0721e+00,\n           -5.2149e+00, -8.6966e-01],\n          [ 9.6577e-01,  2.1570e-01,  3.2774e-01,  ..., -4.1085e+00,\n           -6.6959e+00, -1.1846e+00],\n          ...,\n          [ 5.6866e-01,  3.0307e+00, -1.0974e+00,  ..., -3.2477e+00,\n           -3.6591e+00,  1.7849e+00],\n          [ 6.2218e-01,  1.7561e+00, -1.6330e+00,  ..., -4.1662e+00,\n           -3.3693e+00,  8.4492e-01],\n          [-9.9718e-02, -3.5874e-01,  6.3078e-02,  ..., -5.7476e+00,\n           -1.8059e+00, -5.6820e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 0.0199, -0.0087, -0.0124,  ...,  0.0263, -0.1341,  0.0986],\n          [-0.1910,  0.1115,  0.7845,  ..., -0.3363, -0.3910, -0.6377],\n          [-0.9952,  0.3961,  0.0891,  ...,  0.3636, -0.1562,  0.1998],\n          ...,\n          [-0.2009, -0.2968, -0.4515,  ..., -0.7580, -0.1851, -0.4964],\n          [ 0.4249, -0.2667,  0.2276,  ...,  0.9889, -0.3405,  0.1015],\n          [-0.0678, -0.1611,  0.7415,  ...,  0.5653,  0.8791, -0.5364]],\n\n         [[-0.0126, -0.0066, -0.1787,  ..., -0.1315,  0.0114,  0.0551],\n          [ 0.7682,  0.2922, -0.6003,  ...,  0.4310,  0.4087,  0.1082],\n          [ 1.0197,  0.9754, -0.3150,  ..., -0.7197, -0.5154,  0.4147],\n          ...,\n          [ 0.1960, -0.1133, -0.1150,  ..., -0.0079,  0.1328,  0.3555],\n          [ 0.1820, -0.4934, -0.2292,  ..., -0.3090, -0.6940, -0.2304],\n          [ 0.2250, -0.5213, -0.8863,  ...,  0.5461, -0.2228,  0.3499]],\n\n         [[ 0.1531, -0.0713,  0.0145,  ..., -0.0631,  0.0881,  0.1338],\n          [ 0.0529,  0.0674, -0.9629,  ..., -0.4858,  0.2818, -0.0509],\n          [ 0.2037, -0.4008, -0.7720,  ..., -0.4401,  0.8206, -0.1498],\n          ...,\n          [ 0.1390,  0.1519,  0.7736,  ...,  0.4154, -0.2232,  0.5556],\n          [ 0.0287, -0.3214, -0.0535,  ...,  0.0623, -0.3038,  0.4514],\n          [-0.0312, -0.1729,  0.5680,  ...,  0.5946, -0.3815, -0.2047]],\n\n         ...,\n\n         [[ 0.0270,  0.0596, -0.0467,  ...,  0.1865,  0.0507, -0.0871],\n          [ 1.0828, -0.1267, -0.2870,  ..., -0.7420, -0.1834,  0.5953],\n          [-0.0480, -0.5166, -0.7910,  ..., -0.6132, -1.0172,  0.4957],\n          ...,\n          [ 1.2030, -0.3084,  0.8490,  ...,  0.4454, -0.8504, -0.6449],\n          [ 1.4094, -0.2948,  0.0935,  ...,  0.2719, -0.3546, -0.2002],\n          [ 0.0777, -0.4887, -0.1382,  ..., -0.0304, -0.5213, -0.1072]],\n\n         [[-0.0557, -0.0176,  0.0131,  ..., -0.0219,  0.0617,  0.0667],\n          [-0.5416,  1.0859, -0.3715,  ...,  0.3784, -0.5952, -0.7176],\n          [-0.7959,  0.2060, -0.5209,  ...,  0.4164, -0.7018, -0.8632],\n          ...,\n          [ 0.7245,  0.2064, -1.0559,  ...,  1.2667,  0.6346, -0.9088],\n          [ 0.6433,  0.2631,  0.0091,  ...,  1.2686,  0.1255, -0.8010],\n          [ 1.3426,  0.1523, -0.1900,  ...,  1.1599, -0.4946, -1.3226]],\n\n         [[ 0.0495,  0.0535,  0.1325,  ..., -0.0052, -0.0826, -0.0672],\n          [-0.2958, -0.2405, -0.1993,  ..., -0.4075, -0.2964,  0.7885],\n          [ 0.3288, -1.0919, -0.1170,  ...,  0.3375,  0.0609, -0.0716],\n          ...,\n          [-0.1209, -0.4789,  1.1902,  ..., -0.1331, -0.5614, -1.6860],\n          [-0.9203, -1.0198,  0.2325,  ...,  0.0467, -0.9221, -0.7359],\n          [-0.4341, -0.8488,  0.5657,  ...,  0.2104, -0.6752, -0.9123]]]],\n       grad_fn=<PermuteBackward0>)), (tensor([[[[ 9.8602e-01, -1.3410e-01, -3.2487e-01,  ...,  8.3973e-01,\n            1.1067e+00, -6.8142e-01],\n          [-1.1772e+00, -9.4043e-01, -1.1896e+00,  ..., -1.1448e+00,\n           -2.0120e+00, -1.1364e+00],\n          [-2.5447e+00, -1.6413e+00, -1.3254e+00,  ..., -1.3606e+00,\n           -4.1592e+00, -2.4504e+00],\n          ...,\n          [-2.8003e+00, -8.0995e-01,  8.5006e-01,  ..., -3.1223e+00,\n           -4.1719e+00, -2.7691e+00],\n          [-3.0035e+00, -1.7250e+00,  1.6454e+00,  ..., -2.6978e+00,\n           -4.1289e+00, -3.3616e+00],\n          [-4.1666e+00, -3.0245e-01,  7.0552e-01,  ..., -1.9955e+00,\n           -4.2735e+00, -1.9060e+00]],\n\n         [[-2.3149e-01, -2.1230e-01, -5.3774e-02,  ..., -8.0137e-02,\n           -7.7574e-01, -8.3382e-02],\n          [-4.6553e-01,  7.0847e-01,  6.2259e-01,  ..., -1.0885e-01,\n            6.1795e-02,  3.5720e-01],\n          [ 2.1200e-01, -2.3550e-01, -3.7807e-02,  ..., -7.4502e-01,\n            2.4035e-01,  5.5630e-01],\n          ...,\n          [-5.8594e-01, -1.3862e+00,  2.6233e-01,  ...,  3.3460e+00,\n            3.4873e-01,  2.8372e-01],\n          [-1.0115e+00, -1.4555e-03,  4.3157e-01,  ...,  3.2180e+00,\n            2.2698e+00,  1.3455e+00],\n          [ 3.7707e-01, -1.3358e+00,  6.5898e-01,  ...,  2.5937e+00,\n            1.7928e+00,  2.1578e+00]],\n\n         [[ 6.6125e-02,  5.7937e-02,  9.6843e-01,  ..., -6.1193e-01,\n            1.4991e-01, -8.6127e-01],\n          [ 8.0330e-01, -1.6499e-01,  1.5503e+00,  ...,  2.1702e-01,\n            4.2679e-01, -6.5554e-02],\n          [-3.0969e-01, -9.8851e-01,  1.4250e+00,  ...,  1.5518e-01,\n           -4.6595e-02, -8.8978e-02],\n          ...,\n          [-6.1740e-01,  6.1671e-01, -1.0059e+00,  ..., -1.1532e+00,\n           -2.0528e+00,  9.6587e-01],\n          [-4.6263e-01, -1.2255e+00, -4.0107e+00,  ..., -7.1859e-01,\n           -4.7069e+00,  1.4519e+00],\n          [-4.0274e-01, -2.9628e-01, -3.2088e+00,  ..., -8.8920e-01,\n           -4.3938e+00,  2.8905e-01]],\n\n         ...,\n\n         [[-2.2831e-01,  1.6811e-01, -3.4238e-01,  ..., -4.6979e-02,\n            2.3756e-01,  1.9369e-01],\n          [-1.6696e+00, -2.7342e-01, -2.0884e-01,  ..., -6.8122e-02,\n            6.4015e-02,  6.0151e-01],\n          [-1.3304e+00, -2.5178e-01,  3.1522e-01,  ..., -2.4766e-01,\n           -2.0102e-01, -2.4570e-01],\n          ...,\n          [-4.7155e-01, -1.7459e-01, -1.3429e+00,  ...,  3.2283e-01,\n           -8.0067e-01, -9.7069e-01],\n          [-1.6160e+00, -2.4109e-01,  1.0443e+00,  ...,  9.1587e-01,\n           -1.9325e-01, -7.4547e-01],\n          [-1.4089e+00, -2.3552e-01,  8.0522e-01,  ...,  1.1389e+00,\n           -4.9494e-01, -1.6224e+00]],\n\n         [[-2.3939e-01, -2.3753e+00,  2.0543e-01,  ..., -7.7681e-02,\n           -1.1844e-01,  8.7508e-01],\n          [-3.6298e-01, -2.0523e+00, -3.4474e-01,  ...,  7.7182e-01,\n            2.9637e-01,  2.4739e-01],\n          [-5.6639e-01,  8.3187e-01, -1.9911e+00,  ...,  2.1034e-01,\n           -4.8029e-01, -1.5272e-01],\n          ...,\n          [ 2.5445e+00,  9.2938e-01, -1.3311e+00,  ..., -3.9055e-01,\n           -9.2660e-01, -1.5473e+00],\n          [-1.1802e+00,  3.3015e+00,  1.2435e-01,  ..., -2.2730e+00,\n           -3.6787e-01, -7.8107e-01],\n          [ 1.0142e+00,  2.6261e+00,  6.2323e-01,  ..., -1.1425e+00,\n           -7.0328e-01,  2.0096e+00]],\n\n         [[ 3.9209e-01, -1.5932e-01, -5.0435e-02,  ...,  1.8377e-01,\n           -1.6472e-01,  4.7756e-01],\n          [ 4.9601e-01,  3.8713e-01,  3.8588e-01,  ..., -8.8805e-02,\n            3.1678e-01,  3.6347e-02],\n          [ 4.8462e-01,  1.2816e+00,  4.0377e-01,  ...,  3.8783e-01,\n            1.7954e-01, -1.3614e-01],\n          ...,\n          [-1.9095e+00, -4.5522e-01,  3.2686e-01,  ..., -1.0532e-01,\n           -2.5792e+00, -2.4526e+00],\n          [-1.5031e+00, -8.4546e-01, -6.5360e-02,  ..., -4.8020e-01,\n           -1.1478e-01, -9.0569e-01],\n          [-1.2446e+00, -2.4308e-01,  6.5936e-02,  ...,  4.0389e-01,\n           -1.8221e+00, -1.9281e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[-6.2752e-02,  5.3543e-02, -2.1092e-02,  ..., -1.0406e-01,\n            4.1936e-02,  8.6872e-02],\n          [ 5.1621e-02, -6.1137e-01, -1.0092e-01,  ..., -8.4237e-02,\n           -1.7137e-01, -3.5616e-01],\n          [-1.2210e-01,  4.1154e-01, -1.1133e+00,  ...,  2.9637e-01,\n           -6.3050e-01, -1.6766e-01],\n          ...,\n          [-5.1030e-01, -1.2980e+00, -3.4629e-01,  ..., -3.0695e-01,\n           -3.8914e-01, -8.9414e-01],\n          [ 1.4606e-02, -8.7192e-01, -1.2821e+00,  ...,  1.1450e+00,\n           -1.0162e+00,  4.9151e-01],\n          [-8.7282e-01, -2.5271e+00, -6.6079e-01,  ...,  1.5294e-01,\n            6.1572e-03,  1.5273e-01]],\n\n         [[-2.2934e-02,  2.0868e-04,  2.0920e-01,  ...,  6.0070e-02,\n           -1.7674e-02, -2.4415e-02],\n          [ 1.0024e-01,  2.9254e-02,  3.9656e-01,  ..., -4.3732e-01,\n           -4.5017e-02, -4.1847e-01],\n          [ 1.2605e+00,  3.9107e-01,  3.9869e-01,  ..., -1.4805e+00,\n            5.9956e-01,  4.4918e-01],\n          ...,\n          [ 1.6552e+00,  1.1162e+00, -1.3810e+00,  ...,  7.6276e-01,\n            1.7005e+00,  7.9900e-01],\n          [ 2.2720e+00,  6.2952e-01, -1.0182e-01,  ...,  1.0831e-01,\n            8.7174e-01, -8.4184e-02],\n          [ 2.4602e+00, -1.5281e-01, -3.7974e-01,  ..., -2.2553e-01,\n            2.4813e+00, -1.4800e-01]],\n\n         [[ 1.2206e-03,  6.2199e-03, -2.9682e-02,  ..., -4.1766e-02,\n           -2.7108e-03,  4.6978e-03],\n          [ 1.4679e+00,  9.3104e-03, -6.8771e-01,  ..., -1.2577e-02,\n           -8.1340e-01, -4.2228e-01],\n          [ 8.6120e-01, -4.9433e-03,  1.1586e+00,  ..., -1.5813e+00,\n           -1.6080e+00, -6.4140e-01],\n          ...,\n          [-6.0278e-01,  1.2376e+00,  1.7698e+00,  ...,  4.1949e+00,\n            8.4232e-01,  2.2840e+00],\n          [-8.5422e-02,  2.2420e-01,  8.8870e-01,  ..., -1.8558e+00,\n            4.2597e-01, -9.1149e-02],\n          [ 1.8756e+00,  3.3461e-01,  3.1553e+00,  ...,  7.9838e-01,\n            7.8259e-01,  9.0349e-01]],\n\n         ...,\n\n         [[ 8.6164e-02, -1.4784e-01, -6.8703e-02,  ...,  3.2957e-02,\n            5.5753e-02, -7.5036e-02],\n          [-3.4574e-01, -1.7536e-01,  2.8682e-01,  ...,  2.2871e-01,\n           -5.9915e-02, -4.8490e-01],\n          [ 1.3784e-01, -7.5136e-01,  7.5124e-01,  ..., -5.9130e-01,\n           -1.9978e-01, -5.6584e-01],\n          ...,\n          [ 1.4864e+00,  6.7357e-01,  6.1315e-01,  ...,  1.3113e+00,\n            9.0045e-01, -2.2154e-01],\n          [ 1.2247e+00, -1.5633e-01, -1.0187e+00,  ...,  1.1375e+00,\n            1.6825e+00, -8.8944e-01],\n          [ 9.0919e-01,  2.5527e-01,  4.9148e-03,  ...,  1.2514e+00,\n            1.6568e+00, -1.2530e+00]],\n\n         [[ 1.3625e-01, -5.9697e-02,  1.0035e-01,  ...,  1.6206e-02,\n           -4.2829e-02, -3.2952e-02],\n          [-4.4331e-01,  3.0010e-01, -6.2545e-02,  ..., -6.8408e-01,\n           -2.2847e-01, -2.6119e-01],\n          [ 3.7825e-01,  9.7867e-01, -6.7172e-01,  ...,  5.5770e-02,\n           -2.7496e+00,  2.2595e-02],\n          ...,\n          [-4.7684e-02, -1.3283e-01,  4.5407e-01,  ..., -1.5368e-01,\n            1.8016e+00, -2.2577e+00],\n          [-6.1641e-01,  2.4885e+00,  3.2986e-01,  ..., -2.8765e-01,\n            7.0502e-01, -9.3477e-01],\n          [-1.0332e-01,  3.3179e+00,  1.6641e+00,  ..., -4.3842e-01,\n            1.6062e+00, -1.9473e+00]],\n\n         [[ 6.5138e-02, -2.9824e-02, -6.3254e-02,  ..., -8.6778e-02,\n            1.4943e-01, -7.3095e-02],\n          [ 3.1440e-01,  9.7657e-02, -4.8944e-01,  ..., -6.2506e-01,\n           -2.2523e-01,  7.5599e-01],\n          [ 3.4286e-01,  4.9685e-01, -6.5793e-01,  ..., -9.8072e-01,\n            3.0475e-01,  2.1327e-01],\n          ...,\n          [ 4.4914e-01, -4.7747e-01, -5.2657e-01,  ..., -5.5874e-01,\n           -6.7587e-01,  1.0878e+00],\n          [-7.3795e-01, -4.5210e-01,  5.6438e-01,  ...,  5.0766e-01,\n            1.1249e-02,  1.0396e+00],\n          [ 3.2435e-01, -6.2770e-01,  1.4184e+00,  ..., -1.5092e-01,\n           -8.8488e-01,  2.5156e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-8.4873e-02, -9.0384e-02, -1.3746e-01,  ...,  8.9630e-01,\n            4.7412e-01,  5.5114e-01],\n          [ 2.7835e-01, -5.0923e-02, -1.3183e-01,  ...,  3.6662e-01,\n            9.6946e-01,  1.1396e-01],\n          [ 9.4455e-01, -8.0317e-01, -7.6482e-02,  ..., -1.2483e+00,\n            4.0213e-01,  5.1928e-01],\n          ...,\n          [ 2.7117e+00,  1.3555e+00, -7.1501e-02,  ..., -3.0632e+00,\n           -1.3871e+00,  4.2568e-01],\n          [ 3.7282e+00,  1.5561e+00,  2.5292e-01,  ..., -3.3446e+00,\n           -5.5187e-01,  9.0691e-01],\n          [ 1.6064e+00,  6.6737e-01,  6.5152e-01,  ..., -3.1340e+00,\n            1.0567e+00, -4.3326e-01]],\n\n         [[ 1.4620e-01,  5.3789e-01,  3.2741e-01,  ...,  1.5542e-01,\n           -1.0684e+00, -3.6684e-04],\n          [-1.0694e-01,  1.3596e-01,  5.4573e-01,  ...,  5.8051e-01,\n           -1.1888e-02,  1.7301e-01],\n          [ 3.2033e-01,  7.9250e-01,  4.1425e-02,  ...,  1.0838e+00,\n            2.9698e-01,  2.0976e+00],\n          ...,\n          [-8.7040e-01,  2.1489e-01, -9.3244e-01,  ...,  4.5650e-01,\n            6.7845e-01,  2.7173e+00],\n          [ 1.1574e+00,  2.8919e-01, -5.2404e-01,  ..., -9.1458e-01,\n            1.5072e+00,  1.1043e+00],\n          [ 1.6032e-01,  1.5124e-01, -1.0706e+00,  ..., -2.9099e-01,\n            9.0987e-01,  1.4359e+00]],\n\n         [[-1.5250e+00,  2.0002e-02,  7.0049e-01,  ..., -7.5049e-01,\n            3.7906e-01, -5.2137e-01],\n          [ 3.4132e-02, -6.7340e-01,  1.4980e-01,  ..., -3.5024e-01,\n           -1.1019e-01, -1.1201e+00],\n          [ 1.8436e+00,  3.4792e-01,  5.4133e-02,  ...,  9.2611e-01,\n            4.9148e-01, -1.0801e+00],\n          ...,\n          [ 2.8560e+00,  3.9129e-01, -2.8250e-01,  ...,  1.0889e+00,\n           -7.5057e-01,  4.3308e-01],\n          [ 2.7226e+00, -9.5709e-01,  3.5918e-01,  ...,  8.9640e-01,\n           -1.7273e+00, -1.2086e-01],\n          [ 3.1697e+00,  4.3079e-01, -1.3490e-01,  ...,  4.3863e-01,\n           -1.9439e+00,  2.1139e+00]],\n\n         ...,\n\n         [[ 6.2512e-01, -6.0634e-01,  2.2982e-01,  ..., -6.6939e-01,\n           -1.4417e-02,  9.3060e-01],\n          [-1.0538e-01, -1.3458e+00, -4.2419e-01,  ...,  9.2766e-01,\n           -8.9774e-01,  4.7069e-01],\n          [-6.3236e-01, -3.2019e-01, -1.7082e+00,  ...,  1.9318e+00,\n           -1.2644e-01, -8.5374e-01],\n          ...,\n          [-3.6163e+00, -6.6680e-01, -1.3596e-01,  ...,  3.7377e+00,\n            1.3507e+00, -3.2167e+00],\n          [-2.5316e+00,  4.2014e-01, -1.9152e+00,  ...,  6.7565e-01,\n            2.2897e+00, -1.2980e+00],\n          [-3.0486e+00,  8.9785e-01, -1.8937e-01,  ...,  1.4784e+00,\n            2.0566e+00, -6.4665e-01]],\n\n         [[-1.1025e+00,  3.1147e+00,  2.7114e-01,  ...,  3.5149e-01,\n            2.2051e+00, -7.6251e-01],\n          [-6.0179e-01,  3.0222e+00,  2.4885e-01,  ...,  6.6769e-01,\n            9.6871e-01,  5.3168e-01],\n          [ 1.5900e-01,  2.5175e+00,  5.1810e-02,  ...,  5.2569e-01,\n           -3.7356e-01,  1.5979e+00],\n          ...,\n          [ 1.1682e+00,  6.3998e-01, -3.6601e-01,  ..., -1.2023e+00,\n           -7.2809e-01,  2.0168e+00],\n          [ 3.3163e-01,  9.8095e-01,  1.1956e-02,  ..., -3.6957e-01,\n           -1.6582e+00,  1.3001e+00],\n          [ 1.1252e-01,  7.8318e-01, -1.3791e+00,  ..., -2.3000e+00,\n           -1.7188e+00,  8.0002e-01]],\n\n         [[-2.0692e+00, -5.0200e-01, -9.0104e-01,  ..., -3.2384e-01,\n            3.2213e-02,  2.4289e-01],\n          [-7.3034e-02, -1.7994e-01,  1.9768e-01,  ..., -2.2953e-01,\n            6.4075e-01,  7.0413e-01],\n          [ 8.1838e-01, -1.1832e+00, -5.9078e-01,  ...,  6.0469e-01,\n            5.1034e-01,  4.5660e-01],\n          ...,\n          [ 2.0595e+00,  1.3245e-01,  1.3776e+00,  ...,  8.2466e-01,\n           -1.1954e+00,  1.3478e+00],\n          [ 3.3445e+00, -2.9066e-01,  1.5302e+00,  ...,  2.0510e-02,\n           -1.0299e+00,  2.3878e+00],\n          [ 3.7544e+00,  4.0665e-01,  6.3298e-01,  ...,  9.3438e-01,\n           -2.7292e-01,  1.6673e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[-1.0225e-01, -3.3589e-02,  3.7785e-02,  ...,  2.8276e-02,\n           -1.7043e-02, -1.6652e-01],\n          [-5.9800e-02,  4.5843e-02, -1.1369e+00,  ...,  4.7300e-01,\n            1.0152e+00,  1.9982e-01],\n          [ 4.6396e-02,  7.2068e-01, -1.7278e-01,  ..., -6.4567e-01,\n            2.4734e-01, -1.2815e-01],\n          ...,\n          [ 5.7812e-01, -2.0924e+00, -1.1098e+00,  ..., -1.3717e+00,\n           -6.7618e-01, -2.6552e-01],\n          [ 6.8826e-01, -5.4628e-01, -1.0737e+00,  ..., -1.3102e+00,\n           -1.1361e+00,  3.7829e-01],\n          [-8.1809e-01, -7.1762e-01, -1.4413e+00,  ..., -1.3244e+00,\n           -1.8579e+00, -8.8789e-01]],\n\n         [[ 4.7239e-02, -3.1296e-02, -3.8992e-02,  ...,  6.8573e-04,\n           -2.5185e-02,  5.6116e-02],\n          [ 9.2757e-02,  1.4187e+00, -5.3661e-01,  ..., -2.0773e-01,\n            9.9152e-01, -1.9251e-01],\n          [ 1.2265e+00, -4.2808e-01, -1.4994e+00,  ..., -5.7235e-01,\n            5.5024e-01, -1.0428e+00],\n          ...,\n          [-1.0495e+00,  1.4613e-01, -4.8381e-01,  ...,  1.9275e-01,\n            2.5773e+00,  1.5236e+00],\n          [ 1.6780e+00,  2.6381e+00,  9.1913e-01,  ...,  1.7850e+00,\n           -9.7313e-01,  4.3710e-01],\n          [ 5.7566e-01,  1.7228e-01,  4.1544e-01,  ...,  2.7862e+00,\n           -5.4163e-01, -5.3172e-01]],\n\n         [[-1.7336e-01, -1.2034e-02,  5.2982e-02,  ..., -6.7017e-02,\n            5.7353e-02,  1.5812e-02],\n          [ 1.9620e-01,  3.5744e-01,  1.7405e-01,  ...,  6.9350e-01,\n            9.5843e-02, -6.2069e-01],\n          [ 7.7754e-01, -7.9205e-01, -8.1418e-01,  ..., -1.1200e-01,\n           -1.3831e+00, -5.4963e-01],\n          ...,\n          [ 1.1328e+00,  1.9874e+00,  1.3393e-01,  ..., -8.6577e-02,\n            7.5616e-01, -1.7068e+00],\n          [ 1.6384e+00,  2.6157e+00,  1.1421e+00,  ...,  8.2403e-01,\n           -4.0340e-01, -2.2331e+00],\n          [ 2.6180e+00,  2.3276e+00,  2.2828e-01,  ...,  1.5298e+00,\n           -7.1356e-01, -2.1185e+00]],\n\n         ...,\n\n         [[-3.2882e-02,  2.3061e-03, -7.2032e-02,  ..., -3.6296e-02,\n           -1.0957e-02,  2.1145e-03],\n          [-4.1265e-01,  5.7199e-01,  5.8260e-02,  ...,  1.1723e+00,\n            3.6933e-01,  2.0152e+00],\n          [ 2.8911e+00,  1.8710e+00,  1.1077e+00,  ...,  6.4267e-01,\n            6.3881e-01, -4.7690e-01],\n          ...,\n          [ 9.0218e-01, -3.2867e+00, -1.1364e+00,  ..., -1.4921e-02,\n            8.7744e-01,  1.7557e+00],\n          [-5.4165e-01, -1.9330e+00, -1.5858e-01,  ...,  2.3179e-02,\n            1.1603e+00, -1.0531e+00],\n          [-6.3887e-01,  7.0724e-01,  4.1526e-01,  ...,  5.2921e-01,\n            3.1488e-01,  6.9241e-02]],\n\n         [[ 3.3629e-02,  4.0495e-02,  6.0463e-02,  ...,  3.1216e-02,\n           -5.4167e-02,  2.5725e-03],\n          [-1.2097e-02, -1.4983e-01,  1.8031e-01,  ...,  2.6715e-01,\n           -2.8884e-01, -5.0892e-01],\n          [ 4.6681e-02, -4.2925e-01,  3.2191e-01,  ...,  3.7348e-01,\n           -1.1473e+00,  2.1272e-01],\n          ...,\n          [ 2.3001e-01,  7.8834e-01, -4.3842e-01,  ...,  8.8292e-01,\n           -1.0453e+00,  5.2769e-01],\n          [ 1.1255e+00, -9.9096e-02, -3.7770e-01,  ..., -1.2857e+00,\n           -5.8382e-01,  6.4337e-01],\n          [ 7.3882e-02,  8.7844e-01,  4.4527e-01,  ...,  1.9682e-01,\n           -1.3681e+00,  3.5115e-01]],\n\n         [[ 9.1800e-02,  6.3657e-02,  2.8119e-02,  ...,  3.3984e-02,\n           -3.4366e-02,  4.5893e-03],\n          [ 8.3634e-01,  2.7675e-01, -1.2758e+00,  ...,  1.4161e+00,\n           -1.1398e+00, -2.8862e-01],\n          [-2.4373e-01,  1.1650e+00, -8.0559e-01,  ...,  2.3599e+00,\n           -1.6471e+00, -1.9871e+00],\n          ...,\n          [ 1.2168e+00, -7.6255e-01,  6.4503e-01,  ..., -4.1957e-01,\n            3.9009e-01,  1.2701e+00],\n          [ 2.7012e+00, -8.8828e-01, -2.6886e-01,  ..., -1.7065e+00,\n           -7.7961e-01, -2.9333e-01],\n          [ 1.3746e+00,  6.4494e-01,  3.3399e-01,  ..., -7.7459e-01,\n           -6.8212e-01, -5.4632e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-1.8538,  0.3373,  0.0269,  ...,  0.3742,  0.9387, -0.6357],\n          [-1.4332,  1.3304,  0.5377,  ...,  1.4094,  0.0339, -0.0599],\n          [-1.3879,  1.3471,  0.0830,  ...,  1.3568,  0.3998,  0.8613],\n          ...,\n          [-0.3973,  1.5675,  0.1615,  ..., -0.0943, -0.0571,  1.0504],\n          [-0.5974,  2.4424, -0.3121,  ..., -1.4400,  0.4787,  0.9639],\n          [ 1.3419,  1.8594, -1.4006,  ..., -1.3161, -0.6484, -0.2690]],\n\n         [[-0.2834,  0.9118,  2.7658,  ...,  0.1927,  0.3749, -0.5360],\n          [ 1.0475, -0.2919,  1.5158,  ..., -0.1755,  0.6454, -0.1966],\n          [ 1.7233, -0.8028,  0.5566,  ...,  0.4358,  0.3380,  0.4145],\n          ...,\n          [ 3.0686, -2.1198, -1.7750,  ...,  0.2197, -0.5501, -1.8212],\n          [ 2.0638, -1.4988, -1.9287,  ...,  0.5802,  0.6825, -0.8971],\n          [ 2.1807, -1.1990, -2.0481,  ...,  0.2298, -0.9835, -1.0646]],\n\n         [[-0.8111,  0.8346,  0.4478,  ..., -0.8623, -0.1096,  0.1767],\n          [-0.4158,  1.0715,  0.7467,  ..., -0.2087, -0.4280, -0.6946],\n          [-1.0867,  0.4501, -0.8183,  ..., -0.3752, -0.3266, -0.2980],\n          ...,\n          [-0.2154,  1.1874,  1.6057,  ...,  1.1321, -1.0810, -0.2639],\n          [ 0.0979,  0.0425,  1.9088,  ...,  1.5448, -0.7958,  0.3350],\n          [ 0.0544,  1.2777,  1.5472,  ...,  1.8829, -0.1429, -0.7955]],\n\n         ...,\n\n         [[ 0.4980,  0.5312, -0.5355,  ..., -0.3806,  0.6379,  1.2736],\n          [ 0.1239,  0.6253, -1.7790,  ..., -0.6147,  0.0640,  1.0943],\n          [-0.1819,  1.4372, -1.5819,  ...,  0.2594, -1.0311,  1.6451],\n          ...,\n          [-3.8195, -0.8606, -0.0384,  ..., -2.6253, -0.4176, -0.2442],\n          [-2.3930,  0.2834,  2.0269,  ..., -0.9737, -0.8169, -0.5952],\n          [-2.4334, -0.3947,  1.9298,  ..., -0.9720,  0.2417, -1.4448]],\n\n         [[ 0.1851,  0.3272,  1.4310,  ...,  1.0600, -0.3876,  0.6240],\n          [ 0.8346, -0.2720, -0.2128,  ...,  0.3104, -0.4811, -0.3280],\n          [ 0.8384, -1.0414,  1.1290,  ...,  1.3043, -1.5213, -0.1333],\n          ...,\n          [ 1.0972, -0.0436, -0.3343,  ...,  0.4360, -1.0838,  1.9904],\n          [ 0.6339, -0.4921, -0.7328,  ..., -0.5477, -1.8045,  2.7601],\n          [ 1.4702, -0.2906, -0.9036,  ..., -0.9267, -0.5142,  2.7514]],\n\n         [[-0.6630, -0.3470,  0.2558,  ...,  0.0503, -0.0062,  0.5274],\n          [-0.2690,  0.0867,  0.2868,  ..., -0.9159,  0.0156,  0.2699],\n          [ 0.7877, -0.2737,  0.3661,  ..., -1.3732, -0.4390, -1.1991],\n          ...,\n          [ 0.7432,  0.5370,  0.3801,  ..., -0.9487, -0.8608, -0.3010],\n          [ 2.2261,  0.5574, -0.8648,  ..., -0.6901,  0.4610, -1.8202],\n          [ 1.2014,  1.8430,  0.1258,  ..., -3.2701,  0.4271, -1.2860]]]],\n       grad_fn=<PermuteBackward0>), tensor([[[[-1.0931e-01,  8.5687e-02, -3.4830e-02,  ..., -1.9313e-01,\n            1.5816e-01, -2.8891e-02],\n          [-2.2714e-01, -4.7395e-01,  1.1304e+00,  ..., -5.4872e-01,\n           -1.0681e+00,  5.5090e-01],\n          [-1.1305e-01, -2.1486e-01,  1.9872e+00,  ...,  5.1186e-01,\n           -2.3078e+00,  1.4606e+00],\n          ...,\n          [-6.1662e-01, -6.6142e-01, -4.6649e-01,  ...,  2.3612e+00,\n           -2.1935e+00,  2.3250e+00],\n          [ 1.0346e+00, -7.8334e-01,  7.1940e-01,  ...,  3.5898e+00,\n           -8.8020e-01,  1.5594e+00],\n          [ 1.1645e+00,  4.0993e-01,  2.1964e+00,  ...,  1.7562e+00,\n           -2.4824e+00,  1.2539e+00]],\n\n         [[-1.6666e-01, -4.8379e-02,  1.4091e-01,  ...,  6.2192e-03,\n            2.1029e-01, -4.5950e-03],\n          [-1.4751e+00, -3.1443e-01,  9.2271e-01,  ...,  6.9528e-01,\n            1.0717e+00,  1.0570e+00],\n          [ 1.0109e+00, -1.5953e-01,  1.0335e+00,  ...,  1.4340e+00,\n            1.2995e+00, -2.5891e-01],\n          ...,\n          [ 3.2529e-01, -3.4115e-01, -1.0534e+00,  ..., -2.6833e+00,\n            3.5837e-01, -2.1026e+00],\n          [ 3.2298e-01,  1.4525e+00, -1.6604e+00,  ...,  1.2999e-03,\n           -6.9957e-01, -3.3631e+00],\n          [ 6.8949e-01, -1.7640e+00, -1.4007e+00,  ..., -2.5281e+00,\n            1.3917e+00, -8.4510e-01]],\n\n         [[-1.9005e-01, -1.9519e-01,  1.0384e-01,  ..., -6.8974e-02,\n           -7.1227e-02, -1.3442e-02],\n          [-5.4575e-01,  1.3717e+00,  2.9830e-01,  ..., -3.5064e-01,\n            1.0367e+00,  3.5869e-01],\n          [-2.3250e+00,  2.5908e+00, -4.6272e-01,  ..., -4.1331e-01,\n            3.2155e+00,  5.3559e-01],\n          ...,\n          [-3.3065e+00, -2.1037e+00, -3.2031e+00,  ...,  1.9337e+00,\n           -1.5915e+00,  3.2537e+00],\n          [-1.6532e+00,  8.7735e-01, -2.4955e-01,  ...,  1.8871e+00,\n           -2.6521e+00,  1.3123e+00],\n          [-1.2730e+00, -2.1654e-01, -2.1380e+00,  ...,  2.3023e+00,\n            5.7620e-01,  1.9117e+00]],\n\n         ...,\n\n         [[ 1.4009e-01,  1.2626e-01, -3.8329e-02,  ..., -1.3652e-02,\n           -1.9473e-02, -1.0895e-02],\n          [-2.5119e-01, -5.9523e-01, -6.8309e-01,  ..., -8.3538e-01,\n            3.1866e-01,  7.9912e-01],\n          [ 3.6005e-01, -5.9602e-01, -9.0759e-01,  ..., -8.8429e-01,\n           -3.0334e-01, -2.1959e-02],\n          ...,\n          [ 4.8829e-01,  2.0357e+00, -1.6404e+00,  ..., -1.4997e-01,\n           -6.3198e-02,  1.7738e+00],\n          [-7.8132e-01,  1.6426e+00,  1.1959e+00,  ...,  1.0483e-01,\n            4.2282e-01, -1.3982e+00],\n          [ 8.5656e-01, -4.5035e-01, -5.2497e-01,  ..., -1.0880e-01,\n           -7.7904e-01, -6.3176e-01]],\n\n         [[-9.1092e-04,  5.9787e-02, -1.1717e-01,  ..., -4.3412e-02,\n           -2.2547e-02,  3.5406e-02],\n          [ 8.3835e-01, -5.6795e-01, -1.0318e+00,  ..., -1.4119e-01,\n           -1.0089e+00, -4.2568e-01],\n          [ 1.3323e+00, -7.9603e-01,  1.4058e+00,  ...,  8.7779e-01,\n           -3.7914e-01, -1.8072e+00],\n          ...,\n          [ 1.6048e+00, -1.4587e+00, -2.3229e+00,  ...,  3.0744e-01,\n            1.1724e+00,  1.1777e+00],\n          [ 1.7612e+00, -1.8369e+00, -8.2630e-01,  ..., -3.8059e-01,\n           -9.4643e-01, -3.4247e-01],\n          [ 5.9939e-01, -1.5645e+00, -7.6303e-01,  ..., -1.9231e-01,\n           -1.8153e+00, -1.2791e+00]],\n\n         [[ 3.3956e-02,  6.1036e-02,  9.0944e-02,  ..., -6.9381e-02,\n           -2.2397e-02,  1.5191e-01],\n          [ 1.0886e-01,  9.4969e-01,  1.7650e+00,  ..., -5.4084e-02,\n            6.4813e-01,  3.7038e-01],\n          [-1.5424e+00,  8.2110e-01,  1.2281e+00,  ..., -5.8746e-01,\n            5.9679e-01,  1.4738e+00],\n          ...,\n          [ 4.6856e-01, -7.0286e-01, -1.2946e+00,  ..., -1.4577e-01,\n            2.6149e-01,  8.0905e-01],\n          [ 6.3292e-01, -1.9938e+00, -1.4344e+00,  ...,  1.0607e+00,\n            2.8183e-01,  1.2616e+00],\n          [ 8.7972e-01, -3.9359e-01, -2.9544e+00,  ..., -3.4412e-02,\n            1.1010e-01,  1.0850e+00]]]], grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None, cross_attentions=None)\n:Dictionary inputs to traced functions must have consistent type. Found Tensor and Tuple[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Step 2: Convert the model to TorchScript\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Example using trace\u001b[39;00m\n\u001b[1;32m     10\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|startoftext|>parmesan cheese, aubergines, courgettes\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m traced_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Alternatively, you can use script if your model contains control flow or dynamic shapes:\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# traced_model = torch.jit.script(model)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Step 3: Optimize the TorchScript model for mobile deployment\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# This step is optional but can improve inference speed and reduce model size\u001b[39;00m\n\u001b[1;32m     18\u001b[0m traced_model \u001b[38;5;241m=\u001b[39m traced_model\u001b[38;5;241m.\u001b[39moptimize_for_mobile()\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/jit/_trace.py:806\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    805\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_kwarg_inputs should be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    823\u001b[0m ):\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/jit/_trace.py:1074\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1073\u001b[0m     example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m-> 1074\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tracer cannot infer type of BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-2.0504e-02, -9.7479e-02, -1.0166e-02,  ...,  5.3485e-02,\n           7.0027e-04, -3.4996e-02],\n         [-1.7572e-03, -4.0842e-01, -1.6583e-01,  ...,  7.6675e-03,\n           3.6098e-01, -4.8115e-01],\n         [-5.8423e-01, -3.9608e-01, -3.0226e-02,  ..., -1.0967e-01,\n           6.4393e-02, -5.3426e-01],\n         ...,\n         [-1.3370e-01, -6.8587e-01, -5.1921e-01,  ...,  2.2168e-01,\n           2.3090e-01, -8.5214e-01],\n         [-1.4905e+00, -2.8908e-01,  9.4241e-01,  ...,  7.8099e-01,\n          -1.2476e-01,  4.3328e-02],\n         [ 4.3870e-01,  3.1624e-01, -9.3352e-02,  ...,  3.7711e-01,\n          -3.5457e-01,  2.6597e-01]]], grad_fn=<ViewBackward0>), past_key_values=((tensor([[[[ 0.5570,  0.4307, -0.5053,  ...,  0.1363,  0.9767,  0.1827],\n          [-0.0912,  0.5041, -0.3954,  ...,  0.3305,  0.3119,  0.4014],\n          [ 0.0072,  0.3909, -0.2975,  ...,  0.4898, -0.1312,  0.3635],\n          ...,\n          [-0.4274,  0.1554,  0.2435,  ..., -0.3244,  0.0434,  0.1394],\n          [-0.9649,  0.6916,  0.0453,  ...,  0.1980, -0.6092, -0.3877],\n          [-0.5655, -0.3282,  0.3821,  ...,  0.3691,  0.1234,  0.0638]],\n\n         [[-0.1360,  0.0653, -0.1330,  ...,  0.2739,  0.4653,  0.2990],\n          [ 0.1335, -0.7138, -1.1276,  ..., -0.1697,  2.4266,  2.0509],\n          [ 0.1810, -0.2216, -1.1807,  ..., -0.5042,  1.7773,  0.6654],\n          ...,\n          [-0.5312,  0.7201, -0.2335,  ...,  0.6675,  2.2712,  1.0832],\n          [-0.0121,  0.5752, -1.9812,  ..., -0.3047,  2.2088,  2.1648],\n          [-0.0128,  0.1358, -1.3117,  ..., -0.7086,  1.8075,  0.4795]],\n\n         [[-1.0590,  0.9298,  0.7462,  ...,  0.3235, -1.9719, -0.5007],\n          [-0.7101,  0.7712,  0.6021,  ..., -0.2041, -1.1030,  0.1506],\n          [-0.8992,  0.4033,  0.8071,  ..., -0.3003, -1.5242, -0.1516],\n          ...,\n          [ 0.3206,  0.3093, -0.0665,  ..., -0.2586, -0.5263,  0.4690],\n          [-0.4050,  0.6720,  0.1057,  ..., -0.7532, -0.6950,  0.2516],\n          [-0.8969, -0.1990, -0.4222,  ..., -0.5416, -0.5635,  0.3747]],\n\n         ...,\n\n         [[ 0.0858,  0.0722, -0.3878,  ...,  0.3259,  0.5415,  0.1013],\n          [ 0.2234,  0.0124, -0.1886,  ...,  0.4475,  0.2705,  0.1010],\n          [ 0.0496,  0.1294, -0.2966,  ...,  0.5023,  0.2179,  0.0952],\n          ...,\n          [ 0.1593,  0.0107,  0.4153,  ...,  0.6001,  0.2461, -0.5677],\n          [ 0.3655,  0.0877,  0.0523,  ...,  0.6682,  0.7060, -0.2129],\n          [ 0.2091,  0.2781, -0.3434,  ...,  0.3215,  0.2962, -0.3418]],\n\n         [[ 0.6778,  1.1371, -0.0898,  ...,  0.7484,  0.3885, -1.2396],\n          [ 0.5056,  0.5427, -0.0114,  ..., -0.2251, -0.3384, -0.8099],\n          [ 0.6741,  0.3555, -0.4862,  ...,  0.2175,  0.8025, -0.6468],\n          ...,\n          [ 0.8198, -0.8415, -0.5011,  ...,  0.2834,  0.2901, -0.0906],\n          [ 0.7880,  0.1045, -0.2394,  ..., -0.1340, -0.2220,  0.2862],\n          [ 0.3383, -0.6522, -0.0572,  ...,  0.6745,  0.5346,  0.2301]],\n\n         [[ 0.8520, -0.3360,  0.3494,  ...,  0.0521, -0.1131,  0.0193],\n          [ 0.6906, -0.0287,  0.3010,  ...,  0.3510, -0.0530,  0.5820],\n          [ 0.3735, -0.1988,  0.3173,  ...,  0.1107,  0.0266,  0.7005],\n          ...,\n          [-0.5795,  0.0208, -0.2925,  ...,  0.0193,  0.3640,  0.5842],\n          [-0.5743,  0.0940, -0.4091,  ..., -0.0844,  0.5802,  0.6936],\n          [-0.3374,  0.2189, -0.1000,  ...,  0.1029,  0.1641,  0.3679]]]],\n       grad_fn=<PermuteBackward0>), tensor([[[[-8.0180e-03,  8.0329e-03, -4.0732e-03,  ...,  1.1266e-02,\n           -4.7284e-02, -3.1585e-02],\n          [-5.0192e-02, -1.1495e-01,  9.3411e-02,  ..., -1.0769e-01,\n            7.7619e-02, -1.2615e-02],\n          [-5.5170e-02, -1.7998e-01, -3.5946e-02,  ...,  8.8814e-02,\n            1.0066e-01, -2.1808e-02],\n          ...,\n          [-1.7238e-01, -4.9123e-02,  2.8447e-01,  ...,  5.4339e-02,\n           -1.4692e-01,  2.1305e-01],\n          [-5.7185e-02, -9.5717e-02, -1.7490e-01,  ...,  4.6850e-02,\n            1.9946e-02, -2.3055e-02],\n          [-3.0795e-01,  1.6235e-02, -1.3527e-01,  ...,  3.2349e-04,\n           -1.6680e-01,  4.3219e-02]],\n\n         [[ 3.5610e-02, -5.6243e-03, -2.7740e-02,  ..., -2.3574e-01,\n           -3.5889e-02, -5.3016e-02],\n          [ 7.6815e-02,  1.8402e-01, -3.9058e-02,  ..., -4.8489e-02,\n            4.7006e-03, -1.2947e-01],\n          [-3.7526e-02, -2.9001e-02,  1.0461e-01,  ..., -9.8126e-02,\n           -6.7217e-02,  2.5450e-02],\n          ...,\n          [ 1.4372e-01,  2.4436e-01,  6.2322e-02,  ...,  1.9306e-01,\n            1.8172e-02,  6.6559e-02],\n          [ 1.3890e-01,  2.3108e-02,  2.6508e-01,  ...,  2.0737e-01,\n           -1.6579e-01, -1.0938e-01],\n          [ 9.3900e-02,  1.3562e-01, -4.1007e-02,  ...,  3.0970e-01,\n            6.2654e-02,  9.8444e-02]],\n\n         [[-3.7733e-03,  3.0594e-02, -4.4296e-03,  ..., -6.9540e-02,\n           -4.8268e-02, -7.9085e-03],\n          [ 3.2578e-02,  1.1431e-01, -3.0824e-02,  ...,  1.5780e-02,\n            3.2049e-02,  3.9024e-02],\n          [ 5.0971e-02,  2.1678e-02,  6.7992e-02,  ..., -8.3660e-02,\n           -1.4788e-01,  7.8971e-02],\n          ...,\n          [ 1.5150e-01,  2.6723e-02,  5.0081e-02,  ...,  2.0863e-02,\n            8.2861e-02, -9.7223e-03],\n          [-4.7415e-02, -3.0159e-02, -1.4404e-01,  ..., -2.6329e-02,\n           -1.9738e-01,  1.3340e-01],\n          [ 9.4655e-02,  1.4119e-01, -8.0678e-02,  ...,  4.4626e-02,\n            1.7816e-02, -8.9467e-02]],\n\n         ...,\n\n         [[ 5.6597e-02, -6.2191e-02, -1.6690e-01,  ...,  4.4933e-02,\n           -1.5649e-02,  1.0282e-01],\n          [ 7.5998e-03, -4.2261e-02,  6.9691e-02,  ..., -5.2961e-02,\n            1.9873e-02,  1.4563e-02],\n          [ 4.1898e-02, -8.6402e-02,  1.7153e-01,  ...,  6.2017e-02,\n            8.5031e-02,  5.0571e-02],\n          ...,\n          [ 1.1614e-01,  1.2795e-01,  1.7068e-01,  ...,  7.9671e-02,\n           -6.9800e-02, -1.2324e-01],\n          [-2.2107e-01, -9.8541e-02, -1.9118e-03,  ..., -3.2950e-02,\n            1.8100e-02,  1.1982e-02],\n          [-9.4097e-02, -1.2692e-01,  2.4174e-02,  ...,  9.5763e-02,\n            1.3953e-01,  2.2491e-02]],\n\n         [[ 2.2242e-02,  5.7131e-02, -3.0762e-02,  ...,  2.6214e-02,\n            2.9970e-03,  3.3986e-03],\n          [ 1.9044e-04,  4.8271e-03,  1.2718e-01,  ..., -3.2988e-02,\n           -1.5783e-01, -1.1890e-01],\n          [-1.5572e-02,  4.5726e-02,  9.2194e-02,  ..., -3.0128e-02,\n            2.9833e-02, -1.0219e-02],\n          ...,\n          [-9.2922e-02, -3.2937e-02,  1.4097e-01,  ...,  2.5838e-01,\n           -1.9086e-01,  1.5413e-01],\n          [-9.4651e-02,  5.3090e-02, -4.3112e-02,  ..., -1.0775e-03,\n            2.3202e-02, -1.2163e-01],\n          [-2.5654e-01, -6.2985e-02,  4.3611e-02,  ..., -7.3058e-02,\n            2.8411e-02, -1.8159e-01]],\n\n         [[ 7.2680e-02,  5.9114e-02,  4.0050e-02,  ...,  7.9343e-03,\n            5.7168e-02, -1.4002e-03],\n          [-3.3211e-03,  1.3341e-01, -4.4178e-02,  ..., -1.2128e-03,\n            1.5463e-01,  9.6027e-02],\n          [-3.4787e-02, -8.4045e-02,  4.3126e-02,  ...,  7.4981e-02,\n            1.2282e-01,  1.6133e-01],\n          ...,\n          [-7.6664e-03,  7.0749e-02, -9.2053e-02,  ..., -9.6781e-02,\n           -6.4669e-02, -6.3876e-02],\n          [-6.9160e-02, -7.7359e-02, -1.7227e-02,  ...,  1.3172e-01,\n            2.7567e-01,  3.9278e-02],\n          [ 1.3714e-01,  5.5284e-03,  1.0415e-02,  ..., -1.5494e-01,\n            2.9333e-01, -7.0809e-02]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 1.1793e-02,  6.8702e-02,  2.5883e-02,  ..., -6.1109e-01,\n            3.2751e-01, -1.5668e-01],\n          [ 9.9393e-01, -2.7619e+00,  1.6909e-01,  ..., -7.1415e-01,\n            1.4749e-02, -4.9081e-01],\n          [ 3.1611e-01, -2.9056e+00, -1.3283e+00,  ..., -1.1175e+00,\n           -8.9866e-02, -3.0059e-01],\n          ...,\n          [ 2.6685e-01, -3.9288e+00,  3.4233e-01,  ..., -1.5560e+00,\n            3.1667e-01,  5.8307e-01],\n          [ 1.0176e+00, -3.6953e+00,  5.3016e-01,  ..., -1.8431e+00,\n            1.4715e+00, -3.4032e-01],\n          [ 1.2149e+00, -4.1336e+00,  9.5283e-02,  ..., -1.9314e+00,\n            1.0613e+00,  4.5978e-01]],\n\n         [[-5.7965e-01,  8.8246e-02, -2.0743e-01,  ...,  1.1906e+00,\n           -8.7433e-01, -4.7180e-01],\n          [ 8.2884e-01,  9.9997e-01, -2.3276e-01,  ...,  1.0888e+00,\n            1.9437e-01,  8.8736e-01],\n          [ 1.6898e-01, -1.3176e-01, -9.2537e-02,  ...,  1.4694e-01,\n           -8.9460e-01,  1.1820e+00],\n          ...,\n          [-1.9770e-01, -6.6178e-02, -1.8101e-03,  ..., -2.6230e-01,\n            4.0006e-01, -5.9551e-03],\n          [ 5.7990e-01, -8.6826e-01, -3.6566e-01,  ...,  8.2626e-01,\n           -2.5984e-01, -6.5563e-01],\n          [ 1.0927e-01, -1.2468e+00, -4.2303e-01,  ...,  4.9072e-01,\n            1.2922e-01,  1.1370e-01]],\n\n         [[ 4.0789e-01,  1.2727e+00,  1.8788e+00,  ...,  5.9780e-01,\n            8.7386e-01,  4.1653e-02],\n          [ 4.9057e-02,  2.1836e+00, -3.1013e-02,  ..., -1.3722e+00,\n            8.4098e-01, -2.3731e-01],\n          [-1.5963e+00,  2.5154e+00, -3.4983e-01,  ..., -1.1286e+00,\n            1.0861e+00, -5.1332e-01],\n          ...,\n          [-1.8768e+00,  2.4299e+00, -2.1719e+00,  ..., -2.2557e+00,\n            1.2504e-01, -1.4398e+00],\n          [-2.1156e+00,  2.0658e+00, -2.5702e+00,  ..., -3.3195e+00,\n           -3.5918e-01, -4.0161e-01],\n          [-2.4121e+00,  1.3971e+00, -2.5321e+00,  ..., -4.1632e+00,\n            7.5720e-01, -1.2648e+00]],\n\n         ...,\n\n         [[ 1.0486e+00, -6.8936e-01, -1.2578e+00,  ...,  1.1058e+00,\n            1.6745e+00,  4.8036e-01],\n          [ 3.7329e-01, -4.0185e-02, -2.0516e-01,  ..., -6.7562e-01,\n            8.6103e-02,  3.1120e-01],\n          [-1.2242e+00,  3.5845e-01, -1.5191e+00,  ...,  1.7472e-01,\n           -2.0401e+00,  8.2646e-01],\n          ...,\n          [-1.2020e+00,  6.4826e-02, -9.4485e-01,  ..., -6.1301e-01,\n           -2.4546e+00,  1.0371e+00],\n          [-1.9283e+00,  9.5584e-01, -2.8252e-01,  ..., -6.1653e-01,\n           -2.8950e+00, -5.2733e-01],\n          [-1.9092e+00,  7.2577e-01, -2.2470e-01,  ..., -6.7239e-01,\n           -3.2917e+00, -3.8621e-01]],\n\n         [[ 1.3994e+00,  5.1456e-02,  6.3613e-01,  ..., -3.3289e-02,\n           -6.6591e-01, -1.8809e-01],\n          [ 8.7419e-01, -7.6987e-01,  5.6867e-01,  ...,  1.0583e+00,\n           -1.8279e+00, -1.9588e-01],\n          [ 6.5777e-01, -3.9376e-01,  2.4107e-01,  ...,  4.1576e-02,\n           -2.1135e+00, -8.1792e-01],\n          ...,\n          [ 8.9915e-01,  9.5046e-01, -1.5666e-01,  ...,  6.1330e-01,\n           -1.0625e+00,  3.6371e-01],\n          [ 9.0144e-01,  1.2193e-01,  3.7665e-01,  ..., -6.9513e-01,\n           -1.0217e+00,  3.1789e-01],\n          [ 1.4913e+00, -3.2076e-01,  5.7462e-01,  ...,  1.4740e-01,\n           -9.6536e-01, -7.0432e-02]],\n\n         [[ 7.5715e-02,  7.9794e-02, -1.8751e-01,  ..., -1.3675e-02,\n            8.8612e-02, -4.2339e-02],\n          [-6.1103e-01, -4.8150e-02, -5.8175e-01,  ..., -5.3281e-01,\n            1.7137e-01, -1.1033e+00],\n          [-1.1744e-01,  2.7051e-01, -4.9298e-01,  ..., -5.6000e-01,\n            5.3636e-01,  2.5293e-03],\n          ...,\n          [ 3.4148e-01, -5.5151e-01, -1.9702e-01,  ...,  1.0008e+00,\n            1.9048e-01, -2.1098e-01],\n          [ 3.6585e-01,  7.4925e-02, -2.0566e-01,  ..., -3.6997e-01,\n            6.2052e-02,  5.7593e-01],\n          [ 2.4069e-01, -1.6551e-01,  5.0819e-01,  ...,  1.8271e-01,\n            8.4506e-01,  2.5362e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[-0.0216, -0.0634, -0.1309,  ...,  0.0207, -0.1429, -0.3435],\n          [-0.2166, -0.8090,  0.4947,  ..., -0.0640,  0.6865,  0.5147],\n          [-0.5278, -0.3991,  0.5053,  ...,  1.1802,  1.0349,  0.2762],\n          ...,\n          [ 0.0773, -0.4442,  0.3032,  ...,  0.1042,  0.0181,  0.8915],\n          [ 0.7713, -1.2358,  0.7985,  ...,  0.1847,  0.3874,  0.2830],\n          [ 0.5416, -1.1191,  0.5999,  ...,  0.0558,  0.7045, -0.0562]],\n\n         [[ 0.0154, -0.0484,  0.1736,  ..., -0.0458, -0.0265,  0.0054],\n          [ 0.1130, -1.2983,  0.3993,  ..., -0.9947, -0.1493,  0.2514],\n          [ 0.2502, -1.2443,  0.8783,  ..., -0.0324,  0.0656, -0.3491],\n          ...,\n          [-0.2334, -1.0844,  0.4362,  ...,  1.1638, -0.2594, -0.3856],\n          [-0.2402, -0.8605,  1.0080,  ...,  0.7598, -0.5325, -0.0308],\n          [-0.5397, -0.6933,  1.3280,  ...,  1.1463, -0.3266,  0.3956]],\n\n         [[ 0.0221,  0.0467, -0.0992,  ...,  0.0870,  0.0078,  0.0500],\n          [-0.1187, -0.5206, -0.4095,  ..., -0.6356, -0.6862,  0.5913],\n          [ 0.5174, -1.2189,  0.0714,  ...,  0.5196, -0.0588,  0.1633],\n          ...,\n          [ 0.1438, -0.4790, -0.7399,  ...,  0.4313,  1.0592, -0.4186],\n          [ 0.8644, -0.8879, -0.1108,  ...,  1.0016, -0.4976, -0.9884],\n          [-0.2870, -0.7563,  0.2315,  ...,  0.3784, -0.1330, -1.7259]],\n\n         ...,\n\n         [[-0.0368, -0.0812,  0.0243,  ...,  0.0948,  0.0835, -0.0463],\n          [-0.2196,  0.1204, -0.4731,  ...,  0.6988, -0.0452, -0.0662],\n          [ 0.1808,  0.7153, -0.5813,  ...,  1.1190,  0.4318,  0.2993],\n          ...,\n          [-0.2225, -0.7455,  0.3972,  ...,  0.3874, -0.0231, -0.0094],\n          [ 0.9128,  0.4409, -0.3444,  ..., -0.3482,  0.2007,  0.0128],\n          [ 0.2070, -0.9902,  0.4238,  ..., -0.1258, -0.3764,  0.1518]],\n\n         [[-0.0486, -0.0825, -0.1783,  ...,  0.0604,  0.0468,  0.0403],\n          [-0.4465, -0.0717,  0.7522,  ..., -0.1395,  0.2353, -0.1800],\n          [ 0.4464,  0.2111,  0.3491,  ..., -0.8877,  0.7942, -1.0405],\n          ...,\n          [ 0.6770, -0.0920, -0.2157,  ...,  0.1688, -0.1732,  0.3300],\n          [-0.4555,  0.1030,  1.3020,  ..., -0.1254,  0.0357, -0.3885],\n          [-0.0145, -0.1891,  1.2536,  ...,  0.0664,  0.1096, -0.4065]],\n\n         [[ 0.0250, -0.0799, -0.0031,  ..., -0.0195,  0.2424, -0.0070],\n          [-0.1717,  0.3711,  0.4196,  ..., -0.9720, -0.2964, -0.2742],\n          [-0.1718,  1.0055,  0.9368,  ..., -0.0968, -0.9966,  0.2085],\n          ...,\n          [-0.3634, -0.3298, -0.1930,  ...,  0.2963, -0.4168,  0.4885],\n          [-0.1954, -0.2581,  0.3884,  ..., -0.4952, -0.3121,  0.3417],\n          [ 0.2222, -0.3965,  0.7119,  ..., -1.1530, -0.5427, -0.0055]]]],\n       grad_fn=<PermuteBackward0>)), (tensor([[[[-1.8677e+00,  4.1505e-02,  4.8454e-03,  ..., -9.4523e-01,\n            4.5578e-01, -3.2969e+00],\n          [-1.5114e+00,  7.6960e-01,  1.8194e-01,  ...,  4.1868e-01,\n            6.3745e-01, -1.5788e+00],\n          [-1.5890e+00,  6.7367e-02, -2.7600e-01,  ..., -2.8530e-01,\n            3.6418e-01, -1.3652e+00],\n          ...,\n          [-1.3517e+00, -1.5604e+00, -1.7869e+00,  ..., -1.6559e+00,\n           -1.4155e+00,  2.8737e-01],\n          [-2.1506e-01, -1.9834e-01, -3.8074e-01,  ..., -1.2966e+00,\n           -1.5251e+00,  9.3664e-02],\n          [-1.4389e+00,  4.1704e-01, -2.4139e+00,  ..., -1.3451e+00,\n           -1.1002e+00,  4.5198e-01]],\n\n         [[ 2.8162e-01,  2.3912e-01,  5.7205e-01,  ...,  2.8247e-01,\n            3.5734e-01, -2.9627e+00],\n          [-1.7394e+00,  2.4530e-01,  1.7256e+00,  ..., -2.2244e+00,\n           -6.8350e-01,  6.1590e-01],\n          [-1.9922e+00,  5.3320e-01,  2.1084e+00,  ..., -2.0770e+00,\n           -2.4719e+00,  2.6142e+00],\n          ...,\n          [-1.5102e+00,  3.4903e-01,  2.3285e+00,  ..., -3.0108e+00,\n           -1.7068e+00,  3.6820e+00],\n          [-8.7199e-01,  8.3163e-01,  2.2725e+00,  ..., -1.8048e+00,\n           -2.3996e+00,  4.0445e+00],\n          [-4.5799e-01, -9.1126e-03,  3.4736e+00,  ..., -2.8632e+00,\n           -1.4199e+00,  4.3720e+00]],\n\n         [[ 2.2915e-01, -1.2882e+00, -7.0178e-01,  ...,  3.8016e-01,\n           -1.5352e-01, -3.9352e-01],\n          [ 5.1941e-02,  1.2321e+00,  1.5882e-01,  ...,  3.2166e-01,\n            2.1598e-01,  8.5408e-01],\n          [ 9.4449e-02,  9.0423e-01,  1.8057e-01,  ..., -2.1943e-01,\n            6.4667e-01, -7.1334e-02],\n          ...,\n          [ 5.7649e-01,  3.3475e+00,  3.9305e-01,  ..., -9.0214e-02,\n            1.2997e-02,  8.9739e-01],\n          [ 4.7836e-02,  2.8612e+00,  2.6689e-01,  ..., -6.1207e-01,\n            8.7766e-01,  8.3327e-01],\n          [ 3.1726e-01,  3.6431e+00,  2.9849e-01,  ..., -1.3564e-01,\n            4.4745e-01, -5.7273e-01]],\n\n         ...,\n\n         [[-7.7266e-01,  5.5429e-03, -2.3083e-01,  ...,  1.3252e+00,\n           -3.0876e-01,  2.4335e+00],\n          [ 3.5627e-01, -7.6413e-01,  8.3112e-01,  ...,  1.0047e+00,\n            1.2336e+00,  1.9129e+00],\n          [ 3.1758e-01, -1.7131e+00,  9.2744e-01,  ...,  1.2690e+00,\n            1.1416e+00,  1.4016e+00],\n          ...,\n          [ 1.3919e+00, -1.8304e+00,  1.1591e+00,  ..., -4.1035e-01,\n           -3.4167e-01, -3.4256e-01],\n          [ 1.7601e+00, -1.5028e+00,  4.0935e-01,  ..., -4.6187e-01,\n           -2.8926e-01,  2.4551e-01],\n          [ 3.9764e-01, -3.9966e-01,  4.8949e-01,  ..., -5.9412e-01,\n            3.9944e-01, -1.4487e-02]],\n\n         [[-2.6041e-01, -2.6286e-01,  4.2083e-01,  ...,  4.4153e-01,\n            3.7089e-01,  2.7412e-01],\n          [ 3.8969e-01, -6.2055e-01, -1.5017e+00,  ...,  1.2362e+00,\n            3.4629e-01,  3.8472e-01],\n          [ 2.4498e-01,  4.5572e-01, -7.1889e-01,  ...,  3.9112e-01,\n            6.0691e-01,  4.9542e-01],\n          ...,\n          [ 1.4194e+00, -5.9293e-01,  2.4916e-01,  ...,  1.1707e+00,\n           -3.5224e-01, -1.1789e+00],\n          [ 2.0845e+00,  6.1076e-01, -1.2510e+00,  ...,  2.6020e-01,\n           -4.9280e-01,  1.2502e-01],\n          [ 2.0164e+00, -5.6132e-01, -8.3646e-01,  ...,  1.9588e+00,\n           -5.2657e-01, -7.0055e-01]],\n\n         [[ 1.8915e+00,  1.8761e+00, -9.0029e-01,  ..., -2.0158e+00,\n           -1.3396e+00, -4.0708e-01],\n          [ 6.4143e-01, -7.1463e-01, -1.1588e+00,  ..., -5.0721e+00,\n           -5.2149e+00, -8.6966e-01],\n          [ 9.6577e-01,  2.1570e-01,  3.2774e-01,  ..., -4.1085e+00,\n           -6.6959e+00, -1.1846e+00],\n          ...,\n          [ 5.6866e-01,  3.0307e+00, -1.0974e+00,  ..., -3.2477e+00,\n           -3.6591e+00,  1.7849e+00],\n          [ 6.2218e-01,  1.7561e+00, -1.6330e+00,  ..., -4.1662e+00,\n           -3.3693e+00,  8.4492e-01],\n          [-9.9718e-02, -3.5874e-01,  6.3078e-02,  ..., -5.7476e+00,\n           -1.8059e+00, -5.6820e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 0.0199, -0.0087, -0.0124,  ...,  0.0263, -0.1341,  0.0986],\n          [-0.1910,  0.1115,  0.7845,  ..., -0.3363, -0.3910, -0.6377],\n          [-0.9952,  0.3961,  0.0891,  ...,  0.3636, -0.1562,  0.1998],\n          ...,\n          [-0.2009, -0.2968, -0.4515,  ..., -0.7580, -0.1851, -0.4964],\n          [ 0.4249, -0.2667,  0.2276,  ...,  0.9889, -0.3405,  0.1015],\n          [-0.0678, -0.1611,  0.7415,  ...,  0.5653,  0.8791, -0.5364]],\n\n         [[-0.0126, -0.0066, -0.1787,  ..., -0.1315,  0.0114,  0.0551],\n          [ 0.7682,  0.2922, -0.6003,  ...,  0.4310,  0.4087,  0.1082],\n          [ 1.0197,  0.9754, -0.3150,  ..., -0.7197, -0.5154,  0.4147],\n          ...,\n          [ 0.1960, -0.1133, -0.1150,  ..., -0.0079,  0.1328,  0.3555],\n          [ 0.1820, -0.4934, -0.2292,  ..., -0.3090, -0.6940, -0.2304],\n          [ 0.2250, -0.5213, -0.8863,  ...,  0.5461, -0.2228,  0.3499]],\n\n         [[ 0.1531, -0.0713,  0.0145,  ..., -0.0631,  0.0881,  0.1338],\n          [ 0.0529,  0.0674, -0.9629,  ..., -0.4858,  0.2818, -0.0509],\n          [ 0.2037, -0.4008, -0.7720,  ..., -0.4401,  0.8206, -0.1498],\n          ...,\n          [ 0.1390,  0.1519,  0.7736,  ...,  0.4154, -0.2232,  0.5556],\n          [ 0.0287, -0.3214, -0.0535,  ...,  0.0623, -0.3038,  0.4514],\n          [-0.0312, -0.1729,  0.5680,  ...,  0.5946, -0.3815, -0.2047]],\n\n         ...,\n\n         [[ 0.0270,  0.0596, -0.0467,  ...,  0.1865,  0.0507, -0.0871],\n          [ 1.0828, -0.1267, -0.2870,  ..., -0.7420, -0.1834,  0.5953],\n          [-0.0480, -0.5166, -0.7910,  ..., -0.6132, -1.0172,  0.4957],\n          ...,\n          [ 1.2030, -0.3084,  0.8490,  ...,  0.4454, -0.8504, -0.6449],\n          [ 1.4094, -0.2948,  0.0935,  ...,  0.2719, -0.3546, -0.2002],\n          [ 0.0777, -0.4887, -0.1382,  ..., -0.0304, -0.5213, -0.1072]],\n\n         [[-0.0557, -0.0176,  0.0131,  ..., -0.0219,  0.0617,  0.0667],\n          [-0.5416,  1.0859, -0.3715,  ...,  0.3784, -0.5952, -0.7176],\n          [-0.7959,  0.2060, -0.5209,  ...,  0.4164, -0.7018, -0.8632],\n          ...,\n          [ 0.7245,  0.2064, -1.0559,  ...,  1.2667,  0.6346, -0.9088],\n          [ 0.6433,  0.2631,  0.0091,  ...,  1.2686,  0.1255, -0.8010],\n          [ 1.3426,  0.1523, -0.1900,  ...,  1.1599, -0.4946, -1.3226]],\n\n         [[ 0.0495,  0.0535,  0.1325,  ..., -0.0052, -0.0826, -0.0672],\n          [-0.2958, -0.2405, -0.1993,  ..., -0.4075, -0.2964,  0.7885],\n          [ 0.3288, -1.0919, -0.1170,  ...,  0.3375,  0.0609, -0.0716],\n          ...,\n          [-0.1209, -0.4789,  1.1902,  ..., -0.1331, -0.5614, -1.6860],\n          [-0.9203, -1.0198,  0.2325,  ...,  0.0467, -0.9221, -0.7359],\n          [-0.4341, -0.8488,  0.5657,  ...,  0.2104, -0.6752, -0.9123]]]],\n       grad_fn=<PermuteBackward0>)), (tensor([[[[ 9.8602e-01, -1.3410e-01, -3.2487e-01,  ...,  8.3973e-01,\n            1.1067e+00, -6.8142e-01],\n          [-1.1772e+00, -9.4043e-01, -1.1896e+00,  ..., -1.1448e+00,\n           -2.0120e+00, -1.1364e+00],\n          [-2.5447e+00, -1.6413e+00, -1.3254e+00,  ..., -1.3606e+00,\n           -4.1592e+00, -2.4504e+00],\n          ...,\n          [-2.8003e+00, -8.0995e-01,  8.5006e-01,  ..., -3.1223e+00,\n           -4.1719e+00, -2.7691e+00],\n          [-3.0035e+00, -1.7250e+00,  1.6454e+00,  ..., -2.6978e+00,\n           -4.1289e+00, -3.3616e+00],\n          [-4.1666e+00, -3.0245e-01,  7.0552e-01,  ..., -1.9955e+00,\n           -4.2735e+00, -1.9060e+00]],\n\n         [[-2.3149e-01, -2.1230e-01, -5.3774e-02,  ..., -8.0137e-02,\n           -7.7574e-01, -8.3382e-02],\n          [-4.6553e-01,  7.0847e-01,  6.2259e-01,  ..., -1.0885e-01,\n            6.1795e-02,  3.5720e-01],\n          [ 2.1200e-01, -2.3550e-01, -3.7807e-02,  ..., -7.4502e-01,\n            2.4035e-01,  5.5630e-01],\n          ...,\n          [-5.8594e-01, -1.3862e+00,  2.6233e-01,  ...,  3.3460e+00,\n            3.4873e-01,  2.8372e-01],\n          [-1.0115e+00, -1.4555e-03,  4.3157e-01,  ...,  3.2180e+00,\n            2.2698e+00,  1.3455e+00],\n          [ 3.7707e-01, -1.3358e+00,  6.5898e-01,  ...,  2.5937e+00,\n            1.7928e+00,  2.1578e+00]],\n\n         [[ 6.6125e-02,  5.7937e-02,  9.6843e-01,  ..., -6.1193e-01,\n            1.4991e-01, -8.6127e-01],\n          [ 8.0330e-01, -1.6499e-01,  1.5503e+00,  ...,  2.1702e-01,\n            4.2679e-01, -6.5554e-02],\n          [-3.0969e-01, -9.8851e-01,  1.4250e+00,  ...,  1.5518e-01,\n           -4.6595e-02, -8.8978e-02],\n          ...,\n          [-6.1740e-01,  6.1671e-01, -1.0059e+00,  ..., -1.1532e+00,\n           -2.0528e+00,  9.6587e-01],\n          [-4.6263e-01, -1.2255e+00, -4.0107e+00,  ..., -7.1859e-01,\n           -4.7069e+00,  1.4519e+00],\n          [-4.0274e-01, -2.9628e-01, -3.2088e+00,  ..., -8.8920e-01,\n           -4.3938e+00,  2.8905e-01]],\n\n         ...,\n\n         [[-2.2831e-01,  1.6811e-01, -3.4238e-01,  ..., -4.6979e-02,\n            2.3756e-01,  1.9369e-01],\n          [-1.6696e+00, -2.7342e-01, -2.0884e-01,  ..., -6.8122e-02,\n            6.4015e-02,  6.0151e-01],\n          [-1.3304e+00, -2.5178e-01,  3.1522e-01,  ..., -2.4766e-01,\n           -2.0102e-01, -2.4570e-01],\n          ...,\n          [-4.7155e-01, -1.7459e-01, -1.3429e+00,  ...,  3.2283e-01,\n           -8.0067e-01, -9.7069e-01],\n          [-1.6160e+00, -2.4109e-01,  1.0443e+00,  ...,  9.1587e-01,\n           -1.9325e-01, -7.4547e-01],\n          [-1.4089e+00, -2.3552e-01,  8.0522e-01,  ...,  1.1389e+00,\n           -4.9494e-01, -1.6224e+00]],\n\n         [[-2.3939e-01, -2.3753e+00,  2.0543e-01,  ..., -7.7681e-02,\n           -1.1844e-01,  8.7508e-01],\n          [-3.6298e-01, -2.0523e+00, -3.4474e-01,  ...,  7.7182e-01,\n            2.9637e-01,  2.4739e-01],\n          [-5.6639e-01,  8.3187e-01, -1.9911e+00,  ...,  2.1034e-01,\n           -4.8029e-01, -1.5272e-01],\n          ...,\n          [ 2.5445e+00,  9.2938e-01, -1.3311e+00,  ..., -3.9055e-01,\n           -9.2660e-01, -1.5473e+00],\n          [-1.1802e+00,  3.3015e+00,  1.2435e-01,  ..., -2.2730e+00,\n           -3.6787e-01, -7.8107e-01],\n          [ 1.0142e+00,  2.6261e+00,  6.2323e-01,  ..., -1.1425e+00,\n           -7.0328e-01,  2.0096e+00]],\n\n         [[ 3.9209e-01, -1.5932e-01, -5.0435e-02,  ...,  1.8377e-01,\n           -1.6472e-01,  4.7756e-01],\n          [ 4.9601e-01,  3.8713e-01,  3.8588e-01,  ..., -8.8805e-02,\n            3.1678e-01,  3.6347e-02],\n          [ 4.8462e-01,  1.2816e+00,  4.0377e-01,  ...,  3.8783e-01,\n            1.7954e-01, -1.3614e-01],\n          ...,\n          [-1.9095e+00, -4.5522e-01,  3.2686e-01,  ..., -1.0532e-01,\n           -2.5792e+00, -2.4526e+00],\n          [-1.5031e+00, -8.4546e-01, -6.5360e-02,  ..., -4.8020e-01,\n           -1.1478e-01, -9.0569e-01],\n          [-1.2446e+00, -2.4308e-01,  6.5936e-02,  ...,  4.0389e-01,\n           -1.8221e+00, -1.9281e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[-6.2752e-02,  5.3543e-02, -2.1092e-02,  ..., -1.0406e-01,\n            4.1936e-02,  8.6872e-02],\n          [ 5.1621e-02, -6.1137e-01, -1.0092e-01,  ..., -8.4237e-02,\n           -1.7137e-01, -3.5616e-01],\n          [-1.2210e-01,  4.1154e-01, -1.1133e+00,  ...,  2.9637e-01,\n           -6.3050e-01, -1.6766e-01],\n          ...,\n          [-5.1030e-01, -1.2980e+00, -3.4629e-01,  ..., -3.0695e-01,\n           -3.8914e-01, -8.9414e-01],\n          [ 1.4606e-02, -8.7192e-01, -1.2821e+00,  ...,  1.1450e+00,\n           -1.0162e+00,  4.9151e-01],\n          [-8.7282e-01, -2.5271e+00, -6.6079e-01,  ...,  1.5294e-01,\n            6.1572e-03,  1.5273e-01]],\n\n         [[-2.2934e-02,  2.0868e-04,  2.0920e-01,  ...,  6.0070e-02,\n           -1.7674e-02, -2.4415e-02],\n          [ 1.0024e-01,  2.9254e-02,  3.9656e-01,  ..., -4.3732e-01,\n           -4.5017e-02, -4.1847e-01],\n          [ 1.2605e+00,  3.9107e-01,  3.9869e-01,  ..., -1.4805e+00,\n            5.9956e-01,  4.4918e-01],\n          ...,\n          [ 1.6552e+00,  1.1162e+00, -1.3810e+00,  ...,  7.6276e-01,\n            1.7005e+00,  7.9900e-01],\n          [ 2.2720e+00,  6.2952e-01, -1.0182e-01,  ...,  1.0831e-01,\n            8.7174e-01, -8.4184e-02],\n          [ 2.4602e+00, -1.5281e-01, -3.7974e-01,  ..., -2.2553e-01,\n            2.4813e+00, -1.4800e-01]],\n\n         [[ 1.2206e-03,  6.2199e-03, -2.9682e-02,  ..., -4.1766e-02,\n           -2.7108e-03,  4.6978e-03],\n          [ 1.4679e+00,  9.3104e-03, -6.8771e-01,  ..., -1.2577e-02,\n           -8.1340e-01, -4.2228e-01],\n          [ 8.6120e-01, -4.9433e-03,  1.1586e+00,  ..., -1.5813e+00,\n           -1.6080e+00, -6.4140e-01],\n          ...,\n          [-6.0278e-01,  1.2376e+00,  1.7698e+00,  ...,  4.1949e+00,\n            8.4232e-01,  2.2840e+00],\n          [-8.5422e-02,  2.2420e-01,  8.8870e-01,  ..., -1.8558e+00,\n            4.2597e-01, -9.1149e-02],\n          [ 1.8756e+00,  3.3461e-01,  3.1553e+00,  ...,  7.9838e-01,\n            7.8259e-01,  9.0349e-01]],\n\n         ...,\n\n         [[ 8.6164e-02, -1.4784e-01, -6.8703e-02,  ...,  3.2957e-02,\n            5.5753e-02, -7.5036e-02],\n          [-3.4574e-01, -1.7536e-01,  2.8682e-01,  ...,  2.2871e-01,\n           -5.9915e-02, -4.8490e-01],\n          [ 1.3784e-01, -7.5136e-01,  7.5124e-01,  ..., -5.9130e-01,\n           -1.9978e-01, -5.6584e-01],\n          ...,\n          [ 1.4864e+00,  6.7357e-01,  6.1315e-01,  ...,  1.3113e+00,\n            9.0045e-01, -2.2154e-01],\n          [ 1.2247e+00, -1.5633e-01, -1.0187e+00,  ...,  1.1375e+00,\n            1.6825e+00, -8.8944e-01],\n          [ 9.0919e-01,  2.5527e-01,  4.9148e-03,  ...,  1.2514e+00,\n            1.6568e+00, -1.2530e+00]],\n\n         [[ 1.3625e-01, -5.9697e-02,  1.0035e-01,  ...,  1.6206e-02,\n           -4.2829e-02, -3.2952e-02],\n          [-4.4331e-01,  3.0010e-01, -6.2545e-02,  ..., -6.8408e-01,\n           -2.2847e-01, -2.6119e-01],\n          [ 3.7825e-01,  9.7867e-01, -6.7172e-01,  ...,  5.5770e-02,\n           -2.7496e+00,  2.2595e-02],\n          ...,\n          [-4.7684e-02, -1.3283e-01,  4.5407e-01,  ..., -1.5368e-01,\n            1.8016e+00, -2.2577e+00],\n          [-6.1641e-01,  2.4885e+00,  3.2986e-01,  ..., -2.8765e-01,\n            7.0502e-01, -9.3477e-01],\n          [-1.0332e-01,  3.3179e+00,  1.6641e+00,  ..., -4.3842e-01,\n            1.6062e+00, -1.9473e+00]],\n\n         [[ 6.5138e-02, -2.9824e-02, -6.3254e-02,  ..., -8.6778e-02,\n            1.4943e-01, -7.3095e-02],\n          [ 3.1440e-01,  9.7657e-02, -4.8944e-01,  ..., -6.2506e-01,\n           -2.2523e-01,  7.5599e-01],\n          [ 3.4286e-01,  4.9685e-01, -6.5793e-01,  ..., -9.8072e-01,\n            3.0475e-01,  2.1327e-01],\n          ...,\n          [ 4.4914e-01, -4.7747e-01, -5.2657e-01,  ..., -5.5874e-01,\n           -6.7587e-01,  1.0878e+00],\n          [-7.3795e-01, -4.5210e-01,  5.6438e-01,  ...,  5.0766e-01,\n            1.1249e-02,  1.0396e+00],\n          [ 3.2435e-01, -6.2770e-01,  1.4184e+00,  ..., -1.5092e-01,\n           -8.8488e-01,  2.5156e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-8.4873e-02, -9.0384e-02, -1.3746e-01,  ...,  8.9630e-01,\n            4.7412e-01,  5.5114e-01],\n          [ 2.7835e-01, -5.0923e-02, -1.3183e-01,  ...,  3.6662e-01,\n            9.6946e-01,  1.1396e-01],\n          [ 9.4455e-01, -8.0317e-01, -7.6482e-02,  ..., -1.2483e+00,\n            4.0213e-01,  5.1928e-01],\n          ...,\n          [ 2.7117e+00,  1.3555e+00, -7.1501e-02,  ..., -3.0632e+00,\n           -1.3871e+00,  4.2568e-01],\n          [ 3.7282e+00,  1.5561e+00,  2.5292e-01,  ..., -3.3446e+00,\n           -5.5187e-01,  9.0691e-01],\n          [ 1.6064e+00,  6.6737e-01,  6.5152e-01,  ..., -3.1340e+00,\n            1.0567e+00, -4.3326e-01]],\n\n         [[ 1.4620e-01,  5.3789e-01,  3.2741e-01,  ...,  1.5542e-01,\n           -1.0684e+00, -3.6684e-04],\n          [-1.0694e-01,  1.3596e-01,  5.4573e-01,  ...,  5.8051e-01,\n           -1.1888e-02,  1.7301e-01],\n          [ 3.2033e-01,  7.9250e-01,  4.1425e-02,  ...,  1.0838e+00,\n            2.9698e-01,  2.0976e+00],\n          ...,\n          [-8.7040e-01,  2.1489e-01, -9.3244e-01,  ...,  4.5650e-01,\n            6.7845e-01,  2.7173e+00],\n          [ 1.1574e+00,  2.8919e-01, -5.2404e-01,  ..., -9.1458e-01,\n            1.5072e+00,  1.1043e+00],\n          [ 1.6032e-01,  1.5124e-01, -1.0706e+00,  ..., -2.9099e-01,\n            9.0987e-01,  1.4359e+00]],\n\n         [[-1.5250e+00,  2.0002e-02,  7.0049e-01,  ..., -7.5049e-01,\n            3.7906e-01, -5.2137e-01],\n          [ 3.4132e-02, -6.7340e-01,  1.4980e-01,  ..., -3.5024e-01,\n           -1.1019e-01, -1.1201e+00],\n          [ 1.8436e+00,  3.4792e-01,  5.4133e-02,  ...,  9.2611e-01,\n            4.9148e-01, -1.0801e+00],\n          ...,\n          [ 2.8560e+00,  3.9129e-01, -2.8250e-01,  ...,  1.0889e+00,\n           -7.5057e-01,  4.3308e-01],\n          [ 2.7226e+00, -9.5709e-01,  3.5918e-01,  ...,  8.9640e-01,\n           -1.7273e+00, -1.2086e-01],\n          [ 3.1697e+00,  4.3079e-01, -1.3490e-01,  ...,  4.3863e-01,\n           -1.9439e+00,  2.1139e+00]],\n\n         ...,\n\n         [[ 6.2512e-01, -6.0634e-01,  2.2982e-01,  ..., -6.6939e-01,\n           -1.4417e-02,  9.3060e-01],\n          [-1.0538e-01, -1.3458e+00, -4.2419e-01,  ...,  9.2766e-01,\n           -8.9774e-01,  4.7069e-01],\n          [-6.3236e-01, -3.2019e-01, -1.7082e+00,  ...,  1.9318e+00,\n           -1.2644e-01, -8.5374e-01],\n          ...,\n          [-3.6163e+00, -6.6680e-01, -1.3596e-01,  ...,  3.7377e+00,\n            1.3507e+00, -3.2167e+00],\n          [-2.5316e+00,  4.2014e-01, -1.9152e+00,  ...,  6.7565e-01,\n            2.2897e+00, -1.2980e+00],\n          [-3.0486e+00,  8.9785e-01, -1.8937e-01,  ...,  1.4784e+00,\n            2.0566e+00, -6.4665e-01]],\n\n         [[-1.1025e+00,  3.1147e+00,  2.7114e-01,  ...,  3.5149e-01,\n            2.2051e+00, -7.6251e-01],\n          [-6.0179e-01,  3.0222e+00,  2.4885e-01,  ...,  6.6769e-01,\n            9.6871e-01,  5.3168e-01],\n          [ 1.5900e-01,  2.5175e+00,  5.1810e-02,  ...,  5.2569e-01,\n           -3.7356e-01,  1.5979e+00],\n          ...,\n          [ 1.1682e+00,  6.3998e-01, -3.6601e-01,  ..., -1.2023e+00,\n           -7.2809e-01,  2.0168e+00],\n          [ 3.3163e-01,  9.8095e-01,  1.1956e-02,  ..., -3.6957e-01,\n           -1.6582e+00,  1.3001e+00],\n          [ 1.1252e-01,  7.8318e-01, -1.3791e+00,  ..., -2.3000e+00,\n           -1.7188e+00,  8.0002e-01]],\n\n         [[-2.0692e+00, -5.0200e-01, -9.0104e-01,  ..., -3.2384e-01,\n            3.2213e-02,  2.4289e-01],\n          [-7.3034e-02, -1.7994e-01,  1.9768e-01,  ..., -2.2953e-01,\n            6.4075e-01,  7.0413e-01],\n          [ 8.1838e-01, -1.1832e+00, -5.9078e-01,  ...,  6.0469e-01,\n            5.1034e-01,  4.5660e-01],\n          ...,\n          [ 2.0595e+00,  1.3245e-01,  1.3776e+00,  ...,  8.2466e-01,\n           -1.1954e+00,  1.3478e+00],\n          [ 3.3445e+00, -2.9066e-01,  1.5302e+00,  ...,  2.0510e-02,\n           -1.0299e+00,  2.3878e+00],\n          [ 3.7544e+00,  4.0665e-01,  6.3298e-01,  ...,  9.3438e-01,\n           -2.7292e-01,  1.6673e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[-1.0225e-01, -3.3589e-02,  3.7785e-02,  ...,  2.8276e-02,\n           -1.7043e-02, -1.6652e-01],\n          [-5.9800e-02,  4.5843e-02, -1.1369e+00,  ...,  4.7300e-01,\n            1.0152e+00,  1.9982e-01],\n          [ 4.6396e-02,  7.2068e-01, -1.7278e-01,  ..., -6.4567e-01,\n            2.4734e-01, -1.2815e-01],\n          ...,\n          [ 5.7812e-01, -2.0924e+00, -1.1098e+00,  ..., -1.3717e+00,\n           -6.7618e-01, -2.6552e-01],\n          [ 6.8826e-01, -5.4628e-01, -1.0737e+00,  ..., -1.3102e+00,\n           -1.1361e+00,  3.7829e-01],\n          [-8.1809e-01, -7.1762e-01, -1.4413e+00,  ..., -1.3244e+00,\n           -1.8579e+00, -8.8789e-01]],\n\n         [[ 4.7239e-02, -3.1296e-02, -3.8992e-02,  ...,  6.8573e-04,\n           -2.5185e-02,  5.6116e-02],\n          [ 9.2757e-02,  1.4187e+00, -5.3661e-01,  ..., -2.0773e-01,\n            9.9152e-01, -1.9251e-01],\n          [ 1.2265e+00, -4.2808e-01, -1.4994e+00,  ..., -5.7235e-01,\n            5.5024e-01, -1.0428e+00],\n          ...,\n          [-1.0495e+00,  1.4613e-01, -4.8381e-01,  ...,  1.9275e-01,\n            2.5773e+00,  1.5236e+00],\n          [ 1.6780e+00,  2.6381e+00,  9.1913e-01,  ...,  1.7850e+00,\n           -9.7313e-01,  4.3710e-01],\n          [ 5.7566e-01,  1.7228e-01,  4.1544e-01,  ...,  2.7862e+00,\n           -5.4163e-01, -5.3172e-01]],\n\n         [[-1.7336e-01, -1.2034e-02,  5.2982e-02,  ..., -6.7017e-02,\n            5.7353e-02,  1.5812e-02],\n          [ 1.9620e-01,  3.5744e-01,  1.7405e-01,  ...,  6.9350e-01,\n            9.5843e-02, -6.2069e-01],\n          [ 7.7754e-01, -7.9205e-01, -8.1418e-01,  ..., -1.1200e-01,\n           -1.3831e+00, -5.4963e-01],\n          ...,\n          [ 1.1328e+00,  1.9874e+00,  1.3393e-01,  ..., -8.6577e-02,\n            7.5616e-01, -1.7068e+00],\n          [ 1.6384e+00,  2.6157e+00,  1.1421e+00,  ...,  8.2403e-01,\n           -4.0340e-01, -2.2331e+00],\n          [ 2.6180e+00,  2.3276e+00,  2.2828e-01,  ...,  1.5298e+00,\n           -7.1356e-01, -2.1185e+00]],\n\n         ...,\n\n         [[-3.2882e-02,  2.3061e-03, -7.2032e-02,  ..., -3.6296e-02,\n           -1.0957e-02,  2.1145e-03],\n          [-4.1265e-01,  5.7199e-01,  5.8260e-02,  ...,  1.1723e+00,\n            3.6933e-01,  2.0152e+00],\n          [ 2.8911e+00,  1.8710e+00,  1.1077e+00,  ...,  6.4267e-01,\n            6.3881e-01, -4.7690e-01],\n          ...,\n          [ 9.0218e-01, -3.2867e+00, -1.1364e+00,  ..., -1.4921e-02,\n            8.7744e-01,  1.7557e+00],\n          [-5.4165e-01, -1.9330e+00, -1.5858e-01,  ...,  2.3179e-02,\n            1.1603e+00, -1.0531e+00],\n          [-6.3887e-01,  7.0724e-01,  4.1526e-01,  ...,  5.2921e-01,\n            3.1488e-01,  6.9241e-02]],\n\n         [[ 3.3629e-02,  4.0495e-02,  6.0463e-02,  ...,  3.1216e-02,\n           -5.4167e-02,  2.5725e-03],\n          [-1.2097e-02, -1.4983e-01,  1.8031e-01,  ...,  2.6715e-01,\n           -2.8884e-01, -5.0892e-01],\n          [ 4.6681e-02, -4.2925e-01,  3.2191e-01,  ...,  3.7348e-01,\n           -1.1473e+00,  2.1272e-01],\n          ...,\n          [ 2.3001e-01,  7.8834e-01, -4.3842e-01,  ...,  8.8292e-01,\n           -1.0453e+00,  5.2769e-01],\n          [ 1.1255e+00, -9.9096e-02, -3.7770e-01,  ..., -1.2857e+00,\n           -5.8382e-01,  6.4337e-01],\n          [ 7.3882e-02,  8.7844e-01,  4.4527e-01,  ...,  1.9682e-01,\n           -1.3681e+00,  3.5115e-01]],\n\n         [[ 9.1800e-02,  6.3657e-02,  2.8119e-02,  ...,  3.3984e-02,\n           -3.4366e-02,  4.5893e-03],\n          [ 8.3634e-01,  2.7675e-01, -1.2758e+00,  ...,  1.4161e+00,\n           -1.1398e+00, -2.8862e-01],\n          [-2.4373e-01,  1.1650e+00, -8.0559e-01,  ...,  2.3599e+00,\n           -1.6471e+00, -1.9871e+00],\n          ...,\n          [ 1.2168e+00, -7.6255e-01,  6.4503e-01,  ..., -4.1957e-01,\n            3.9009e-01,  1.2701e+00],\n          [ 2.7012e+00, -8.8828e-01, -2.6886e-01,  ..., -1.7065e+00,\n           -7.7961e-01, -2.9333e-01],\n          [ 1.3746e+00,  6.4494e-01,  3.3399e-01,  ..., -7.7459e-01,\n           -6.8212e-01, -5.4632e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-1.8538,  0.3373,  0.0269,  ...,  0.3742,  0.9387, -0.6357],\n          [-1.4332,  1.3304,  0.5377,  ...,  1.4094,  0.0339, -0.0599],\n          [-1.3879,  1.3471,  0.0830,  ...,  1.3568,  0.3998,  0.8613],\n          ...,\n          [-0.3973,  1.5675,  0.1615,  ..., -0.0943, -0.0571,  1.0504],\n          [-0.5974,  2.4424, -0.3121,  ..., -1.4400,  0.4787,  0.9639],\n          [ 1.3419,  1.8594, -1.4006,  ..., -1.3161, -0.6484, -0.2690]],\n\n         [[-0.2834,  0.9118,  2.7658,  ...,  0.1927,  0.3749, -0.5360],\n          [ 1.0475, -0.2919,  1.5158,  ..., -0.1755,  0.6454, -0.1966],\n          [ 1.7233, -0.8028,  0.5566,  ...,  0.4358,  0.3380,  0.4145],\n          ...,\n          [ 3.0686, -2.1198, -1.7750,  ...,  0.2197, -0.5501, -1.8212],\n          [ 2.0638, -1.4988, -1.9287,  ...,  0.5802,  0.6825, -0.8971],\n          [ 2.1807, -1.1990, -2.0481,  ...,  0.2298, -0.9835, -1.0646]],\n\n         [[-0.8111,  0.8346,  0.4478,  ..., -0.8623, -0.1096,  0.1767],\n          [-0.4158,  1.0715,  0.7467,  ..., -0.2087, -0.4280, -0.6946],\n          [-1.0867,  0.4501, -0.8183,  ..., -0.3752, -0.3266, -0.2980],\n          ...,\n          [-0.2154,  1.1874,  1.6057,  ...,  1.1321, -1.0810, -0.2639],\n          [ 0.0979,  0.0425,  1.9088,  ...,  1.5448, -0.7958,  0.3350],\n          [ 0.0544,  1.2777,  1.5472,  ...,  1.8829, -0.1429, -0.7955]],\n\n         ...,\n\n         [[ 0.4980,  0.5312, -0.5355,  ..., -0.3806,  0.6379,  1.2736],\n          [ 0.1239,  0.6253, -1.7790,  ..., -0.6147,  0.0640,  1.0943],\n          [-0.1819,  1.4372, -1.5819,  ...,  0.2594, -1.0311,  1.6451],\n          ...,\n          [-3.8195, -0.8606, -0.0384,  ..., -2.6253, -0.4176, -0.2442],\n          [-2.3930,  0.2834,  2.0269,  ..., -0.9737, -0.8169, -0.5952],\n          [-2.4334, -0.3947,  1.9298,  ..., -0.9720,  0.2417, -1.4448]],\n\n         [[ 0.1851,  0.3272,  1.4310,  ...,  1.0600, -0.3876,  0.6240],\n          [ 0.8346, -0.2720, -0.2128,  ...,  0.3104, -0.4811, -0.3280],\n          [ 0.8384, -1.0414,  1.1290,  ...,  1.3043, -1.5213, -0.1333],\n          ...,\n          [ 1.0972, -0.0436, -0.3343,  ...,  0.4360, -1.0838,  1.9904],\n          [ 0.6339, -0.4921, -0.7328,  ..., -0.5477, -1.8045,  2.7601],\n          [ 1.4702, -0.2906, -0.9036,  ..., -0.9267, -0.5142,  2.7514]],\n\n         [[-0.6630, -0.3470,  0.2558,  ...,  0.0503, -0.0062,  0.5274],\n          [-0.2690,  0.0867,  0.2868,  ..., -0.9159,  0.0156,  0.2699],\n          [ 0.7877, -0.2737,  0.3661,  ..., -1.3732, -0.4390, -1.1991],\n          ...,\n          [ 0.7432,  0.5370,  0.3801,  ..., -0.9487, -0.8608, -0.3010],\n          [ 2.2261,  0.5574, -0.8648,  ..., -0.6901,  0.4610, -1.8202],\n          [ 1.2014,  1.8430,  0.1258,  ..., -3.2701,  0.4271, -1.2860]]]],\n       grad_fn=<PermuteBackward0>), tensor([[[[-1.0931e-01,  8.5687e-02, -3.4830e-02,  ..., -1.9313e-01,\n            1.5816e-01, -2.8891e-02],\n          [-2.2714e-01, -4.7395e-01,  1.1304e+00,  ..., -5.4872e-01,\n           -1.0681e+00,  5.5090e-01],\n          [-1.1305e-01, -2.1486e-01,  1.9872e+00,  ...,  5.1186e-01,\n           -2.3078e+00,  1.4606e+00],\n          ...,\n          [-6.1662e-01, -6.6142e-01, -4.6649e-01,  ...,  2.3612e+00,\n           -2.1935e+00,  2.3250e+00],\n          [ 1.0346e+00, -7.8334e-01,  7.1940e-01,  ...,  3.5898e+00,\n           -8.8020e-01,  1.5594e+00],\n          [ 1.1645e+00,  4.0993e-01,  2.1964e+00,  ...,  1.7562e+00,\n           -2.4824e+00,  1.2539e+00]],\n\n         [[-1.6666e-01, -4.8379e-02,  1.4091e-01,  ...,  6.2192e-03,\n            2.1029e-01, -4.5950e-03],\n          [-1.4751e+00, -3.1443e-01,  9.2271e-01,  ...,  6.9528e-01,\n            1.0717e+00,  1.0570e+00],\n          [ 1.0109e+00, -1.5953e-01,  1.0335e+00,  ...,  1.4340e+00,\n            1.2995e+00, -2.5891e-01],\n          ...,\n          [ 3.2529e-01, -3.4115e-01, -1.0534e+00,  ..., -2.6833e+00,\n            3.5837e-01, -2.1026e+00],\n          [ 3.2298e-01,  1.4525e+00, -1.6604e+00,  ...,  1.2999e-03,\n           -6.9957e-01, -3.3631e+00],\n          [ 6.8949e-01, -1.7640e+00, -1.4007e+00,  ..., -2.5281e+00,\n            1.3917e+00, -8.4510e-01]],\n\n         [[-1.9005e-01, -1.9519e-01,  1.0384e-01,  ..., -6.8974e-02,\n           -7.1227e-02, -1.3442e-02],\n          [-5.4575e-01,  1.3717e+00,  2.9830e-01,  ..., -3.5064e-01,\n            1.0367e+00,  3.5869e-01],\n          [-2.3250e+00,  2.5908e+00, -4.6272e-01,  ..., -4.1331e-01,\n            3.2155e+00,  5.3559e-01],\n          ...,\n          [-3.3065e+00, -2.1037e+00, -3.2031e+00,  ...,  1.9337e+00,\n           -1.5915e+00,  3.2537e+00],\n          [-1.6532e+00,  8.7735e-01, -2.4955e-01,  ...,  1.8871e+00,\n           -2.6521e+00,  1.3123e+00],\n          [-1.2730e+00, -2.1654e-01, -2.1380e+00,  ...,  2.3023e+00,\n            5.7620e-01,  1.9117e+00]],\n\n         ...,\n\n         [[ 1.4009e-01,  1.2626e-01, -3.8329e-02,  ..., -1.3652e-02,\n           -1.9473e-02, -1.0895e-02],\n          [-2.5119e-01, -5.9523e-01, -6.8309e-01,  ..., -8.3538e-01,\n            3.1866e-01,  7.9912e-01],\n          [ 3.6005e-01, -5.9602e-01, -9.0759e-01,  ..., -8.8429e-01,\n           -3.0334e-01, -2.1959e-02],\n          ...,\n          [ 4.8829e-01,  2.0357e+00, -1.6404e+00,  ..., -1.4997e-01,\n           -6.3198e-02,  1.7738e+00],\n          [-7.8132e-01,  1.6426e+00,  1.1959e+00,  ...,  1.0483e-01,\n            4.2282e-01, -1.3982e+00],\n          [ 8.5656e-01, -4.5035e-01, -5.2497e-01,  ..., -1.0880e-01,\n           -7.7904e-01, -6.3176e-01]],\n\n         [[-9.1092e-04,  5.9787e-02, -1.1717e-01,  ..., -4.3412e-02,\n           -2.2547e-02,  3.5406e-02],\n          [ 8.3835e-01, -5.6795e-01, -1.0318e+00,  ..., -1.4119e-01,\n           -1.0089e+00, -4.2568e-01],\n          [ 1.3323e+00, -7.9603e-01,  1.4058e+00,  ...,  8.7779e-01,\n           -3.7914e-01, -1.8072e+00],\n          ...,\n          [ 1.6048e+00, -1.4587e+00, -2.3229e+00,  ...,  3.0744e-01,\n            1.1724e+00,  1.1777e+00],\n          [ 1.7612e+00, -1.8369e+00, -8.2630e-01,  ..., -3.8059e-01,\n           -9.4643e-01, -3.4247e-01],\n          [ 5.9939e-01, -1.5645e+00, -7.6303e-01,  ..., -1.9231e-01,\n           -1.8153e+00, -1.2791e+00]],\n\n         [[ 3.3956e-02,  6.1036e-02,  9.0944e-02,  ..., -6.9381e-02,\n           -2.2397e-02,  1.5191e-01],\n          [ 1.0886e-01,  9.4969e-01,  1.7650e+00,  ..., -5.4084e-02,\n            6.4813e-01,  3.7038e-01],\n          [-1.5424e+00,  8.2110e-01,  1.2281e+00,  ..., -5.8746e-01,\n            5.9679e-01,  1.4738e+00],\n          ...,\n          [ 4.6856e-01, -7.0286e-01, -1.2946e+00,  ..., -1.4577e-01,\n            2.6149e-01,  8.0905e-01],\n          [ 6.3292e-01, -1.9938e+00, -1.4344e+00,  ...,  1.0607e+00,\n            2.8183e-01,  1.2616e+00],\n          [ 8.7972e-01, -3.9359e-01, -2.9544e+00,  ..., -3.4412e-02,\n            1.1010e-01,  1.0850e+00]]]], grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None, cross_attentions=None)\n:Dictionary inputs to traced functions must have consistent type. Found Tensor and Tuple[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor]]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Step 1: Load the trained GPT model\n",
    "model = GPT2Model.from_pretrained(\"recipes_generation_model\", torchscript=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"recipes_generation_model\")\n",
    "\n",
    "def forward(inputs):\n",
    "    return model(**inputs)[0]\n",
    "# Step 2: Convert the model to TorchScript\n",
    "# Example using trace\n",
    "dummy_input = tokenizer.encode(\"<|startoftext|>parmesan cheese, aubergines, courgettes\", return_tensors=\"pt\")\n",
    "traced_model = torch.jit.trace(model, )#forward, {\"input_ids\": dummy_input})\n",
    "\n",
    "# Alternatively, you can use script if your model contains control flow or dynamic shapes:\n",
    "# traced_model = torch.jit.script(model)\n",
    "\n",
    "# Step 3: Optimize the TorchScript model for mobile deployment\n",
    "# This step is optional but can improve inference speed and reduce model size\n",
    "traced_model = traced_model.optimize_for_mobile()\n",
    "\n",
    "# Step 4: Save the optimized TorchScript model to a file\n",
    "traced_model.save(\"recipes_generator.pt\")\n",
    "\n",
    "# Optionally, save the tokenizer vocabulary for later use\n",
    "tokenizer.save_pretrained(\"tokenizer_directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c7e31b-11c9-4357-a3df-652dc735c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained GPT model and tokenizer\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Create a custom forward function that returns only the model output\n",
    "def forward(inputs):\n",
    "    return model(**inputs)[0]\n",
    "\n",
    "# Tracing the model with a dummy input\n",
    "dummy_input = tokenizer.encode(\"Sample input\", return_tensors=\"pt\")\n",
    "traced_model = torch.jit.trace(forward, {\"input_ids\": dummy_input})\n",
    "\n",
    "# Optimize the traced model for mobile deployment\n",
    "traced_model = traced_model.optimize_for_mobile()\n",
    "\n",
    "# Save the optimized TorchScript model to a file\n",
    "traced_model.save(\"optimized_gpt_model.pt\")\n",
    "\n",
    "# Optionally, save the tokenizer vocabulary for later use\n",
    "tokenizer.save_pretrained(\"tokenizer_directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc2e26-21c1-445e-af0b-bc3826fca11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6970f19-063a-406b-aad7-9d650e0dcdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-04-25 17:47:41.646141: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-25 17:47:41.650527: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-25 17:47:41.654059: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-25 17:47:41.659207: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-25 17:47:41.662656: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-25 17:47:41.665952: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-25 17:47:41.773012: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-25 17:47:41.774251: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-25 17:47:41.775395: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-25 17:47:41.776522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8857 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Dummy Examples \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"tokenizer\", bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "input_encoded = tokenizer(\"<|startoftext|>Prompt:eggs,parmesan cheese\", return_tensors=\"tf\")\n",
    "\n",
    "output = []\n",
    "for i in range(0):#len(input_encoded['input_ids'])):\n",
    "    input_vector = tf.constant([[input_encoded['input_ids'][0][i].numpy()]])\n",
    "    # input type ids\n",
    "    input_ids = input_vector#input_encoded['input_ids'][i]\n",
    "    interpreter.set_tensor(\n",
    "        input_details[1]['index'],\n",
    "        input_ids,\n",
    "    )\n",
    "    \n",
    "    # input_mask\n",
    "    input_mask = input_vector#input_encoded['attention_mask']\n",
    "    interpreter.set_tensor(\n",
    "        input_details[0]['index'],\n",
    "        input_mask,\n",
    "    )\n",
    "    \n",
    "    # input ids\n",
    "    \n",
    "    \n",
    "    # Invoke inputs\n",
    "    interpreter.invoke()\n",
    "    # Take last output\n",
    "    tflite_output = interpreter.get_tensor(output_details[-1]['index'])\n",
    "    \n",
    "    # Keras Model outputs .\n",
    "    #model_inputs = {'input_ids': input_ids, 'input_mask': input_mask, 'input_type_ids': input_type_ids}\n",
    "    #model_outputs = model(model_inputs)\n",
    "    \n",
    "    # We need a slightly higher rtol here to assert :-)\n",
    "    #tf.debugging.assert_near(tflite_output, model_outputs['token_embeddings'], rtol=3.0)\n",
    "    #print(\"Outputs asserted and succesful:  ✅\")\n",
    "    #print(tflite_output)\n",
    "    output.append(tflite_output)\n",
    "\n",
    "#print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1eb9f03f-f8a5-4bbe-86e3-90f35a6fafa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[50257 24129   457    25 33856    82    11    79  1670 42890  9891]], shape=(1, 11), dtype=int32)\n",
      "{'name': 'serving_default_input_ids:0', 'index': 1, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n"
     ]
    }
   ],
   "source": [
    "print(input_encoded['input_ids'])\n",
    "print(input_details[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "493c80f0-c83c-482e-a627-e452b4648460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 17:37:00.057454: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-30 17:37:00.082110: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-30 17:37:00.570297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-04-30 17:37:01.522129: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-30 17:37:01.528506: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-30 17:37:01.532040: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-30 17:37:01.536851: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-30 17:37:01.540355: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-30 17:37:01.543614: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-30 17:37:01.655098: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-30 17:37:01.656448: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-30 17:37:01.657582: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-30 17:37:01.658707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10168 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel\n",
    "model_loaded = TFGPT2LMHeadModel.from_pretrained(\"recipes_generation_model\", ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53ab12aa-a0d7-4138-a3aa-a9efd42def56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_tflite_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_tflite_model/assets\n"
     ]
    }
   ],
   "source": [
    "model_loaded.save(\"saved_tflite_model\", save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bc87bce-a9c6-438b-86cb-b7a2a3c667c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1714491520.237830   16127 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1714491520.237858   16127 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-04-30 17:38:40.238168: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: saved_tflite_model\n",
      "2024-04-30 17:38:40.244622: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-04-30 17:38:40.244634: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: saved_tflite_model\n",
      "2024-04-30 17:38:40.298010: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-04-30 17:38:40.303233: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-04-30 17:38:40.455321: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: saved_tflite_model\n",
      "2024-04-30 17:38:40.501811: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 263646 microseconds.\n",
      "2024-04-30 17:38:40.568755: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFlite conversion succesful\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"saved_tflite_model\") # path to the SavedModel directory\n",
    "converter.experimental_new_converter = True\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "open(\"new_saved_model.tflite\".format(\"distilbert/distilgpt2\"), \"wb\").write(tflite_model)\n",
    "print(\"TFlite conversion succesful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4c736466-6e56-4233-b934-31dae3dcd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_loaded = GPT2LMHeadModel.from_pretrained(\"model\", config=configuration, ignore_mismatched_sizes=True)\n",
    "def infer(prompt, model, tokenizer, max_length):\n",
    "    input = f\"<|startoftext|>Prompt: {prompt}\"\n",
    "    input = tokenizer(input, return_tensors=\"pt\")\n",
    "    input_ids      = input[\"input_ids\"]\n",
    "    attention_mask = input[\"attention_mask\"]\n",
    "\n",
    "    output = model.generate(input_ids.to(device),\n",
    "                            attention_mask=attention_mask.to(device),\n",
    "                            max_new_tokens=max_length,\n",
    "                            num_beams=5, \n",
    "                            no_repeat_ngram_size=2, \n",
    "                            max_length = 600,\n",
    "                            num_return_sequences=1,\n",
    "                            eos_token_id=tokenizer.eos_token_id,\n",
    "                            do_sample = True, top_k = 100, top_p = 0.85)\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef0115b4-f297-4438-acb3-8090f2e391f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: onnxruntime_tools is deprecated. Use onnxruntime or onnxruntime-gpu instead. For more information, see https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/README.md.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 22:20:28.778408: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-03 22:20:28.803187: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-03 22:20:29.291743: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:335: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  past_key, past_value = layer_past\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input_ids : Long(*, *, strides=[1, 1], requires_grad=0, device=cpu),\n",
      "      %position_ids : Long(*, *, strides=[2, 1], requires_grad=0, device=cpu),\n",
      "      %attention_mask : Float(*, *, strides=[2, 1], requires_grad=0, device=cpu),\n",
      "      %past_0 : Float(2, *, 12, *, 64, strides=[768, 768, 64, 64, 1], requires_grad=0, device=cpu),\n",
      "      %past_1 : Float(2, *, 12, *, 64, strides=[768, 768, 64, 64, 1], requires_grad=0, device=cpu),\n",
      "      %past_2 : Float(2, *, 12, *, 64, strides=[768, 768, 64, 64, 1], requires_grad=0, device=cpu),\n",
      "      %past_3 : Float(2, *, 12, *, 64, strides=[768, 768, 64, 64, 1], requires_grad=0, device=cpu),\n",
      "      %past_4 : Float(2, *, 12, *, 64, strides=[768, 768, 64, 64, 1], requires_grad=0, device=cpu),\n",
      "      %past_5 : Float(2, *, 12, *, 64, strides=[768, 768, 64, 64, 1], requires_grad=0, device=cpu),\n",
      "      %transformer.wte.weight : Float(50259, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.wpe.weight : Float(1024, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.0.ln_1.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.0.ln_1.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.0.attn.c_attn.weight : Float(768, 2304, strides=[2304, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.0.attn.c_attn.bias : Float(2304, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.0.attn.c_proj.weight : Float(768, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.0.attn.c_proj.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.0.ln_2.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.0.ln_2.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.0.mlp.c_fc.weight : Float(768, 3072, strides=[3072, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.0.mlp.c_fc.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.0.mlp.c_proj.weight : Float(3072, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.0.mlp.c_proj.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.1.ln_1.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.1.ln_1.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.1.attn.c_attn.weight : Float(768, 2304, strides=[2304, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.1.attn.c_attn.bias : Float(2304, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.1.attn.c_proj.weight : Float(768, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.1.attn.c_proj.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.1.ln_2.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.1.ln_2.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.1.mlp.c_fc.weight : Float(768, 3072, strides=[3072, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.1.mlp.c_fc.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.1.mlp.c_proj.weight : Float(3072, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.1.mlp.c_proj.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.2.ln_1.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.2.ln_1.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.2.attn.c_attn.weight : Float(768, 2304, strides=[2304, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.2.attn.c_attn.bias : Float(2304, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.2.attn.c_proj.weight : Float(768, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.2.attn.c_proj.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.2.ln_2.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.2.ln_2.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.2.mlp.c_fc.weight : Float(768, 3072, strides=[3072, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.2.mlp.c_fc.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.2.mlp.c_proj.weight : Float(3072, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.2.mlp.c_proj.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.3.ln_1.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.3.ln_1.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.3.attn.c_attn.weight : Float(768, 2304, strides=[2304, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.3.attn.c_attn.bias : Float(2304, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.3.attn.c_proj.weight : Float(768, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.3.attn.c_proj.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.3.ln_2.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.3.ln_2.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.3.mlp.c_fc.weight : Float(768, 3072, strides=[3072, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.3.mlp.c_fc.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.3.mlp.c_proj.weight : Float(3072, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.3.mlp.c_proj.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.4.ln_1.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.4.ln_1.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.4.attn.c_attn.weight : Float(768, 2304, strides=[2304, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.4.attn.c_attn.bias : Float(2304, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.4.attn.c_proj.weight : Float(768, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.4.attn.c_proj.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.4.ln_2.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.4.ln_2.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.4.mlp.c_fc.weight : Float(768, 3072, strides=[3072, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.4.mlp.c_fc.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.4.mlp.c_proj.weight : Float(3072, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.4.mlp.c_proj.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.5.ln_1.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.5.ln_1.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.5.attn.c_attn.weight : Float(768, 2304, strides=[2304, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.5.attn.c_attn.bias : Float(2304, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.5.attn.c_proj.weight : Float(768, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.5.attn.c_proj.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.5.ln_2.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.5.ln_2.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.5.mlp.c_fc.weight : Float(768, 3072, strides=[3072, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.5.mlp.c_fc.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.5.mlp.c_proj.weight : Float(3072, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %transformer.h.5.mlp.c_proj.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.ln_f.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.ln_f.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %onnx::MatMul_1665 : Float(768, 50259, strides=[1, 768], requires_grad=0, device=cpu)):\n",
      "  %/transformer/Shape_output_0 : Long(2, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/transformer/Shape\"](%input_ids), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1005:0\n",
      "  %/transformer/Constant_output_0 : Long(device=cpu) = onnx::Constant[value={1}, onnx_name=\"/transformer/Constant\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1005:0\n",
      "  %/transformer/Gather_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/transformer/Gather\"](%/transformer/Shape_output_0, %/transformer/Constant_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1005:0\n",
      "  %/transformer/Constant_1_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name=\"/transformer/Constant_1\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer\n",
      "  %/transformer/Unsqueeze_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"/transformer/Unsqueeze\"](%/transformer/Gather_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer\n",
      "  %/transformer/Concat_output_0 : Long(2, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"/transformer/Concat\"](%/transformer/Constant_1_output_0, %/transformer/Unsqueeze_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1006:0\n",
      "  %/transformer/Reshape_output_0 : Long(*, *, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Reshape[onnx_name=\"/transformer/Reshape\"](%input_ids, %/transformer/Concat_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1006:0\n",
      "  %/transformer/Shape_1_output_0 : Long(2, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/transformer/Shape_1\"](%/transformer/Reshape_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1007:0\n",
      "  %/transformer/Constant_2_output_0 : Long(device=cpu) = onnx::Constant[value={0}, onnx_name=\"/transformer/Constant_2\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1007:0\n",
      "  %/transformer/Gather_1_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/transformer/Gather_1\"](%/transformer/Shape_1_output_0, %/transformer/Constant_2_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1007:0\n",
      "  %/transformer/Unsqueeze_1_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"/transformer/Unsqueeze_1\"](%/transformer/Gather_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer\n",
      "  %/transformer/Constant_3_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name=\"/transformer/Constant_3\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer\n",
      "  %/transformer/Concat_1_output_0 : Long(2, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"/transformer/Concat_1\"](%/transformer/Unsqueeze_1_output_0, %/transformer/Constant_3_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1030:0\n",
      "  %/transformer/Reshape_1_output_0 : Float(*, *, strides=[2, 1], requires_grad=0, device=cpu) = onnx::Reshape[onnx_name=\"/transformer/Reshape_1\"](%attention_mask, %/transformer/Concat_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1030:0\n",
      "  %/transformer/Unsqueeze_2_output_0 : Float(*, 1, *, strides=[2, 2, 1], requires_grad=0, device=cpu) = onnx::Unsqueeze[axes=[1], onnx_name=\"/transformer/Unsqueeze_2\"](%/transformer/Reshape_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1039:0\n",
      "  %/transformer/Unsqueeze_3_output_0 : Float(*, 1, 1, *, strides=[2, 2, 2, 1], requires_grad=0, device=cpu) = onnx::Unsqueeze[axes=[2], onnx_name=\"/transformer/Unsqueeze_3\"](%/transformer/Unsqueeze_2_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1039:0\n",
      "  %/transformer/Cast_output_0 : Float(*, 1, 1, *, strides=[2, 2, 2, 1], requires_grad=0, device=cpu) = onnx::Cast[to=1, onnx_name=\"/transformer/Cast\"](%/transformer/Unsqueeze_3_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1046:0\n",
      "  %/transformer/Constant_4_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={1}, onnx_name=\"/transformer/Constant_4\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/torch/_tensor.py:966:0\n",
      "  %/transformer/Sub_output_0 : Float(*, 1, 1, *, strides=[2, 2, 2, 1], requires_grad=0, device=cpu) = onnx::Sub[onnx_name=\"/transformer/Sub\"](%/transformer/Constant_4_output_0, %/transformer/Cast_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/torch/_tensor.py:966:0\n",
      "  %/transformer/Constant_5_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={-3.40282e+38}, onnx_name=\"/transformer/Constant_5\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1047:0\n",
      "  %/transformer/Mul_output_0 : Float(*, 1, 1, *, strides=[2, 2, 2, 1], requires_grad=0, device=cpu) = onnx::Mul[onnx_name=\"/transformer/Mul\"](%/transformer/Sub_output_0, %/transformer/Constant_5_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1047:0\n",
      "  %/transformer/wte/Gather_output_0 : Float(*, *, 768, strides=[768, 768, 1], requires_grad=1, device=cpu) = onnx::Gather[onnx_name=\"/transformer/wte/Gather\"](%transformer.wte.weight, %/transformer/Reshape_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/torch.nn.modules.sparse.Embedding::wte # /home/dagus/venv/lib/python3.11/site-packages/torch/nn/functional.py:2264:0\n",
      "  %/transformer/wpe/Gather_output_0 : Float(*, *, 768, strides=[768, 768, 1], requires_grad=1, device=cpu) = onnx::Gather[onnx_name=\"/transformer/wpe/Gather\"](%transformer.wpe.weight, %position_ids), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/torch.nn.modules.sparse.Embedding::wpe # /home/dagus/venv/lib/python3.11/site-packages/torch/nn/functional.py:2264:0\n",
      "  %/transformer/Add_output_0 : Float(*, *, 768, strides=[768, 768, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/transformer/Add\"](%/transformer/wte/Gather_output_0, %/transformer/wpe/Gather_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1070:0\n",
      "  %/transformer/Shape_2_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/transformer/Shape_2\"](%/transformer/Add_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1078:0\n",
      "  %/transformer/Constant_6_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name=\"/transformer/Constant_6\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1078:0\n",
      "  %/transformer/Constant_7_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-1}, onnx_name=\"/transformer/Constant_7\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1078:0\n",
      "  %/transformer/Constant_8_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name=\"/transformer/Constant_8\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1078:0\n",
      "  %/transformer/Slice_output_0 : Long(1, strides=[1], device=cpu) = onnx::Slice[onnx_name=\"/transformer/Slice\"](%/transformer/Shape_2_output_0, %/transformer/Constant_7_output_0, %/transformer/Constant_8_output_0, %/transformer/Constant_6_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1078:0\n",
      "  %/transformer/Squeeze_output_0 : Long(device=cpu) = onnx::Squeeze[axes=[0], onnx_name=\"/transformer/Squeeze\"](%/transformer/Slice_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1078:0\n",
      "  %/transformer/h.0/ln_1/ReduceMean_output_0 : Float(*, *, 1, device=cpu) = onnx::ReduceMean[axes=[-1], onnx_name=\"/transformer/h.0/ln_1/ReduceMean\"](%/transformer/Add_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/torch.nn.modules.normalization.LayerNorm::ln_1 # /home/dagus/venv/lib/python3.11/site-packages/torch/nn/functional.py:2573:0\n",
      "  %/transformer/h.0/ln_1/Sub_output_0 : Float(*, *, 768, device=cpu) = onnx::Sub[onnx_name=\"/transformer/h.0/ln_1/Sub\"](%/transformer/Add_output_0, %/transformer/h.0/ln_1/ReduceMean_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/torch.nn.modules.normalization.LayerNorm::ln_1 # /home/dagus/venv/lib/python3.11/site-packages/torch/nn/functional.py:2573:0\n",
      "  %/transformer/h.0/ln_1/Constant_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={2}, onnx_name=\"/transformer/h.0/ln_1/Constant\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/torch.nn.modules.normalization.LayerNorm::ln_1 # /home/dagus/venv/lib/python3.11/site-packages/torch/nn/functional.py:2573:0\n",
      "  %/transformer/h.0/ln_1/Pow_output_0 : Float(*, *, 768, device=cpu) = onnx::Pow[onnx_name=\"/transformer/h.0/ln_1/Pow\"](%/transformer/h.0/ln_1/Sub_output_0, %/transformer/h.0/ln_1/Constant_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/torch.nn.modules.normalization.LayerNorm::ln_1 # /home/dagus/venv/lib/python3.11/site-packages/torch/nn/functional.py:2573:0\n",
      "  %/transformer/h.0/ln_1/ReduceMean_1_output_0 : Float(*, *, 1, device=cpu) = onnx::ReduceMean[axes=[-1], onnx_name=\"/transformer/h.0/ln_1/ReduceMean_1\"](%/transformer/h.0/ln_1/Pow_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/torch.nn.modules.normalization.LayerNorm::ln_1 # /home/dagus/venv/lib/python3.11/site-packages/torch/nn/functional.py:2573:0\n",
      "  %/transformer/h.0/ln_1/Constant_1_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={1e-05}, onnx_name=\"/transformer/h.0/ln_1/Constant_1\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/torch.nn.modules.normalization.LayerNorm::ln_1 # /home/dagus/venv/lib/python3.11/site-packages/torch/nn/functional.py:2573:0\n",
      "  %/transformer/h.0/ln_1/Add_output_0 : Float(*, *, 1, device=cpu) = onnx::Add[onnx_name=\"/transformer/h.0/ln_1/Add\"](%/transformer/h.0/ln_1/ReduceMean_1_output_0, %/transformer/h.0/ln_1/Constant_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/torch.nn.modules.normalization.LayerNorm::ln_1 # /home/dagus/venv/lib/python3.11/site-packages/torch/nn/functional.py:2573:0\n",
      "  %/transformer/h.0/ln_1/Sqrt_output_0 : Float(*, *, 1, device=cpu) = onnx::Sqrt[onnx_name=\"/transformer/h.0/ln_1/Sqrt\"](%/transformer/h.0/ln_1/Add_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/torch.nn.modules.normalization.LayerNorm::ln_1 # /home/dagus/venv/lib/python3.11/site-packages/torch/nn/functional.py:2573:0\n",
      "  %/transformer/h.0/ln_1/Div_output_0 : Float(*, *, 768, device=cpu) = onnx::Div[onnx_name=\"/transformer/h.0/ln_1/Div\"](%/transformer/h.0/ln_1/Sub_output_0, %/transformer/h.0/ln_1/Sqrt_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/torch.nn.modules.normalization.LayerNorm::ln_1 # /home/dagus/venv/lib/python3.11/site-packages/torch/nn/functional.py:2573:0\n",
      "  %/transformer/h.0/ln_1/Mul_output_0 : Float(*, *, 768, device=cpu) = onnx::Mul[onnx_name=\"/transformer/h.0/ln_1/Mul\"](%/transformer/h.0/ln_1/Div_output_0, %transformer.h.0.ln_1.weight), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/torch.nn.modules.normalization.LayerNorm::ln_1 # /home/dagus/venv/lib/python3.11/site-packages/torch/nn/functional.py:2573:0\n",
      "  %/transformer/h.0/ln_1/Add_1_output_0 : Float(*, *, 768, strides=[768, 768, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/transformer/h.0/ln_1/Add_1\"](%/transformer/h.0/ln_1/Mul_output_0, %transformer.h.0.ln_1.bias), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/torch.nn.modules.normalization.LayerNorm::ln_1 # /home/dagus/venv/lib/python3.11/site-packages/torch/nn/functional.py:2573:0\n",
      "  %/transformer/h.0/attn/c_attn/Shape_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/transformer/h.0/attn/c_attn/Shape\"](%/transformer/h.0/ln_1/Add_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:102:0\n",
      "  %/transformer/h.0/attn/c_attn/Constant_output_0 : Long(device=cpu) = onnx::Constant[value={0}, onnx_name=\"/transformer/h.0/attn/c_attn/Constant\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:102:0\n",
      "  %/transformer/h.0/attn/c_attn/Gather_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/transformer/h.0/attn/c_attn/Gather\"](%/transformer/h.0/attn/c_attn/Shape_output_0, %/transformer/h.0/attn/c_attn/Constant_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:102:0\n",
      "  %/transformer/h.0/attn/c_attn/Shape_1_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/transformer/h.0/attn/c_attn/Shape_1\"](%/transformer/h.0/ln_1/Add_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:102:0\n",
      "  %/transformer/h.0/attn/c_attn/Constant_1_output_0 : Long(device=cpu) = onnx::Constant[value={1}, onnx_name=\"/transformer/h.0/attn/c_attn/Constant_1\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:102:0\n",
      "  %/transformer/h.0/attn/c_attn/Gather_1_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/transformer/h.0/attn/c_attn/Gather_1\"](%/transformer/h.0/attn/c_attn/Shape_1_output_0, %/transformer/h.0/attn/c_attn/Constant_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:102:0\n",
      "  %/transformer/h.0/attn/c_attn/Shape_2_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/transformer/h.0/attn/c_attn/Shape_2\"](%/transformer/h.0/ln_1/Add_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:103:0\n",
      "  %/transformer/h.0/attn/c_attn/Constant_2_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name=\"/transformer/h.0/attn/c_attn/Constant_2\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:103:0\n",
      "  %/transformer/h.0/attn/c_attn/Constant_3_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-1}, onnx_name=\"/transformer/h.0/attn/c_attn/Constant_3\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:103:0\n",
      "  %/transformer/h.0/attn/c_attn/Constant_4_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name=\"/transformer/h.0/attn/c_attn/Constant_4\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:103:0\n",
      "  %/transformer/h.0/attn/c_attn/Slice_output_0 : Long(1, strides=[1], device=cpu) = onnx::Slice[onnx_name=\"/transformer/h.0/attn/c_attn/Slice\"](%/transformer/h.0/attn/c_attn/Shape_2_output_0, %/transformer/h.0/attn/c_attn/Constant_3_output_0, %/transformer/h.0/attn/c_attn/Constant_4_output_0, %/transformer/h.0/attn/c_attn/Constant_2_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:103:0\n",
      "  %/transformer/h.0/attn/c_attn/Squeeze_output_0 : Long(device=cpu) = onnx::Squeeze[axes=[0], onnx_name=\"/transformer/h.0/attn/c_attn/Squeeze\"](%/transformer/h.0/attn/c_attn/Slice_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:103:0\n",
      "  %/transformer/h.0/attn/c_attn/Constant_5_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name=\"/transformer/h.0/attn/c_attn/Constant_5\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn\n",
      "  %/transformer/h.0/attn/c_attn/Unsqueeze_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"/transformer/h.0/attn/c_attn/Unsqueeze\"](%/transformer/h.0/attn/c_attn/Squeeze_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn\n",
      "  %/transformer/h.0/attn/c_attn/Concat_output_0 : Long(2, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"/transformer/h.0/attn/c_attn/Concat\"](%/transformer/h.0/attn/c_attn/Constant_5_output_0, %/transformer/h.0/attn/c_attn/Unsqueeze_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:103:0\n",
      "  %/transformer/h.0/attn/c_attn/Reshape_output_0 : Float(*, *, strides=[768, 1], requires_grad=1, device=cpu) = onnx::Reshape[onnx_name=\"/transformer/h.0/attn/c_attn/Reshape\"](%/transformer/h.0/ln_1/Add_1_output_0, %/transformer/h.0/attn/c_attn/Concat_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:103:0\n",
      "  %/transformer/h.0/attn/c_attn/Gemm_output_0 : Float(*, 2304, strides=[2304, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., onnx_name=\"/transformer/h.0/attn/c_attn/Gemm\"](%/transformer/h.0/attn/c_attn/Reshape_output_0, %transformer.h.0.attn.c_attn.weight, %transformer.h.0.attn.c_attn.bias), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:103:0\n",
      "  %/transformer/h.0/attn/c_attn/Unsqueeze_1_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"/transformer/h.0/attn/c_attn/Unsqueeze_1\"](%/transformer/h.0/attn/c_attn/Gather_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn\n",
      "  %/transformer/h.0/attn/c_attn/Unsqueeze_2_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"/transformer/h.0/attn/c_attn/Unsqueeze_2\"](%/transformer/h.0/attn/c_attn/Gather_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn\n",
      "  %/transformer/h.0/attn/c_attn/Constant_6_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={2304}, onnx_name=\"/transformer/h.0/attn/c_attn/Constant_6\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn\n",
      "  %/transformer/h.0/attn/c_attn/Concat_1_output_0 : Long(3, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"/transformer/h.0/attn/c_attn/Concat_1\"](%/transformer/h.0/attn/c_attn/Unsqueeze_1_output_0, %/transformer/h.0/attn/c_attn/Unsqueeze_2_output_0, %/transformer/h.0/attn/c_attn/Constant_6_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:104:0\n",
      "  %/transformer/h.0/attn/c_attn/Reshape_1_output_0 : Float(*, *, 2304, strides=[2304, 2304, 1], requires_grad=1, device=cpu) = onnx::Reshape[onnx_name=\"/transformer/h.0/attn/c_attn/Reshape_1\"](%/transformer/h.0/attn/c_attn/Gemm_output_0, %/transformer/h.0/attn/c_attn/Concat_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn/transformers.pytorch_utils.Conv1D::c_attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:104:0\n",
      "  %/transformer/h.0/attn/Split_output_0 : Float(*, *, 768, strides=[2304, 2304, 1], requires_grad=1, device=cpu), %/transformer/h.0/attn/Split_output_1 : Float(*, *, 768, strides=[2304, 2304, 1], requires_grad=1, device=cpu), %/transformer/h.0/attn/Split_output_2 : Float(*, *, 768, strides=[2304, 2304, 1], requires_grad=1, device=cpu) = onnx::Split[axis=2, split=[768, 768, 768], onnx_name=\"/transformer/h.0/attn/Split\"](%/transformer/h.0/attn/c_attn/Reshape_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/torch/_tensor.py:919:0\n",
      "  %/transformer/h.0/attn/Shape_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/transformer/h.0/attn/Shape\"](%/transformer/h.0/attn/Split_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Constant_output_0 : Long(device=cpu) = onnx::Constant[value={0}, onnx_name=\"/transformer/h.0/attn/Constant\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Gather_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/transformer/h.0/attn/Gather\"](%/transformer/h.0/attn/Shape_output_0, %/transformer/h.0/attn/Constant_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Shape_1_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/transformer/h.0/attn/Shape_1\"](%/transformer/h.0/attn/Split_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Constant_1_output_0 : Long(device=cpu) = onnx::Constant[value={1}, onnx_name=\"/transformer/h.0/attn/Constant_1\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Gather_1_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/transformer/h.0/attn/Gather_1\"](%/transformer/h.0/attn/Shape_1_output_0, %/transformer/h.0/attn/Constant_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Unsqueeze_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"/transformer/h.0/attn/Unsqueeze\"](%/transformer/h.0/attn/Gather_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn\n",
      "  %/transformer/h.0/attn/Unsqueeze_1_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"/transformer/h.0/attn/Unsqueeze_1\"](%/transformer/h.0/attn/Gather_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn\n",
      "  %/transformer/h.0/attn/Constant_2_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={12}, onnx_name=\"/transformer/h.0/attn/Constant_2\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn\n",
      "  %/transformer/h.0/attn/Constant_3_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={64}, onnx_name=\"/transformer/h.0/attn/Constant_3\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn\n",
      "  %/transformer/h.0/attn/Concat_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"/transformer/h.0/attn/Concat\"](%/transformer/h.0/attn/Unsqueeze_output_0, %/transformer/h.0/attn/Unsqueeze_1_output_0, %/transformer/h.0/attn/Constant_2_output_0, %/transformer/h.0/attn/Constant_3_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:295:0\n",
      "  %/transformer/h.0/attn/Reshape_output_0 : Float(*, *, 12, 64, strides=[768, 768, 64, 1], requires_grad=1, device=cpu) = onnx::Reshape[onnx_name=\"/transformer/h.0/attn/Reshape\"](%/transformer/h.0/attn/Split_output_0, %/transformer/h.0/attn/Concat_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:295:0\n",
      "  %/transformer/h.0/attn/Transpose_output_0 : Float(*, 12, *, 64, strides=[768, 64, 768, 1], requires_grad=1, device=cpu) = onnx::Transpose[perm=[0, 2, 1, 3], onnx_name=\"/transformer/h.0/attn/Transpose\"](%/transformer/h.0/attn/Reshape_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:296:0\n",
      "  %/transformer/h.0/attn/Shape_2_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/transformer/h.0/attn/Shape_2\"](%/transformer/h.0/attn/Split_output_1), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Constant_4_output_0 : Long(device=cpu) = onnx::Constant[value={0}, onnx_name=\"/transformer/h.0/attn/Constant_4\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Gather_2_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/transformer/h.0/attn/Gather_2\"](%/transformer/h.0/attn/Shape_2_output_0, %/transformer/h.0/attn/Constant_4_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Shape_3_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/transformer/h.0/attn/Shape_3\"](%/transformer/h.0/attn/Split_output_1), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Constant_5_output_0 : Long(device=cpu) = onnx::Constant[value={1}, onnx_name=\"/transformer/h.0/attn/Constant_5\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Gather_3_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/transformer/h.0/attn/Gather_3\"](%/transformer/h.0/attn/Shape_3_output_0, %/transformer/h.0/attn/Constant_5_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Unsqueeze_2_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"/transformer/h.0/attn/Unsqueeze_2\"](%/transformer/h.0/attn/Gather_2_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn\n",
      "  %/transformer/h.0/attn/Unsqueeze_3_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"/transformer/h.0/attn/Unsqueeze_3\"](%/transformer/h.0/attn/Gather_3_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn\n",
      "  %/transformer/h.0/attn/Constant_6_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={12}, onnx_name=\"/transformer/h.0/attn/Constant_6\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn\n",
      "  %/transformer/h.0/attn/Constant_7_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={64}, onnx_name=\"/transformer/h.0/attn/Constant_7\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn\n",
      "  %/transformer/h.0/attn/Concat_1_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"/transformer/h.0/attn/Concat_1\"](%/transformer/h.0/attn/Unsqueeze_2_output_0, %/transformer/h.0/attn/Unsqueeze_3_output_0, %/transformer/h.0/attn/Constant_6_output_0, %/transformer/h.0/attn/Constant_7_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:295:0\n",
      "  %/transformer/h.0/attn/Reshape_1_output_0 : Float(*, *, 12, 64, strides=[768, 768, 64, 1], requires_grad=1, device=cpu) = onnx::Reshape[onnx_name=\"/transformer/h.0/attn/Reshape_1\"](%/transformer/h.0/attn/Split_output_1, %/transformer/h.0/attn/Concat_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:295:0\n",
      "  %/transformer/h.0/attn/Transpose_1_output_0 : Float(*, 12, *, 64, strides=[768, 64, 768, 1], requires_grad=1, device=cpu) = onnx::Transpose[perm=[0, 2, 1, 3], onnx_name=\"/transformer/h.0/attn/Transpose_1\"](%/transformer/h.0/attn/Reshape_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:296:0\n",
      "  %/transformer/h.0/attn/Shape_4_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/transformer/h.0/attn/Shape_4\"](%/transformer/h.0/attn/Split_output_2), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Constant_8_output_0 : Long(device=cpu) = onnx::Constant[value={0}, onnx_name=\"/transformer/h.0/attn/Constant_8\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Gather_4_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/transformer/h.0/attn/Gather_4\"](%/transformer/h.0/attn/Shape_4_output_0, %/transformer/h.0/attn/Constant_8_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Shape_5_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/transformer/h.0/attn/Shape_5\"](%/transformer/h.0/attn/Split_output_2), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Constant_9_output_0 : Long(device=cpu) = onnx::Constant[value={1}, onnx_name=\"/transformer/h.0/attn/Constant_9\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Gather_5_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/transformer/h.0/attn/Gather_5\"](%/transformer/h.0/attn/Shape_5_output_0, %/transformer/h.0/attn/Constant_9_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:294:0\n",
      "  %/transformer/h.0/attn/Unsqueeze_4_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"/transformer/h.0/attn/Unsqueeze_4\"](%/transformer/h.0/attn/Gather_4_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn\n",
      "  %/transformer/h.0/attn/Unsqueeze_5_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"/transformer/h.0/attn/Unsqueeze_5\"](%/transformer/h.0/attn/Gather_5_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn\n",
      "  %/transformer/h.0/attn/Constant_10_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={12}, onnx_name=\"/transformer/h.0/attn/Constant_10\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn\n",
      "  %/transformer/h.0/attn/Constant_11_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={64}, onnx_name=\"/transformer/h.0/attn/Constant_11\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn\n",
      "  %/transformer/h.0/attn/Concat_2_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"/transformer/h.0/attn/Concat_2\"](%/transformer/h.0/attn/Unsqueeze_4_output_0, %/transformer/h.0/attn/Unsqueeze_5_output_0, %/transformer/h.0/attn/Constant_10_output_0, %/transformer/h.0/attn/Constant_11_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:295:0\n",
      "  %/transformer/h.0/attn/Reshape_2_output_0 : Float(*, *, 12, 64, strides=[768, 768, 64, 1], requires_grad=1, device=cpu) = onnx::Reshape[onnx_name=\"/transformer/h.0/attn/Reshape_2\"](%/transformer/h.0/attn/Split_output_2, %/transformer/h.0/attn/Concat_2_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:295:0\n",
      "  %/transformer/h.0/attn/Transpose_2_output_0 : Float(*, 12, *, 64, strides=[768, 64, 768, 1], requires_grad=1, device=cpu) = onnx::Transpose[perm=[0, 2, 1, 3], onnx_name=\"/transformer/h.0/attn/Transpose_2\"](%/transformer/h.0/attn/Reshape_2_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:296:0\n",
      "  %/transformer/h.0/attn/Split_1_output_0 : Float(1, *, 12, *, 64, device=cpu), %/transformer/h.0/attn/Split_1_output_1 : Float(1, *, 12, *, 64, device=cpu) = onnx::Split[axis=0, split=[1, 1], onnx_name=\"/transformer/h.0/attn/Split_1\"](%past_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/torch/_tensor.py:1057:0\n",
      "  %/transformer/h.0/attn/Squeeze_output_0 : Float(*, 12, *, 64, strides=[768, 64, 64, 1], requires_grad=0, device=cpu) = onnx::Squeeze[axes=[0], onnx_name=\"/transformer/h.0/attn/Squeeze\"](%/transformer/h.0/attn/Split_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/torch/_tensor.py:1057:0\n",
      "  %/transformer/h.0/attn/Squeeze_1_output_0 : Float(*, 12, *, 64, strides=[768, 64, 64, 1], requires_grad=0, device=cpu) = onnx::Squeeze[axes=[0], onnx_name=\"/transformer/h.0/attn/Squeeze_1\"](%/transformer/h.0/attn/Split_1_output_1), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/torch/_tensor.py:1057:0\n",
      "  %/transformer/h.0/attn/Concat_3_output_0 : Float(*, 12, *, 64, strides=[1536, 128, 64, 1], requires_grad=1, device=cpu) = onnx::Concat[axis=-2, onnx_name=\"/transformer/h.0/attn/Concat_3\"](%/transformer/h.0/attn/Squeeze_output_0, %/transformer/h.0/attn/Transpose_1_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:336:0\n",
      "  %/transformer/h.0/attn/Concat_4_output_0 : Float(*, 12, *, 64, strides=[1536, 128, 64, 1], requires_grad=1, device=cpu) = onnx::Concat[axis=-2, onnx_name=\"/transformer/h.0/attn/Concat_4\"](%/transformer/h.0/attn/Squeeze_1_output_0, %/transformer/h.0/attn/Transpose_2_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:337:0\n",
      "  %/transformer/h.0/attn/Transpose_3_output_0 : Float(*, 12, 64, *, strides=[1536, 128, 1, 64], requires_grad=1, device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name=\"/transformer/h.0/attn/Transpose_3\"](%/transformer/h.0/attn/Concat_3_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:199:0\n",
      "  %/transformer/h.0/attn/MatMul_output_0 : Float(*, 12, *, *, strides=[24, 2, 2, 1], requires_grad=1, device=cpu) = onnx::MatMul[onnx_name=\"/transformer/h.0/attn/MatMul\"](%/transformer/h.0/attn/Transpose_output_0, %/transformer/h.0/attn/Transpose_3_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:199:0\n",
      "  %/transformer/h.0/attn/Shape_6_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/transformer/h.0/attn/Shape_6\"](%/transformer/h.0/attn/Concat_4_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:203:0\n",
      "  %/transformer/h.0/attn/Constant_12_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name=\"/transformer/h.0/attn/Constant_12\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:203:0\n",
      "  %/transformer/h.0/attn/Constant_13_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-1}, onnx_name=\"/transformer/h.0/attn/Constant_13\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:203:0\n",
      "  %/transformer/h.0/attn/Constant_14_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name=\"/transformer/h.0/attn/Constant_14\"](), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_gpt2.GPT2Model::transformer/transformers.models.gpt2.modeling_gpt2.GPT2Block::h.0/transformers.models.gpt2.modeling_gpt2.GPT2Attention::attn # /home/dagus/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:203:0\n",
      "  %/transformer/h.0/attn/Slice_output_0 : Long(1, strides=[1], device=cpu) = onnx::Slice[onnx_name=\"/transformer/h.0/attn/Slice\"](%/transformer/h.0/attn/Shape_6_output_0, %/transformer/h.0/attn/Constant_13_output_0, %/transformer/h.0/attn/Constant_14_output_0, %/transformer/h.0/attn/Constant_12_output_0), scope: onnxruntime_tools.transformers.gpt2_helper.MyGPT2LMHeadModel::/transformers.models.gpt2.modeling_"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConverting model...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Use GPT2Helper from onnxruntime tools to export GPT2-XL.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mGpt2Helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_onnx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[43monnx_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[43muse_external_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/onnxruntime_tools/transformers/gpt2_helper.py:327\u001b[0m, in \u001b[0;36mGpt2Helper.export_onnx\u001b[0;34m(model, device, onnx_model_path, verbose, use_external_data_format, has_position_ids, has_attention_mask)\u001b[0m\n\u001b[1;32m    321\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapes: input_ids=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdummy_inputs\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m past=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdummy_inputs\u001b[38;5;241m.\u001b[39mpast[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m output=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m present=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    323\u001b[0m )\n\u001b[1;32m    325\u001b[0m Path(onnx_model_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 327\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m                  \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monnx_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m                  \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m#example_outputs=outputs,\u001b[39;49;00m\n\u001b[1;32m    333\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m#use_external_data_format=use_external_data_format,\u001b[39;49;00m\n\u001b[1;32m    337\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/onnx/utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/onnx/utils.py:1691\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1686\u001b[0m proto \u001b[38;5;241m=\u001b[39m onnx_proto_utils\u001b[38;5;241m.\u001b[39m_add_onnxscript_fn(\n\u001b[1;32m   1687\u001b[0m     proto,\n\u001b[1;32m   1688\u001b[0m     custom_opsets,\n\u001b[1;32m   1689\u001b[0m )\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m-> 1691\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExported graph: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1692\u001b[0m onnx_proto_utils\u001b[38;5;241m.\u001b[39m_export_file(proto, f, export_type, export_map)\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;66;03m# The ONNX checker only works for ONNX graph. So if the operator_export_type is not ONNX,\u001b[39;00m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# we can skip this check.\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;66;03m# If large model format export is enabled, proto will only contain data location instead of\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;66;03m# raw data and _check_onnx_proto() will fail because it can only handle the raw ONNX proto\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;66;03m# string in memory.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from onnxruntime_tools.transformers.gpt2_helper import Gpt2Helper, MyGPT2LMHeadModel\n",
    "device = 'cpu'\n",
    "# Use GPT2 model wrapper from onnxruntime's gpt2 helper supple.\n",
    "# conversion will not work without this wrapper.\n",
    "#print('Downloading GPT2-XL...')\n",
    "model = MyGPT2LMHeadModel.from_pretrained('recipes_generation_model')\n",
    "onnx_model_path = \"recipes_generation_model.onnx\"\n",
    "print('Converting model...')\n",
    "# Use GPT2Helper from onnxruntime tools to export GPT2-XL.\n",
    "Gpt2Helper.export_onnx(\n",
    "  model,\n",
    "  device,\n",
    "  onnx_model_path,\n",
    "  use_external_data_format=True,\n",
    "  verbose=True\n",
    ")\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f45e91c-9e45-4166-8190-4a5a1f4e9d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: onnxruntime_tools is deprecated. Use onnxruntime or onnxruntime-gpu instead. For more information, see https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/README.md.\n",
      "\n",
      "num heads: 12. hidden size: 768.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 2.235682039497533e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -6.794542706245466e-10 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 1.2535335436325568e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -1.1795354915022926e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 7.348716479782524e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 1.4994551378322285e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -1.2810025040721484e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -2.735129678299586e-09 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 3.2045431197502694e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -1.691036999318385e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -3.0476755341624084e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 4.007114640103282e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -2.8763723847191613e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 2.3770017065771754e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -3.699700457104882e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -6.474437697079338e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 3.6225522137556254e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 2.3395045900542755e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -4.990588564623977e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/home/dagus/venv/lib/python3.11/site-packages/onnxconverter_common/float16.py:50: UserWarning: the float32 number -3.4028234663852886e+38 will be truncated to -10000.0\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_min, -max_finite_val))\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime_tools import optimizer\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "tf_model = AutoModelForCausalLM.from_pretrained('recipes_generation_model')\n",
    "print('num heads: %s. hidden size: %s.' % (tf_model.config.n_head, tf_model.config.n_embd))\n",
    "optimized_model = optimizer.optimize_model(\n",
    "    \"recipes_generation_model.onnx\",\n",
    "    model_type='gpt2',\n",
    "    num_heads=tf_model.config.n_head,\n",
    "    hidden_size=tf_model.config.n_embd,\n",
    ")\n",
    "optimized_model.convert_model_float32_to_float16()\n",
    "optimized_model.change_input_to_int32()\n",
    "optimized_model.save_model_to_file(\"recipes_generation_model-optimized.onnx\", use_external_data_format=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b46e79ff-bc47-4c05-9c35-2da877471067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from psutil import cpu_count\n",
    "from os import environ\n",
    "# Constants from the performance optimization available in onnxruntime\n",
    "# It needs to be done before importing onnxruntime\n",
    "environ[\"OMP_NUM_THREADS\"] = str(cpu_count(logical=True))\n",
    "environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'\n",
    "from onnxruntime_tools.transformers.gpt2_helper import Gpt2Helper\n",
    "from onnxruntime.capi._pybind_state import set_seed as ort_set_seed\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import numpy\n",
    "from typing import Iterable, List, Optional, Tuple, Any\n",
    "import random\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_banned_ngram_tokens(prev_input_ids: Tensor, num_hypos: int, no_repeat_ngram_size: int, cur_len: int) -> List:\n",
    "    \"\"\"Copied from fairseq for no_repeat_ngram in beam_search\"\"\"\n",
    "    if cur_len + 1 < no_repeat_ngram_size:\n",
    "        # return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\n",
    "        return [[] for _ in range(num_hypos)]\n",
    "    generated_ngrams = [{} for _ in range(num_hypos)]\n",
    "    for idx in range(num_hypos):\n",
    "        gen_tokens = prev_input_ids[idx].tolist()\n",
    "        generated_ngram = generated_ngrams[idx]\n",
    "        for ngram in zip(*[gen_tokens[i:] for i in range(no_repeat_ngram_size)]):\n",
    "            prev_ngram_tuple = tuple(ngram[:-1])\n",
    "            generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n",
    "\n",
    "    def _get_generated_ngrams(hypo_idx):\n",
    "        # Before decoding the next token, prevent decoding of ngrams that have already appeared\n",
    "        start_idx = cur_len + 1 - no_repeat_ngram_size\n",
    "        ngram_idx = tuple(prev_input_ids[hypo_idx, start_idx:cur_len].tolist())\n",
    "        return generated_ngrams[hypo_idx].get(ngram_idx, [])\n",
    "\n",
    "    banned_tokens = [_get_generated_ngrams(hypo_idx) for hypo_idx in range(num_hypos)]\n",
    "    return banned_tokens\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def enforce_repetition_penalty_(lprobs, batch_size, num_beams, prev_output_tokens, repetition_penalty):\n",
    "    \"\"\"\n",
    "    Enforce the repetition penalty (from the `CTRL paper `__).\n",
    "    \"\"\"\n",
    "    for i in range(batch_size * num_beams):\n",
    "        for previous_token in set(prev_output_tokens[i].tolist()):\n",
    "            # if score < 0 then repetition penalty has to multiplied to reduce the previous token probability\n",
    "            if lprobs[i, previous_token] < 0:\n",
    "                lprobs[i, previous_token] *= repetition_penalty\n",
    "            else:\n",
    "                lprobs[i, previous_token] /= repetition_penalty\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_banned_bad_words_ids(prev_input_ids: Iterable[int], bad_words_ids: Iterable[int]) -> Iterable[int]:\n",
    "    banned_tokens = []\n",
    "    def _tokens_match(prev_tokens, tokens):\n",
    "        if len(tokens) == 0:\n",
    "            # if bad word tokens is just one token always ban it\n",
    "            return True\n",
    "        if len(tokens) > len(prev_tokens):\n",
    "            # if bad word tokens are longer than prev tokens they can't be equal\n",
    "            return False\n",
    "\n",
    "        if prev_tokens[-len(tokens):] == tokens:\n",
    "            # if tokens match\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    for prev_input_ids_slice in prev_input_ids:\n",
    "        banned_tokens_slice = []\n",
    "        for banned_token_seq in bad_words_ids:\n",
    "            assert len(banned_token_seq) > 0, \"Banned words token sequences {} cannot have an empty list\".format(\n",
    "                bad_words_ids\n",
    "            )\n",
    "            if _tokens_match(prev_input_ids_slice, banned_token_seq[:-1]) is False:\n",
    "                # if tokens do not match continue\n",
    "                continue\n",
    "            banned_tokens_slice.append(banned_token_seq[-1])\n",
    "        banned_tokens.append(banned_tokens_slice)\n",
    "    return banned_tokens\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def set_scores_to_inf_for_banned_tokens(scores: torch.Tensor, banned_tokens: List[List[int]]) -> None:\n",
    "    \"\"\"Modifies the scores in place by setting the banned token positions to `-inf`. Banned token is expected to be\n",
    "    a list of list of banned tokens to ban in the format [[batch index, vocabulary position],...]\n",
    "        Args:\n",
    "            scores: logits distribution of shape (batch size, vocabulary size)\n",
    "            banned_tokens: list of list of tokens to ban of length (batch_size)\n",
    "    \"\"\"\n",
    "    banned_mask_list = []\n",
    "    for idx, batch_banned_tokens in enumerate(banned_tokens):\n",
    "        for token in batch_banned_tokens:\n",
    "            banned_mask_list.append([idx, token])\n",
    "    if not banned_mask_list:\n",
    "        return\n",
    "    banned_mask = torch.LongTensor(banned_mask_list)\n",
    "    indices = torch.ones(len(banned_mask))\n",
    "    # A sparse tensor is generated from a list of coordinates: [[0, 1], [0, 2], [2, 0]].\n",
    "    banned_mask = torch.sparse.Tensor(banned_mask.t(), indices, scores.size()).to(scores.device).to_dense().bool()\n",
    "    scores.masked_fill_(banned_mask, -float(\"inf\"))\n",
    "\n",
    "\n",
    "class BeamHypotheses(object):\n",
    "    def __init__(self, num_beams, max_length, length_penalty, early_stopping):\n",
    "        \"\"\"\n",
    "        Initialize n-best list of hypotheses.\n",
    "        \"\"\"\n",
    "        self.max_length = max_length - 1  # ignoring bos_token\n",
    "        self.length_penalty = length_penalty\n",
    "        self.early_stopping = early_stopping\n",
    "        self.num_beams = num_beams\n",
    "        self.beams = []\n",
    "        self.worst_score = 1e9\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of hypotheses in the list.\n",
    "        \"\"\"\n",
    "        return len(self.beams)\n",
    "\n",
    "    def add(self, hyp, sum_logprobs):\n",
    "        \"\"\"\n",
    "        Add a new hypothesis to the list.\n",
    "        \"\"\"\n",
    "        score = sum_logprobs / len(hyp) ** self.length_penalty\n",
    "        if len(self) < self.num_beams or score > self.worst_score:\n",
    "            self.beams.append((score, hyp))\n",
    "            if len(self) > self.num_beams:\n",
    "                sorted_scores = sorted([(s, idx) for idx, (s, _) in enumerate(self.beams)])\n",
    "                del self.beams[sorted_scores[0][1]]\n",
    "                self.worst_score = sorted_scores[1][0]\n",
    "            else:\n",
    "                self.worst_score = min(score, self.worst_score)\n",
    "\n",
    "    def is_done(self, best_sum_logprobs, cur_len):\n",
    "        \"\"\"\n",
    "        If there are enough hypotheses and that none of the hypotheses being generated\n",
    "        can become better than the worst one in the heap, then we are done with this sentence.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self) < self.num_beams:\n",
    "            return False\n",
    "        elif self.early_stopping:\n",
    "            return True\n",
    "        else:\n",
    "            cur_score = best_sum_logprobs / cur_len ** self.length_penalty\n",
    "            ret = self.worst_score >= cur_score\n",
    "            return ret\n",
    "\n",
    "\n",
    "class GPT2ONNXModel:\n",
    "    def __init__(self, onnx_model_path, gpt2_model_path, device='cuda', verbose=False, threads=None, is_float16=False):\n",
    "        self.config = AutoConfig.from_pretrained(gpt2_model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(gpt2_model_path)\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.num_attention_heads = self.config.n_head\n",
    "        self.hidden_size = self.config.n_embd\n",
    "        self.num_layer = self.config.n_layer\n",
    "        self.verbose = verbose\n",
    "        self.is_float16 = is_float16\n",
    "        # Set the seed for onnxruntime.\n",
    "        torch.random.manual_seed(random.randint(1, 9999999))\n",
    "        torch.cuda.manual_seed_all(random.randint(1, 9999999))\n",
    "        torch.manual_seed(random.randint(1, 9999999))\n",
    "        numpy.random.seed(random.randint(1, 9999999))\n",
    "        ort.set_seed(random.randint(1, 9999999))\n",
    "        # Set our session options.\n",
    "        options = ort.SessionOptions()\n",
    "        # set session threads if user supplied.\n",
    "        if threads is not None:\n",
    "            options.inter_op_num_threads = threads\n",
    "            options.intra_op_num_threads = threads\n",
    "        # options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "        # Only CPU supports parallel.\n",
    "        if device == 'cpu':\n",
    "            options.execution_mode = ort.ExecutionMode.ORT_PARALLEL\n",
    "        # Start the ONNX session.\n",
    "        self.session = ort.InferenceSession(\n",
    "            onnx_model_path,\n",
    "            sess_options=options,\n",
    "            providers=['CUDAExecutionProvider' if device == 'cuda' else 'CPUExecutionProvider']\n",
    "        )\n",
    "        self.device = device\n",
    "        if self.verbose is True:\n",
    "            print('ONNX Session Created!')\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_text, max_length: Optional[int] = None, min_length: Optional[int] = None, do_sample: Optional[bool] = None,\n",
    "        early_stopping: Optional[bool] = None, num_beams: Optional[int] = None, temperature: Optional[float] = None,\n",
    "        top_k: Optional[int] = None, top_p: Optional[float] = None, repetition_penalty: Optional[float] = None, bad_words_ids: Optional[Iterable[int]] = None,\n",
    "        bos_token_id: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Optional[int] = None, length_penalty: Optional[float] = None,\n",
    "        no_repeat_ngram_size: Optional[int] = None, num_return_sequences: Optional[int] = None, use_cache: Optional[bool] = None,\n",
    "        **model_kwargs\n",
    "    ) -> torch.LongTensor:\n",
    "        r\"\"\"\n",
    "        Generates sequences for models with a language modeling head. The method currently supports greedy decoding,\n",
    "        beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(input_text) > 1024:\n",
    "            input_text = input_text[-1024:]\n",
    "\n",
    "        # Get inputs from input text. (input ids, attention mask, position, and past)\n",
    "        input_ids, attention_mask, position_ids, past = self.get_inputs(input_text)\n",
    "        # Get user supplied text generation options. Replace with model config on unset options.\n",
    "        max_length = max_length if max_length is not None else self.config.max_length\n",
    "        min_length = min_length if min_length is not None else self.config.min_length\n",
    "        do_sample = do_sample if do_sample is not None else self.config.do_sample\n",
    "        early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        num_beams = num_beams if num_beams is not None else self.config.num_beams\n",
    "        temperature = temperature if temperature is not None else self.config.temperature\n",
    "        top_k = top_k if top_k is not None else self.config.top_k\n",
    "        top_p = top_p if top_p is not None else self.config.top_p\n",
    "        repetition_penalty = repetition_penalty if repetition_penalty is not None else self.config.repetition_penalty\n",
    "        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "        length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
    "        no_repeat_ngram_size = (\n",
    "            no_repeat_ngram_size if no_repeat_ngram_size is not None else self.config.no_repeat_ngram_size\n",
    "        )\n",
    "        bad_words_ids = bad_words_ids if bad_words_ids is not None else self.config.bad_words_ids\n",
    "        num_return_sequences = (\n",
    "            num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\n",
    "        )\n",
    "        if input_ids is not None:\n",
    "            batch_size = input_ids.shape[0]  # overriden by the input batch_size\n",
    "        else:\n",
    "            batch_size = 1\n",
    "        # Make sure all options were set correctly.\n",
    "        assert isinstance(max_length, int) and max_length > 0, \"`max_length` should be a strictly positive integer.\"\n",
    "        assert isinstance(min_length, int) and min_length >= 0, \"`min_length` should be a positive integer.\"\n",
    "        assert isinstance(do_sample, bool), \"`do_sample` should be a boolean.\"\n",
    "        assert isinstance(early_stopping, bool), \"`early_stopping` should be a boolean.\"\n",
    "        assert isinstance(use_cache, bool), \"`use_cache` should be a boolean.\"\n",
    "        assert isinstance(num_beams, int) and num_beams > 0, \"`num_beams` should be a strictly positive integer.\"\n",
    "        assert temperature > 0, \"`temperature` should be strictly positive.\"\n",
    "        assert isinstance(top_k, int) and top_k >= 0, \"`top_k` should be a positive integer.\"\n",
    "        assert 0 <= top_p <= 1, \"`top_p` should be between 0 and 1.\"\n",
    "        assert repetition_penalty >= 1.0, \"`repetition_penalty` should be >= 1.\"\n",
    "        assert pad_token_id is None or (\n",
    "            isinstance(pad_token_id, int) and (pad_token_id >= 0)\n",
    "        ), \"`pad_token_id` should be a positive integer.\"\n",
    "        assert (eos_token_id is None) or (\n",
    "            isinstance(eos_token_id, int) and (eos_token_id >= 0)\n",
    "        ), \"`eos_token_id` should be a positive integer.\"\n",
    "        assert length_penalty > 0, \"`length_penalty` should be strictly positive.\"\n",
    "        assert (\n",
    "            isinstance(no_repeat_ngram_size, int) and no_repeat_ngram_size >= 0\n",
    "        ), \"`no_repeat_ngram_size` should be a positive integer.\"\n",
    "        assert (\n",
    "            isinstance(num_return_sequences, int) and num_return_sequences > 0\n",
    "        ), \"`num_return_sequences` should be a strictly positive integer.\"\n",
    "        assert (\n",
    "            bad_words_ids is None or isinstance(bad_words_ids, list) and isinstance(bad_words_ids[0], list)\n",
    "        ), \"`bad_words_ids` is either `None` or a list of lists of tokens that should not be generated\"\n",
    "\n",
    "        # not allow to duplicate outputs when greedy decoding\n",
    "        if do_sample is False:\n",
    "            if num_beams == 1:\n",
    "                # no_beam_search greedy generation conditions\n",
    "                assert (\n",
    "                    num_return_sequences == 1\n",
    "                ), \"Greedy decoding will always produce the same output for num_beams == 1 and num_return_sequences > 1. Please set num_return_sequences = 1\"\n",
    "\n",
    "            else:\n",
    "                # beam_search greedy generation conditions\n",
    "                assert (\n",
    "                    num_beams >= num_return_sequences\n",
    "                ), \"Greedy beam search decoding cannot return more sequences than it has beams. Please set num_beams >= num_return_sequences\"\n",
    "\n",
    "        max_length = len(input_ids[0]) + max_length\n",
    "\n",
    "        # create attention mask if necessary\n",
    "        # TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140\n",
    "        if (attention_mask is None) and (pad_token_id is not None) and (pad_token_id in input_ids):\n",
    "            attention_mask = input_ids.ne(pad_token_id).long()\n",
    "        elif attention_mask is None:\n",
    "            attention_mask = input_ids.new_ones(input_ids.shape)\n",
    "\n",
    "        # set pad_token_id to eos_token_id if not set. Important that this is done after\n",
    "        # attention_mask is created\n",
    "        if pad_token_id is None and eos_token_id is not None:\n",
    "            pad_token_id = eos_token_id\n",
    "\n",
    "        # current position and vocab size\n",
    "        vocab_size = self.config.vocab_size\n",
    "\n",
    "        # set effective batch size and effective batch multiplier according to do_sample\n",
    "        if do_sample:\n",
    "            effective_batch_size = batch_size * num_return_sequences\n",
    "            effective_batch_mult = num_return_sequences\n",
    "        else:\n",
    "            effective_batch_size = batch_size\n",
    "            effective_batch_mult = 1\n",
    "\n",
    "        # Expand input ids if num_beams > 1 or num_return_sequences > 1\n",
    "        if num_return_sequences > 1 or num_beams > 1:\n",
    "            input_ids_len = input_ids.shape[-1]\n",
    "            input_ids = input_ids.unsqueeze(1).expand(batch_size, effective_batch_mult * num_beams, input_ids_len)\n",
    "            attention_mask = attention_mask.unsqueeze(1).expand(\n",
    "                batch_size, effective_batch_mult * num_beams, input_ids_len\n",
    "            )\n",
    "\n",
    "            input_ids = input_ids.contiguous().view(\n",
    "                effective_batch_size * num_beams, input_ids_len\n",
    "            )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)\n",
    "            attention_mask = attention_mask.contiguous().view(\n",
    "                effective_batch_size * num_beams, input_ids_len\n",
    "            )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)\n",
    "\n",
    "        cur_len = input_ids.shape[-1]\n",
    "\n",
    "        assert (\n",
    "            cur_len < max_length\n",
    "        ), f\"The context has {cur_len} number of tokens, but `max_length` is only {max_length}. Please make sure \" \\\n",
    "           f\"that `max_length` is bigger than the number of tokens, by setting either `generate(max_length=...,...)` \" \\\n",
    "           f\"or `config.max_length = ...`\"\n",
    "\n",
    "        if num_beams > 1:\n",
    "            output = self._generate_beam_search(\n",
    "                input_ids, cur_len=cur_len, max_length=max_length, min_length=min_length, do_sample=do_sample,\n",
    "                early_stopping=early_stopping, temperature=temperature, top_k=top_k, top_p=top_p,\n",
    "                repetition_penalty=repetition_penalty, no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                bad_words_ids=bad_words_ids, pad_token_id=pad_token_id, eos_token_id=eos_token_id,\n",
    "                batch_size=effective_batch_size, num_return_sequences=num_return_sequences,\n",
    "                length_penalty=length_penalty, num_beams=num_beams, vocab_size=vocab_size,\n",
    "                attention_mask=attention_mask, past=past, position_ids=position_ids, use_cache=use_cache,\n",
    "                model_kwargs=model_kwargs,\n",
    "            )\n",
    "        else:\n",
    "            output = self._generate_no_beam_search(\n",
    "                input_ids, cur_len=cur_len, max_length=max_length, min_length=min_length, do_sample=do_sample,\n",
    "                temperature=temperature, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty,\n",
    "                no_repeat_ngram_size=no_repeat_ngram_size, bad_words_ids=bad_words_ids, pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id, batch_size=effective_batch_size, attention_mask=attention_mask,\n",
    "                use_cache=use_cache, model_kwargs=model_kwargs,\n",
    "            )\n",
    "\n",
    "        output = self.tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _generate_no_beam_search(self, input_ids, cur_len, max_length, min_length, do_sample, temperature,\n",
    "        top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, pad_token_id, eos_token_id,\n",
    "        batch_size, attention_mask, use_cache, model_kwargs,\n",
    "    ):\n",
    "        \"\"\"Generate sequences for each example without beam search (num_beams == 1).\n",
    "        All returned sequence are generated independantly.\n",
    "        \"\"\"\n",
    "        # length of generated sentences / unfinished sentences\n",
    "        unfinished_sents = input_ids.new(batch_size).fill_(1)\n",
    "        sent_lengths = input_ids.new(batch_size).fill_(max_length)\n",
    "\n",
    "        past_shape = [2, input_ids.shape[0], self.num_attention_heads, 0, self.hidden_size // self.num_attention_heads]\n",
    "        past = []\n",
    "        for i in range(self.num_layer):\n",
    "            past.append(torch.empty(past_shape, dtype=torch.float32).to(self.device))\n",
    "        attention_mask = attention_mask.clone().detach()\n",
    "        \n",
    "        while cur_len < max_length:\n",
    "\n",
    "            position_ids = (attention_mask.long().cumsum(-1) - 1)\n",
    "            position_ids.masked_fill_(position_ids < 0, 0)\n",
    "\n",
    "            outputs = self.inference_with_io_binding(\n",
    "                input_ids.to(self.device),\n",
    "                position_ids.to(self.device),\n",
    "                attention_mask.to(self.device),\n",
    "                past\n",
    "            )\n",
    "            next_token_logits = outputs[0][:, -1, :]\n",
    "\n",
    "            scores = self.postprocess_next_token_scores(\n",
    "                scores=next_token_logits, input_ids=input_ids, no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                bad_words_ids=bad_words_ids, cur_len=cur_len, min_length=min_length, max_length=max_length,\n",
    "                eos_token_id=eos_token_id, repetition_penalty=repetition_penalty, batch_size=batch_size, num_beams=1,\n",
    "            )\n",
    "\n",
    "            # if model has past, then set the past variable to speed up decoding\n",
    "            # if \"past_key_values\" in outputs:\n",
    "            #     past = outputs.past_key_values\n",
    "            # elif \"mems\" in outputs:\n",
    "            #     past = outputs.mems\n",
    "\n",
    "            if do_sample:\n",
    "                # Temperature (higher temperature => more likely to sample low probability tokens)\n",
    "                if temperature != 1.0:\n",
    "                    scores = scores / temperature\n",
    "                # Top-p/top-k filtering\n",
    "                next_token_logscores = self.top_k_top_p_filtering(scores, top_k=top_k, top_p=top_p)\n",
    "                # Sample\n",
    "                probs = F.softmax(next_token_logscores, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "            else:\n",
    "                # Greedy decoding\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "\n",
    "            # update generations and finished sentences\n",
    "            if eos_token_id is not None:\n",
    "                # pad finished sentences if eos_token_id exist\n",
    "                tokens_to_add = next_token * unfinished_sents + (pad_token_id) * (1 - unfinished_sents)\n",
    "            else:\n",
    "                tokens_to_add = next_token\n",
    "\n",
    "            # add token and increase length by one\n",
    "            input_ids = torch.cat([input_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            if eos_token_id is not None:\n",
    "                eos_in_sents = tokens_to_add == eos_token_id\n",
    "                # if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length\n",
    "                is_sents_unfinished_and_token_to_add_is_eos = unfinished_sents.mul(eos_in_sents.long()).bool()\n",
    "                sent_lengths.masked_fill_(is_sents_unfinished_and_token_to_add_is_eos, cur_len)\n",
    "                # unfinished_sents is set to zero if eos in sentence\n",
    "                unfinished_sents.mul_((~eos_in_sents).long())\n",
    "\n",
    "            # stop when there is a  in each sentence, or if we exceed the maximul length\n",
    "            if unfinished_sents.max() == 0:\n",
    "                break\n",
    "\n",
    "            attention_mask = torch.cat(\n",
    "                [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
    "            )\n",
    "\n",
    "        return input_ids\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _generate_beam_search(self, input_ids, cur_len, max_length, min_length, do_sample, early_stopping, temperature,\n",
    "        top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, pad_token_id, eos_token_id, batch_size,\n",
    "        num_return_sequences, length_penalty, num_beams, vocab_size, attention_mask, past, position_ids, use_cache,\n",
    "        model_kwargs,\n",
    "    ):\n",
    "        \"\"\"Generate sequences for each example with beam search.\"\"\"\n",
    "\n",
    "        # generated hypotheses\n",
    "        generated_hyps = [\n",
    "            BeamHypotheses(num_beams, max_length, length_penalty, early_stopping=early_stopping)\n",
    "            for _ in range(batch_size)\n",
    "        ]\n",
    "\n",
    "        # scores for each sentence in the beam\n",
    "        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
    "\n",
    "        # for greedy decoding it is made sure that only tokens of the first beam are considered to avoid sampling the exact same tokens three times\n",
    "        if do_sample is False:\n",
    "            beam_scores[:, 1:] = -1e9\n",
    "        beam_scores = beam_scores.view(-1)  # shape (batch_size * num_beams,)\n",
    "\n",
    "        # done sentences\n",
    "        done = [False for _ in range(batch_size)]\n",
    "\n",
    "        past_shape = [2, batch_size * num_beams, self.num_attention_heads, 0, self.hidden_size // self.num_attention_heads]\n",
    "        past = []\n",
    "        for i in range(self.num_layer):\n",
    "            past.append(torch.empty(past_shape, dtype=torch.float32).to(self.device))\n",
    "\n",
    "        next_tokens = []\n",
    "        while cur_len < max_length:\n",
    "            position_ids = (attention_mask.long().cumsum(-1) - 1)\n",
    "            position_ids.masked_fill_(position_ids < 0, 0)\n",
    "            # outputs = self(**model_inputs, return_dict=True)  # (batch_size * num_beams, cur_len, vocab_size)\n",
    "            outputs = self.inference_with_io_binding(\n",
    "                input_ids.to(self.device),\n",
    "                position_ids.to(self.device),\n",
    "                attention_mask.to(self.device),\n",
    "                past\n",
    "            )\n",
    "            next_token_logits = outputs[0][:, -1, :]  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "            # if model has past, then set the past variable to speed up decoding\n",
    "            # if \"past_key_values\" in outputs:\n",
    "            #     past = outputs.past_key_values\n",
    "            # elif \"mems\" in outputs:\n",
    "            #     past = outputs.mems\n",
    "\n",
    "            scores = F.log_softmax(next_token_logits, dim=-1)  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "            scores = self.postprocess_next_token_scores(scores=scores, input_ids=input_ids, no_repeat_ngram_size=no_repeat_ngram_size, bad_words_ids=bad_words_ids,\n",
    "                cur_len=cur_len, min_length=min_length, max_length=max_length, eos_token_id=eos_token_id, repetition_penalty=repetition_penalty,\n",
    "                batch_size=batch_size, num_beams=num_beams,\n",
    "            )\n",
    "\n",
    "            assert scores.shape == (batch_size * num_beams, vocab_size), \"Shapes of scores: {} != {}\".format(\n",
    "                scores.shape, (batch_size * num_beams, vocab_size)\n",
    "            )\n",
    "\n",
    "            if do_sample:\n",
    "                _scores = scores + beam_scores[:, None].expand_as(scores)  # (batch_size * num_beams, vocab_size)\n",
    "                # Temperature\n",
    "                if temperature != 1.0:\n",
    "                    _scores = _scores / temperature\n",
    "                # Top-p/top-k filtering\n",
    "                _scores = self.top_k_top_p_filtering(\n",
    "                    _scores, top_k=top_k, top_p=top_p, min_tokens_to_keep=2\n",
    "                )  # (batch_size * num_beams, vocab_size)\n",
    "                # re-organize to group the beam together to sample from all beam_idxs\n",
    "                _scores = _scores.contiguous().view(\n",
    "                    batch_size, num_beams * vocab_size\n",
    "                )  # (batch_size, num_beams * vocab_size)\n",
    "\n",
    "                # Sample 2 next tokens for each beam (so we have some spare tokens and match output of greedy beam search)\n",
    "                probs = F.softmax(_scores, dim=-1)\n",
    "                next_tokens = torch.multinomial(probs, num_samples=2 * num_beams)  # (batch_size, num_beams * 2)\n",
    "                # Compute next scores\n",
    "                next_scores = torch.gather(_scores, -1, next_tokens)  # (batch_size, num_beams * 2)\n",
    "                # sort the sampled vector to make sure that the first num_beams samples are the best\n",
    "                next_scores, next_scores_indices = torch.sort(next_scores, descending=True, dim=1)\n",
    "                next_tokens = torch.gather(next_tokens, -1, next_scores_indices)  # (batch_size, num_beams * 2)\n",
    "\n",
    "            else:\n",
    "                next_scores = scores + beam_scores[:, None].expand_as(scores)  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "                # re-organize to group the beam together (we are keeping top hypothesis accross beams)\n",
    "                next_scores = next_scores.view(\n",
    "                    batch_size, num_beams * vocab_size\n",
    "                )  # (batch_size, num_beams * vocab_size)\n",
    "\n",
    "                next_scores, next_tokens = torch.topk(next_scores, 2 * num_beams, dim=1, largest=True, sorted=True)\n",
    "\n",
    "            assert next_scores.size() == next_tokens.size() == (batch_size, 2 * num_beams)\n",
    "\n",
    "            # next batch beam content\n",
    "            next_batch_beam = []\n",
    "\n",
    "            # for each sentence\n",
    "            for batch_idx in range(batch_size):\n",
    "\n",
    "                # if we are done with this sentence, add a pad token\n",
    "                if done[batch_idx]:\n",
    "                    assert (\n",
    "                        len(generated_hyps[batch_idx]) >= num_beams\n",
    "                    ), \"Batch can only be done if at least {} beams have been generated\".format(num_beams)\n",
    "                    assert (\n",
    "                        eos_token_id is not None and pad_token_id is not None\n",
    "                    ), \"generated beams >= num_beams -> eos_token_id and pad_token have to be defined\"\n",
    "                    next_batch_beam.extend([(0, pad_token_id, 0)] * num_beams)  # pad the batch\n",
    "                    continue\n",
    "\n",
    "                # next sentence beam content, this will get added to next_batch_beam\n",
    "                next_sent_beam = []\n",
    "\n",
    "                # next tokens for this sentence\n",
    "                for beam_token_rank, (beam_token_id, beam_token_score) in enumerate(\n",
    "                    zip(next_tokens[batch_idx], next_scores[batch_idx])\n",
    "                ):\n",
    "                    # get beam and token IDs\n",
    "                    beam_id = beam_token_id // vocab_size\n",
    "                    token_id = beam_token_id % vocab_size\n",
    "\n",
    "                    effective_beam_id = batch_idx * num_beams + beam_id\n",
    "                    # add to generated hypotheses if end of sentence\n",
    "                    if (eos_token_id is not None) and (token_id.item() == eos_token_id):\n",
    "                        # if beam_token does not belong to top num_beams tokens, it should not be added\n",
    "                        is_beam_token_worse_than_top_num_beams = beam_token_rank >= num_beams\n",
    "                        if is_beam_token_worse_than_top_num_beams:\n",
    "                            continue\n",
    "                        generated_hyps[batch_idx].add(\n",
    "                            input_ids[effective_beam_id].clone(),\n",
    "                            beam_token_score.item(),\n",
    "                        )\n",
    "                    else:\n",
    "                        # add next predicted token since it is not eos_token\n",
    "                        next_sent_beam.append((beam_token_score, token_id, effective_beam_id))\n",
    "\n",
    "                    # once the beam for next step is full, don't add more tokens to it.\n",
    "                    if len(next_sent_beam) == num_beams:\n",
    "                        break\n",
    "\n",
    "                # Check if we are done so that we can save a pad step if all(done)\n",
    "                done[batch_idx] = done[batch_idx] or generated_hyps[batch_idx].is_done(\n",
    "                    next_scores[batch_idx].max().item(), cur_len\n",
    "                )\n",
    "\n",
    "                # update next beam content\n",
    "                assert len(next_sent_beam) == num_beams, \"Beam should always be full\"\n",
    "                next_batch_beam.extend(next_sent_beam)\n",
    "                assert len(next_batch_beam) == num_beams * (batch_idx + 1), \"We should have added num_beams each step\"\n",
    "\n",
    "            # stop when we are done with each sentence\n",
    "            if all(done):\n",
    "                break\n",
    "\n",
    "            # sanity check / prepare next batch\n",
    "            assert len(next_batch_beam) == batch_size * num_beams\n",
    "            beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\n",
    "            beam_tokens = input_ids.new([x[1] for x in next_batch_beam])\n",
    "            beam_idx = input_ids.new([x[2] for x in next_batch_beam])\n",
    "\n",
    "            # re-order batch and update current length\n",
    "            input_ids = input_ids[beam_idx, :]\n",
    "            input_ids = torch.cat([input_ids, beam_tokens.unsqueeze(1)], dim=-1)\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            # re-order internal states\n",
    "            if past is not None:\n",
    "                past = self._reorder_cache(past, beam_idx)\n",
    "\n",
    "            # extend attention_mask for new generated input if only decoder\n",
    "            if self.config.is_encoder_decoder is False:\n",
    "                attention_mask = torch.cat(\n",
    "                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
    "                )\n",
    "\n",
    "        # finalize all open beam hypotheses and add to generated hypotheses\n",
    "        for batch_idx in range(batch_size):\n",
    "            if done[batch_idx]:\n",
    "                continue\n",
    "\n",
    "            # test that beam scores match previously calculated scores if not eos and batch_idx not done\n",
    "            if eos_token_id is not None and all(\n",
    "                (token_id % vocab_size).item() != eos_token_id for token_id in next_tokens[batch_idx]\n",
    "            ):\n",
    "                assert torch.all(\n",
    "                    next_scores[batch_idx, :num_beams] == beam_scores.view(batch_size, num_beams)[batch_idx]\n",
    "                ), \"If batch_idx is not done, final next scores: {} have to equal to accumulated beam_scores: {}\".format(\n",
    "                    next_scores[:, :num_beams][batch_idx],\n",
    "                    beam_scores.view(batch_size, num_beams)[batch_idx],\n",
    "                )\n",
    "\n",
    "            # need to add best num_beams hypotheses to generated hyps\n",
    "            for beam_id in range(num_beams):\n",
    "                effective_beam_id = batch_idx * num_beams + beam_id\n",
    "                final_score = beam_scores[effective_beam_id].item()\n",
    "                final_tokens = input_ids[effective_beam_id]\n",
    "                generated_hyps[batch_idx].add(final_tokens, final_score)\n",
    "\n",
    "        # depending on whether greedy generation is wanted or not define different output_batch_size and output_num_return_sequences_per_batch\n",
    "        output_batch_size = batch_size if do_sample else batch_size * num_return_sequences\n",
    "        output_num_return_sequences_per_batch = 1 if do_sample else num_return_sequences\n",
    "\n",
    "        # select the best hypotheses\n",
    "        sent_lengths = input_ids.new(output_batch_size)\n",
    "        best = []\n",
    "\n",
    "        # retrieve best hypotheses\n",
    "        for i, hypotheses in enumerate(generated_hyps):\n",
    "            sorted_hyps = sorted(hypotheses.beams, key=lambda x: x[0])\n",
    "            for j in range(output_num_return_sequences_per_batch):\n",
    "                effective_batch_idx = output_num_return_sequences_per_batch * i + j\n",
    "                best_hyp = sorted_hyps.pop()[1]\n",
    "                sent_lengths[effective_batch_idx] = len(best_hyp)\n",
    "                best.append(best_hyp)\n",
    "\n",
    "        # shorter batches are padded\n",
    "        if sent_lengths.min().item() != sent_lengths.max().item():\n",
    "            assert pad_token_id is not None, \"`Pad_token_id` has to be defined\"\n",
    "            sent_max_len = min(sent_lengths.max().item() + 1, max_length)\n",
    "            decoded = input_ids.new(output_batch_size, sent_max_len).fill_(pad_token_id)\n",
    "\n",
    "            # fill with hypothesis and eos_token_id if necessary\n",
    "            for i, hypo in enumerate(best):\n",
    "                decoded[i, : sent_lengths[i]] = hypo\n",
    "                if sent_lengths[i] < max_length:\n",
    "                    decoded[i, sent_lengths[i]] = eos_token_id\n",
    "        else:\n",
    "            # none of the hypotheses have an eos_token\n",
    "            assert (len(hypo) == max_length for hypo in best)\n",
    "            decoded = torch.stack(best).type(torch.long).to(self.device)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_inputs(self, prompt_text):\n",
    "        encodings_dict = self.tokenizer.batch_encode_plus([prompt_text], padding=True)\n",
    "        input_ids = torch.tensor(encodings_dict['input_ids'], dtype=torch.int64, device=self.device)\n",
    "        attention_mask = torch.tensor(encodings_dict['attention_mask'], dtype=torch.float32, device=self.device)\n",
    "        position_ids = (attention_mask.long().cumsum(-1) - 1).to(self.device)\n",
    "        position_ids.masked_fill_(position_ids < 0, 0)\n",
    "        # Empty Past State for generating first word\n",
    "        batch_size = input_ids.size(0)\n",
    "        past_shape = [2, batch_size, self.num_attention_heads, 0, self.hidden_size // self.num_attention_heads]\n",
    "        empty_past = []\n",
    "        for i in range(self.num_layer):\n",
    "            empty_past.append(torch.empty(past_shape, dtype=torch.float32).to(self.device))\n",
    "        return input_ids, attention_mask, position_ids, empty_past\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference_with_io_binding(self, input_ids, position_ids, attention_mask, past):\n",
    "        \"\"\"\n",
    "        For performing gpt2 inference using our persistant session.\n",
    "        \"\"\"\n",
    "        output_shapes = Gpt2Helper.get_output_shapes(\n",
    "            batch_size=input_ids.size(0), past_sequence_length=past[0].size(3),\n",
    "            sequence_length=input_ids.size(1), config=self.config\n",
    "        )\n",
    "        output_buffers = Gpt2Helper.get_output_buffers(output_shapes, self.device, is_float16=self.is_float16)\n",
    "        io_binding = Gpt2Helper.prepare_io_binding(\n",
    "            self.session, input_ids, position_ids, attention_mask,\n",
    "            past, output_buffers, output_shapes\n",
    "        )\n",
    "        self.session.run_with_iobinding(io_binding)\n",
    "        outputs = Gpt2Helper.get_outputs_from_io_binding_buffer(\n",
    "            self.session, output_buffers, output_shapes,\n",
    "            return_numpy=False\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def top_k_top_p_filtering(self, logits: Tensor, top_k: int = 0, top_p: float = 1.0,\n",
    "        filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1,\n",
    "    ) -> torch.Tensor:\n",
    "        if top_k > 0:\n",
    "            top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n",
    "            # Remove all tokens with a probability less than the last token of the top-k\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            logits[indices_to_remove] = filter_value\n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            if min_tokens_to_keep > 1:\n",
    "                # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
    "                sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
    "            # Shift the indices to the right to keep also the first token above the threshold\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            # scatter sorted tensors to original indexing\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "            logits[indices_to_remove] = filter_value\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def postprocess_next_token_scores(self, scores, input_ids, no_repeat_ngram_size, bad_words_ids, cur_len, min_length,\n",
    "        max_length, eos_token_id, repetition_penalty, batch_size, num_beams,\n",
    "    ):\n",
    "        # repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)\n",
    "        if repetition_penalty != 1.0:\n",
    "            enforce_repetition_penalty_(\n",
    "                scores,\n",
    "                batch_size,\n",
    "                num_beams,\n",
    "                input_ids,\n",
    "                repetition_penalty,\n",
    "            )\n",
    "\n",
    "        # set eos token prob to zero if min_length is not reached\n",
    "        if eos_token_id is not None and cur_len < min_length:\n",
    "            scores[:, eos_token_id] = -float(\"inf\")\n",
    "\n",
    "        if no_repeat_ngram_size > 0:\n",
    "            # calculate a list of banned tokens to prevent repetitively generating the same ngrams\n",
    "            num_batch_hypotheses = batch_size * num_beams\n",
    "            # from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345\n",
    "            banned_batch_tokens = calc_banned_ngram_tokens(\n",
    "                input_ids, num_batch_hypotheses, no_repeat_ngram_size, cur_len\n",
    "            )\n",
    "            for i, banned_tokens in enumerate(banned_batch_tokens):\n",
    "                scores[i, banned_tokens] = -float(\"inf\")\n",
    "\n",
    "        if bad_words_ids is not None:\n",
    "            # Exclude EOS token (already processed)\n",
    "            bad_words_ids = list(filter(lambda bad_token_seq: bad_token_seq != [eos_token_id], bad_words_ids))\n",
    "            # calculate a list of banned tokens according to bad words\n",
    "            banned_tokens = calc_banned_bad_words_ids(input_ids.tolist(), bad_words_ids)\n",
    "            # Modify the scores in place by setting the banned tokens logits to `-inf`\n",
    "            set_scores_to_inf_for_banned_tokens(scores, banned_tokens)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past: Tuple, beam_idx: Tensor) -> Tuple[Any]:\n",
    "        return tuple(layer_past.index_select(1, beam_idx) for layer_past in past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e4a758-e244-4678-b53c-e6ad2e11f431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dagus/venv/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "onnx_model = GPT2ONNXModel(\n",
    "  'recipes_generation_model.onnx', \n",
    "  'gpt2', \n",
    "  device='cuda', \n",
    "  verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5543fe7c-1d14-477a-8ee7-190b907d59eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error when binding input: There's no data transfer registered for copying tensors from Device:[DeviceType:1 MemoryType:0 DeviceId:0] to Device:[DeviceType:0 MemoryType:0 DeviceId:0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generated \u001b[38;5;241m=\u001b[39m \u001b[43monnx_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<|startoftext|>Prompt:cheese, courgettes, aubergines\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated)\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 341\u001b[0m, in \u001b[0;36mGPT2ONNXModel.generate\u001b[0;34m(self, input_text, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, use_cache, **model_kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_beam_search(\n\u001b[1;32m    331\u001b[0m         input_ids, cur_len\u001b[38;5;241m=\u001b[39mcur_len, max_length\u001b[38;5;241m=\u001b[39mmax_length, min_length\u001b[38;5;241m=\u001b[39mmin_length, do_sample\u001b[38;5;241m=\u001b[39mdo_sample,\n\u001b[1;32m    332\u001b[0m         early_stopping\u001b[38;5;241m=\u001b[39mearly_stopping, temperature\u001b[38;5;241m=\u001b[39mtemperature, top_k\u001b[38;5;241m=\u001b[39mtop_k, top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m         model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[1;32m    339\u001b[0m     )\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 341\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_no_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbad_words_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbad_words_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meffective_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 375\u001b[0m, in \u001b[0;36mGPT2ONNXModel._generate_no_beam_search\u001b[0;34m(self, input_ids, cur_len, max_length, min_length, do_sample, temperature, top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, pad_token_id, eos_token_id, batch_size, attention_mask, use_cache, model_kwargs)\u001b[0m\n\u001b[1;32m    372\u001b[0m position_ids \u001b[38;5;241m=\u001b[39m (attention_mask\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mcumsum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    373\u001b[0m position_ids\u001b[38;5;241m.\u001b[39mmasked_fill_(position_ids \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 375\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_with_io_binding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m    383\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess_next_token_scores(\n\u001b[1;32m    384\u001b[0m     scores\u001b[38;5;241m=\u001b[39mnext_token_logits, input_ids\u001b[38;5;241m=\u001b[39minput_ids, no_repeat_ngram_size\u001b[38;5;241m=\u001b[39mno_repeat_ngram_size,\n\u001b[1;32m    385\u001b[0m     bad_words_ids\u001b[38;5;241m=\u001b[39mbad_words_ids, cur_len\u001b[38;5;241m=\u001b[39mcur_len, min_length\u001b[38;5;241m=\u001b[39mmin_length, max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    386\u001b[0m     eos_token_id\u001b[38;5;241m=\u001b[39meos_token_id, repetition_penalty\u001b[38;5;241m=\u001b[39mrepetition_penalty, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    387\u001b[0m )\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 697\u001b[0m, in \u001b[0;36mGPT2ONNXModel.inference_with_io_binding\u001b[0;34m(self, input_ids, position_ids, attention_mask, past)\u001b[0m\n\u001b[1;32m    692\u001b[0m output_shapes \u001b[38;5;241m=\u001b[39m Gpt2Helper\u001b[38;5;241m.\u001b[39mget_output_shapes(\n\u001b[1;32m    693\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), past_sequence_length\u001b[38;5;241m=\u001b[39mpast[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m    694\u001b[0m     sequence_length\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    695\u001b[0m )\n\u001b[1;32m    696\u001b[0m output_buffers \u001b[38;5;241m=\u001b[39m Gpt2Helper\u001b[38;5;241m.\u001b[39mget_output_buffers(output_shapes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, is_float16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_float16)\n\u001b[0;32m--> 697\u001b[0m io_binding \u001b[38;5;241m=\u001b[39m \u001b[43mGpt2Helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_io_binding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_buffers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrun_with_iobinding(io_binding)\n\u001b[1;32m    702\u001b[0m outputs \u001b[38;5;241m=\u001b[39m Gpt2Helper\u001b[38;5;241m.\u001b[39mget_outputs_from_io_binding_buffer(\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession, output_buffers, output_shapes,\n\u001b[1;32m    704\u001b[0m     return_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    705\u001b[0m )\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/onnxruntime_tools/transformers/gpt2_helper.py:431\u001b[0m, in \u001b[0;36mGpt2Helper.prepare_io_binding\u001b[0;34m(ort_session, input_ids, position_ids, attention_mask, past, output_buffers, output_shapes)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# Bind inputs\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m input_ids\u001b[38;5;241m.\u001b[39mis_contiguous()\n\u001b[0;32m--> 431\u001b[0m \u001b[43mio_binding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlonglong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m data_type \u001b[38;5;241m=\u001b[39m output_buffers[ort_session\u001b[38;5;241m.\u001b[39mget_outputs()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname]\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    435\u001b[0m float_type \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;28;01mif\u001b[39;00m data_type \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;28;01melse\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39mfloat32\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:555\u001b[0m, in \u001b[0;36mIOBinding.bind_input\u001b[0;34m(self, name, device_type, device_id, element_type, shape, buffer_ptr)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, device_type, device_id, element_type, shape, buffer_ptr):\n\u001b[1;32m    547\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    :param name: input name\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m    :param device_type: e.g. cpu, cuda, cann\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;124;03m    :param buffer_ptr: memory pointer to input data\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iobinding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOrtDevice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_ort_device_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m            \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOrtDevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43melement_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error when binding input: There's no data transfer registered for copying tensors from Device:[DeviceType:1 MemoryType:0 DeviceId:0] to Device:[DeviceType:0 MemoryType:0 DeviceId:0]"
     ]
    }
   ],
   "source": [
    "generated = onnx_model.generate(\n",
    "  '<|startoftext|>Prompt:cheese, courgettes, aubergines', \n",
    "  temperature=0.7, \n",
    "  top_k=5, \n",
    "  top_p=0.9,\n",
    "  max_length=100,\n",
    "  min_length=5,\n",
    "  num_return_sequences=1,\n",
    "  repetition_penalty=1.0,\n",
    "  do_sample=True\n",
    ")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc20f4c-ae54-4681-959c-300a142a8225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
